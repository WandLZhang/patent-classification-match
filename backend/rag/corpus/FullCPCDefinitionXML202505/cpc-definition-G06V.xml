<?xml version="1.0" encoding="UTF-8"?>
<definitions publication-date="2025-01-01" publication-type="official">
<definition-item date-revised="2024-08-01"><classification-symbol scheme="cpc">G06V</classification-symbol><definition-title>IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING</definition-title><definition-statement><section-title>Definition statement</section-title><section-body><paragraph-text type="preamble">This place covers:</paragraph-text><paragraph-text type="body">Higher-level interpretation and recognition of images or videos, which includes pattern recognition, pattern learning and semantic interpretation as fundamental aspects. These aspects involve the detection, categorisation, identification, authentication of image or video patterns. For this purpose, image or video data are acquired and preprocessed. In the next step, distinctive features are extracted. Based on these features or representations derived from them, matching, clustering or classification is performed which may lead to one or several decisions, related confidence values (e.g. probabilities), classification or clustering labels. The aim is to find an explanation or to derive a specific meaning.</paragraph-text><paragraph-text type="body">Pattern recognition or pattern learning in a specific, image or video-related context that includes:</paragraph-text><list><list-item><paragraph-text type="body">scene-related patterns and scene-specific elements &#8211;&#160;group <class-ref scheme="cpc">G06V20/00</class-ref>; </paragraph-text></list-item><list-item><paragraph-text type="body">character recognition or recognising digital ink; document-oriented image-based pattern recognition &#8211;&#160;group <class-ref scheme="cpc">G06V30/00</class-ref>; </paragraph-text></list-item><list-item><paragraph-text type="body">human-related, animal-related or biometric patterns in image or video data&#160;&#8211; group <class-ref scheme="cpc">G06V40/00</class-ref>.</paragraph-text></list-item></list><paragraph-text type="body">Further details are given in the Definition statement of group <class-ref scheme="cpc">G06V10/00</class-ref>. Image or video recognition can be carried out by using electronic means (<class-ref scheme="cpc">G06V10/70</class-ref>) or by using optical means (<class-ref scheme="cpc">G06V10/88</class-ref>).</paragraph-text><paragraph-text type="body">Typically, a pattern recognition system involves one or more of the following techniques:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Number of samples</paragraph-text></table-column><table-column><paragraph-text type="body">Data entities (e.g. image objects) involved; Individual</paragraph-text></table-column><table-column><paragraph-text type="body">Data entities (e.g. image objects) involved; Groups (classes)</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">One data sample</paragraph-text></table-column><table-column><paragraph-text type="body">Authentication</paragraph-text></table-column><table-column><paragraph-text type="body">Categorisation</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Several data samples</paragraph-text></table-column><table-column><paragraph-text type="body">Identification</paragraph-text></table-column><table-column><paragraph-text type="body">Clustering</paragraph-text></table-column></table-row></table></section-body></definition-statement><relationship><section-title>Relationships with other classification places</section-title><section-body><paragraph-text type="body">Pattern recognition techniques in general are classified in group <class-ref scheme="cpc">G06F18/00</class-ref>.</paragraph-text><paragraph-text type="body">Some techniques of image or video understanding performed in the preprocessing step &#8212; which start with a bitmap image as an input and derive a non-bitmap representation from it &#8212; can also be encountered in general image analysis. If these techniques do not involve one of the functions of image or video pattern authentication, identification, categorisation or clustering, classification should be made only in the appropriate subgroups of subclass <class-ref scheme="cpc">G06T</class-ref>.</paragraph-text><paragraph-text type="body">Some examples of these techniques are: general methods for image segmentation, e.g. obtaining contiguous image regions with similar pixels, for position and size determination of an object without establishing its identity, for calculating the motion of an image region corresponding to an object irrespective as to the identity of the object, for camera calibration, etc.</paragraph-text><paragraph-text type="body">Techniques based on coding, decoding, compressing or decompressing digital video signals using video object coding are classified in group <class-ref scheme="cpc">H04N19/20</class-ref>.</paragraph-text><paragraph-text type="body">Velocity or trajectory determination systems or sense-of-movement determination systems using radar, sonar or lidar are classified in groups <class-ref scheme="cpc">G01S13/58</class-ref>, <class-ref scheme="cpc">G01S15/58</class-ref>, <class-ref scheme="cpc">G01S17/58</class-ref>, respectively. Radar, sonar or lidar systems specially adapted for mapping or imaging are classified in groups <class-ref scheme="cpc">G01S13/89</class-ref>, <class-ref scheme="cpc">G01S15/89</class-ref>, <class-ref scheme="cpc">G01S17/89</class-ref>.</paragraph-text><paragraph-text type="body">General purpose image data processing, in particular image watermarking, is classified in group <class-ref scheme="cpc">G06T1/00</class-ref>, while selective content distribution, such as generation or processing of protective or descriptive data associated with content involving watermarking is covered by group <class-ref scheme="cpc">H04N21/8358</class-ref>. General purpose image data acquisition and related pre-processing using digital cameras, and processing used to control digital cameras is classified in group <class-ref scheme="cpc">H04N5/00</class-ref>. Play-back, editing or synchronising of a music score, including interpretation therefor, as well as transmission of a music score between systems of musical instruments for play-back, editing or synchronising is classified in subclass <class-ref scheme="cpc">G10H</class-ref>.</paragraph-text></section-body></relationship><references><section-title>References</section-title><application-references><section-title>Application-oriented references</section-title><section-body><paragraph-text type="preamble">Examples of places where the subject matter of this place is covered when specially adapted, used for a particular purpose, or incorporated in a larger system:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Detecting, measuring and recording for medical diagnostic purposes</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">A61B5/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Identifications of persons in medical applications</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">A61B5/117</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Sorting of mail or documents using means for detection of the destination</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">B07C3/10</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Input arrangements for interaction between user and computer</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06F3/01</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Testing to determine the identity or genuineness of paper currency or similar valuable papers or for segregating those which are unacceptable, e.g. banknotes that are alien to a currency</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G07D7/00</class-ref></paragraph-text></table-column></table-row></table></section-body></application-references><informative-references><section-title>Informative references</section-title><section-body><paragraph-text type="preamble">Attention is drawn to the following places, which may be of interest for search:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Programme-controlled manipulators</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">B25J9/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Optical viewing arrangements in vehicles</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">B60R1/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Photogrammetry or videogrammetry, e.g. stereogrammetry; Photographic surveying</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G01C11/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Testing balance of machines or structures</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G01M</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Investigating or analysing materials by determining their chemical or physical properties</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G01N</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Radio direction-finding; Radio navigation; Determining distance or velocity by use of radio waves; Locating or presence-detecting by use of the reflection or reradiation of radio waves; Analogous arrangements using other waves</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G01S</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Geophysics</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G01V</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Optical elements, systems or apparatus</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G02B</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Photomechanical production of textured or patterned surfaces, e.g. for printing, for processing of semiconductor devices</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G03F</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Control or regulating systems in general</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G05B</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06F3/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Comparing digital values in methods or arrangements for processing data by operating upon the order or content of the data handled</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06F7/02</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Content-based image retrieval</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06F16/50</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Fourier, Walsh or analogous domain transformations in digital computers</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06F17/14</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Security arrangements for protecting computer systems against unauthorised activity</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06F21/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Authentication, i.e. establishing the identity or authorisation of security principals</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06F21/30</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Computer-aided design [CAD]</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06F30/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Handling natural language data</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06F40/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Methods or arrangements for sensing record carriers</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06K7/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Record carriers for use with machines and with at least a part designed to carry digital markings</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06K19/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Computer systems based on specific computational models</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06N</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Data processing for business purposes, logistics, stock management</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06Q</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">General purpose image data processing, e.g. specific image analysis processor architectures or configurations</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06T1/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Geometric image transformation in the plane of the image, e.g. rotation of a whole image or part thereof</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06T3/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Image enhancement or restoration</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06T5/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Image analysis in general</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06T7/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Motion image analysis using feature-based methods</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06T7/246</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Image analysis using feature-based methods for determination of transform parameters for the alignment of images</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06T7/33</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Image analysis of texture</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06T7/40</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Image analysis for depth or shape recovery</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06T7/50</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Image analysis using feature-based methods for determining position and orientation of objects</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06T7/73</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Image analysis for determination of colour characteristics</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06T7/90</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Image coding</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06T9/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Image contour coding, e.g. using detection of edges</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06T9/20</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Two-dimensional [2D] image generation</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06T11/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Three-dimensional [3D] image rendering</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06T15/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Lighting effects in 3D image rendering</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06T15/50</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Three-dimensional [3D] modelling for computer graphics</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06T17/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Manipulating 3D models or images for computer graphics</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06T19/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Checking-devices for individual registration on entry or exit</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G07C9/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Burglar, theft or intruder alarms using image scanning and comparing means</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G08B13/194</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Traffic control systems for road vehicles</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G08G1/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Labels, tag tickets or similar identification or indication means</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G09F3/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Speech recognition</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G10L15/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Speaker recognition</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G10L17/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Bioinformatics</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G16B</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Chemoinformatics and computational material science</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G16C</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Healthcare informatics</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G16H</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Semiconductor devices</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">H01L</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Arrangements for secret or secure communications; Network security protocols</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">H04L9/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Scanning, transmission or reproduction of documents, e.g. facsimile transmission</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">H04N1/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Studio circuitry for television systems</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">H04N5/222</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Closed circuit television systems</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">H04N7/18</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using video object coding</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">H04N19/20</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Methods or arrangements for coding, decoding, compressing or decompressing digital video signals, region motion estimation for predictive coding</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">H04N19/543</class-ref></paragraph-text></table-column></table-row></table></section-body></informative-references></references><special-rules><section-title>Special rules of classification</section-title><section-body><paragraph-text type="body">Pattern recognition or pattern learning techniques for image or video understanding involving feature extraction or matching, clustering or classification should be classified in groups <class-ref scheme="cpc">G06V10/40</class-ref> or <class-ref scheme="cpc">G06V10/70</class-ref> irrespective whether an application-related context provided by the groups <class-ref scheme="cpc">G06V20/00</class-ref> - <class-ref scheme="cpc">G06V40/00</class-ref> exists.</paragraph-text></section-body></special-rules><glossary-of-terms><section-title>Glossary of terms</section-title><section-body><paragraph-text type="preamble">In this place, the following terms or expressions are used with the meaning indicated:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">authentication</paragraph-text></table-column><table-column><paragraph-text type="body">verifying the identity of a sample using a test of genuineness by undertaking a one-to-one comparison with the genuine (authentic) sample</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">categorisation</paragraph-text></table-column><table-column><paragraph-text type="body">assigning a sample to a class according to certain distinguishing properties (or characteristics) of that class; it generally involves a one-to-many test in which one data sample is compared with the characteristics of several classes.</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">classification</paragraph-text></table-column><table-column><paragraph-text type="body">assigning labels to patterns</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">clustering</paragraph-text></table-column><table-column><paragraph-text type="body">grouping or separating samples in groups or classes according to their (dis)similarity or closeness. It generally involves many-to-many comparisons using a (dis)similarity measure or a distance function.</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">feature extraction</paragraph-text></table-column><table-column><paragraph-text type="body">deriving descriptive or quantitative measures from data.</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">identification</paragraph-text></table-column><table-column><paragraph-text type="body">in the context of collecting of samples, identification means selecting a particular sample having a (predefined) characteristic which distinguishes it from the others. Several samples are generally matched against the one to be identified in a many-to-one process.</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">image and video understanding</paragraph-text></table-column><table-column><paragraph-text type="body">techniques for semantic interpretation, pattern recognition or pattern learning specifically applied to images and videos</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">pattern</paragraph-text></table-column><table-column><paragraph-text type="body">data having characteristic regularity, or a representation derived from it, having some explanatory value or a meaning, e.g. an object depicted in an image</paragraph-text></table-column></table-row></table></section-body></glossary-of-terms></definition-item>
<definition-item date-revised="2024-08-01"><classification-symbol scheme="cpc">G06V10/00</classification-symbol><definition-title>Arrangements for image or video recognition or understanding  (character recognition in images or video <class-ref scheme="cpc">G06V30/10</class-ref>)</definition-title><definition-statement><section-title>Definition statement</section-title><section-body><paragraph-text type="preamble">This place covers:</paragraph-text><paragraph-text type="body">The functions performed at each step in the operation of an image or video recognition or understanding system.</paragraph-text><paragraph-text type="body">These steps include:</paragraph-text><paragraph-text type="body"><media id="media198.png" file-name="cpc-def-G06V-0198.png" type="png" preferred-width="9.55cm" preferred-height="6.33cm"/></paragraph-text><paragraph-text type="body">Processing steps involved in a pattern recognition or understanding system</paragraph-text><paragraph-text type="body">Classification of each of these steps may be made in groups as follows:</paragraph-text><list><list-item><paragraph-text type="body"><class-ref scheme="cpc">G06V10/10</class-ref> &#8211; Image acquisition;</paragraph-text></list-item><list-item><paragraph-text type="body"><class-ref scheme="cpc">G06V10/20</class-ref> &#8211; Image pre-processing;</paragraph-text></list-item><list-item><paragraph-text type="body"><class-ref scheme="cpc">G06V10/40</class-ref> &#8211; Extraction of image or video features;</paragraph-text></list-item><list-item><paragraph-text type="body"><class-ref scheme="cpc">G06V10/70</class-ref> &#8211; Arrangements for image recognition using pattern recognition or machine learning, e.g. matching, clustering or classification.</paragraph-text></list-item></list></section-body></definition-statement><references><section-title>References</section-title><limiting-references><section-title>Limiting references</section-title><section-body><paragraph-text type="preamble">This place does not cover:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Character recognition in images or videos</paragraph-text></table-column><table-column><paragraph-text type="body"> <class-ref scheme="cpc">G06V30/10</class-ref></paragraph-text></table-column></table-row></table></section-body></limiting-references><application-references><section-title>Application-oriented references</section-title><section-body><paragraph-text type="preamble">Examples of places where the subject matter of this place is covered when specially adapted, used for a particular purpose, or incorporated in a larger system:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Image or video recognition or understanding of scene-related patterns and scene-specific elements</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V20/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Image or video recognition or understanding of human-related, animal-related or biometric patterns in image or video data</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V40/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Detecting, measuring and recording for medical diagnostic purposes</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">A61B5/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Identifications of persons in medical applications</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">A61B5/117</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Sorting of mail or documents using means for detecting the destination</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">B07C3/10</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Input arrangements for interaction between user and computer</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06F3/01</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Checking-devices for individual registration on entry or exit</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G07C9/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Testing to determine the identity or genuineness of paper currency or similar valuable papers</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G07D7/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Burglar, theft or intruder alarms using image scanning and comparing means</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G08B13/194</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Traffic control systems for road vehicles</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G08G1/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Scanning, transmission or reproduction of documents, e.g. facsimile transmission</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">H04N1/00</class-ref></paragraph-text></table-column></table-row></table></section-body></application-references><informative-references><section-title>Informative references</section-title><section-body><paragraph-text type="preamble">Attention is drawn to the following places, which may be of interest for search:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Optical viewing arrangements in vehicles</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">B60R1/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Investigating or analysing materials by determining their chemical or physical properties</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G01N</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Radio direction-finding; Radio navigation; Determining distance or velocity by use of radio waves; Locating or presence-detecting by use of the reflection or reradiation of radio waves; Analogous arrangements using other waves</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G01S</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Geophysics</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G01V</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Optical elements, systems or apparatus</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G02B</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Photomechanical production of textured or patterned surfaces, e.g. for printing, for processing of semiconductor devices</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G03F</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06F3/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Content-based image retrieval</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06F16/50</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Fourier, Walsh or analogous domain transformations</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06F17/14</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Security arrangements for protecting computer systems against unauthorised activity</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06F21/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">User authentication in security arrangements for protecting computers, components thereof, programs or data against unauthorised activity</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06F21/31</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Computer-aided design</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06F30/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Handling natural language data</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06F40/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Computer systems based on specific computational models</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06N</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">General purpose image data processing, e.g. specific image analysis processor architectures or configurations</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06T1/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Geometric image transformation in the plane of the image, e.g. rotation of a whole image or part thereof</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06T3/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Image enhancement or restoration</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06T5/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Image analysis in general</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06T7/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Motion image analysis using feature-based methods</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06T7/246</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Image analysis using feature-based methods for determination of transform parameters for the alignment of images</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06T7/33</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Image analysis of texture</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06T7/40</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Image analysis for depth or shape recovery</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06T7/50</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Image analysis using feature-based methods for determining position and orientation of objects</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06T7/73</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Image analysis for determination of colour characteristics</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06T7/90</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Image coding</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06T9/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Image contour coding, e.g. using detection of edges</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06T9/20</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Two-dimensional image generation</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06T11/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Three-dimensional [3D] image rendering</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06T15/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Lighting effects in 3D image rendering</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06T15/50</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Three-dimensional [3D] modelling for computer graphics</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06T17/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Manipulating 3D models or images for computer graphics</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06T19/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Bioinformatics</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G16B</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Chemoinformatics and computational material science</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G16C</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Healthcare informatics</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G16H</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Secret or secure communication</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">H04L9/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Studio circuitry for television systems</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">H04N5/222</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Closed circuit television systems</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">H04N7/18</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using video object coding</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">H04N19/20</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Methods or arrangements for coding, decoding, compressing or decompressing digital video signals, region motion estimation for predictive coding</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">H04N19/543</class-ref></paragraph-text></table-column></table-row></table></section-body></informative-references></references><glossary-of-terms><section-title>Glossary of terms</section-title><section-body><paragraph-text type="preamble">In this place, the following terms or expressions are used with the meaning indicated:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">classification</paragraph-text></table-column><table-column><paragraph-text type="body">assigning category labels to patterns</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">clustering</paragraph-text></table-column><table-column><paragraph-text type="body">grouping or separating samples in groups or classes according to their (dis)similarity or closeness. It generally involves many-to-many comparisons using a (dis)similarity measure or a distance function.</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">feature extraction</paragraph-text></table-column><table-column><paragraph-text type="body">deriving descriptive or quantitative measures from data</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">image and video understanding</paragraph-text></table-column><table-column><paragraph-text type="body">techniques for semantic interpretation, pattern recognition or pattern learning specifically applied to images and videos</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">pattern</paragraph-text></table-column><table-column><paragraph-text type="body">data having characteristic regularity, or a representation derived from it, having some explanatory value or a meaning, e.g. an object depicted in an image</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">pattern recognition</paragraph-text></table-column><table-column><paragraph-text type="body">detection, categorisation, authentication and identification of patterns for explanatory purposes or to derive a certain meaning in images or video data, by acquiring, pre-processing or extracting distinctive features and matching, clustering or classifying these features or representations thereof</paragraph-text></table-column></table-row></table></section-body></glossary-of-terms></definition-item>
<definition-item date-revised="2025-01-01"><classification-symbol scheme="cpc">G06V10/10</classification-symbol><definition-title>Image acquisition  (document image scanning and transmission <class-ref scheme="cpc">H04N1/00</class-ref>; control of digital cameras <class-ref scheme="cpc">H04N23/60</class-ref>)</definition-title><definition-statement><section-title>Definition statement</section-title><section-body><paragraph-text type="preamble">This place covers:</paragraph-text><paragraph-text type="body">The process of acquiring still images or video sequences for the purpose of subsequently recognising patterns in the acquired images.</paragraph-text><paragraph-text type="body">Image capturing arrangements which visually emphasise those features of the objects that are relevant to the pattern recognition process.</paragraph-text><paragraph-text type="body">Optimising the image capturing conditions, such as correctly placing the object with respect to a camera, choosing the right moment for triggering the image sensor, or suitably setting the parameters of the image sensor.</paragraph-text><paragraph-text type="body">Devices for image acquisition including sensors that generate a conventional two-dimensional image irrespective of its nature (e.g. grey level image, colour image, infrared image, etc.), a three-dimensional point cloud, a sequence of temporally-related images or a video.</paragraph-text><paragraph-text type="subheading">Notes &#8211; other classification places</paragraph-text><paragraph-text type="body">Constructional details of the image acquisition arrangements are covered by a hierarchy of subgroups branching from group <class-ref scheme="cpc">G06V10/12</class-ref>:</paragraph-text><list><list-item><paragraph-text type="body">Group <class-ref scheme="cpc">G06V10/14</class-ref> covers the design of the optical path, including the light source (if any), the different optical elements such as lenses, prisms, mirrors, apertures/diaphragms, filters, the individual optical characteristics of these elements (e.g. refraction indices, focal lengths, chromatic aberrations or distortions) and their optical arrangement;</paragraph-text></list-item><list-item><paragraph-text type="body">Group <class-ref scheme="cpc">G06V10/141</class-ref> covers the control of the illumination, e.g. strategies for activating additional light sources if the ambient lighting is insufficient for a reliable pattern recognition or if individual facets of an object are obstructed by shadows;</paragraph-text></list-item><list-item><paragraph-text type="body">Group <class-ref scheme="cpc">G06V10/143</class-ref> covers processes or devices which emit or sense radiation in different parts of the electromagnetic spectrum (e.g. infrared light, the visual spectrum and ultraviolet light) so as to obtain a comprehensive set of sensor readings, which when combined facilitate an automated distinction of different kinds of objects. For example, an infrared image could be used for isolating living bodies from the background to analyse the presence of a living body in a second image modality, like an RGB image, the second image being aligned with the infrared image. The images captured in infrared could be used for night vision, e.g. detecting pedestrians or animals for collision avoidance. Sensors using multiple wavelengths are also typically used in remote sensing (e.g. when detecting different kinds of crops, forests, lakes, rivers or urban areas in multispectral or hyperspectral satellite imagery; see also group <class-ref scheme="cpc">G06V20/13</class-ref>);</paragraph-text></list-item><list-item><paragraph-text type="body">Group <class-ref scheme="cpc">G06V10/145</class-ref> covers illumination arrangements which are specially adapted to increase the reliability of the pattern recognition process. For example, mitigating shadow artefacts, which are likely to deteriorate the pattern recognition process, by providing specially designed arrangements of light sources (light domes, softboxes, ring flashes, etc). The pattern recognition process can also be supported by means of a structured light projector, which projects specific patterns (e.g. stripes or fringe patterns) onto the object so as to augment the two-dimensional image data with three-dimensional information and for this purpose, additional optical elements such as gratings or filter masks may be added to the illumination system. These various special illumination arrangements are also commonly used for recognising patterns in microscopic imagery (see also group <class-ref scheme="cpc">G06V20/69</class-ref>);</paragraph-text></list-item><list-item><paragraph-text type="body">Group <class-ref scheme="cpc">G06V10/147</class-ref> covers technical details of the image sensor, such as the sensor technology (photodiodes, CCD, CMOS, etc.), the size and the geometrical distribution of light receiving elements on the sensor surface, or the presence of additional optical elements on the sensor (e.g. micro-lenses, diaphragms, collimators or coded aperture masks).</paragraph-text></list-item></list><paragraph-text type="body">Illustrative examples of subject matter classified in this place:</paragraph-text><paragraph-text type="body">1.</paragraph-text><paragraph-text type="body"><media id="media1.png" file-name="cpc-def-G06V-0001.png" type="png" preferred-width="7.45cm" preferred-height="4.91cm"/></paragraph-text><paragraph-text type="body">2.</paragraph-text><paragraph-text type="body"><media id="media2.png" file-name="cpc-def-G06V-0002.png" type="png" preferred-width="6.07cm" preferred-height="5.27cm"/></paragraph-text><paragraph-text type="body">Illumination by casting infrared (IR) light onto a person to highlight regions of a hand, to assist in gesture recognition.</paragraph-text></section-body></definition-statement><relationship><section-title>Relationships with other classification places</section-title><section-body><paragraph-text type="body">CCTV and image transmission systems are classified in group <class-ref scheme="cpc">H04N7/00</class-ref>.</paragraph-text></section-body></relationship><references><section-title>References</section-title><limiting-references><section-title>Limiting references</section-title><section-body><paragraph-text type="preamble">This place does not cover:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Image acquisition in photocopiers or fax machines</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">H04N1/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Controlling digital cameras</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">H04N23/60</class-ref></paragraph-text></table-column></table-row></table></section-body></limiting-references><application-references><section-title>Application-oriented references</section-title><section-body><paragraph-text type="preamble">Examples of places where the subject matter of this place is covered when specially adapted, used for a particular purpose, or incorporated in a larger system:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Image acquisition arrangements specifically designed for optical character recognition</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V30/14</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Image acquisition arrangements specifically designed for fingerprint or palmprint sensors</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V40/13</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Image acquisition arrangements specifically designed for vascular sensors</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V40/145</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Image acquisition arrangements specifically designed for taking pictures of the eye</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V40/19</class-ref></paragraph-text></table-column></table-row></table></section-body></application-references><informative-references><section-title>Informative references</section-title><section-body><paragraph-text type="preamble">Attention is drawn to the following places, which may be of interest for search:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Recognising patterns in satellite imagery</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V20/13</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Recognition of microscopic objects in scenes</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V20/69</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Devices for illuminating a surgical field</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">A61B90/30</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Optical instruments for measuring contours or curvatures</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G01B11/24</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Means for illuminating specimens in microscopes</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G02B21/06</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Digital video cameras</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">H04N23/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Digital image sensors</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">H10F39/12</class-ref>, <class-ref scheme="cpc">H10F39/15</class-ref>, <class-ref scheme="cpc">H10F39/18</class-ref></paragraph-text></table-column></table-row></table></section-body></informative-references></references><glossary-of-terms><section-title>Glossary of terms</section-title><section-body><paragraph-text type="preamble">In this place, the following terms or expressions are used with the meaning indicated:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">CCD</paragraph-text></table-column><table-column><paragraph-text type="body">charge-coupled device</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">CMOS</paragraph-text></table-column><table-column><paragraph-text type="body">complementary metal-oxide-semiconductor</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">visible light</paragraph-text></table-column><table-column><paragraph-text type="body">light as seen by the eye, typically in the range 400 &#8211; 750 nm</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">IR</paragraph-text></table-column><table-column><paragraph-text type="body">infrared, wavelength longer than those of visible light, typically in the range 750 nm - 1 mm</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">LIDAR</paragraph-text></table-column><table-column><paragraph-text type="body">light detection and ranging, optical range sensing method, which targets a laser at objects and generates a three-dimensional representation (a point cloud)</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">NIR</paragraph-text></table-column><table-column><paragraph-text type="body">near-infrared, typically having wavelengths between 750 nm - 2,5 &#956;m.</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">UAV</paragraph-text></table-column><table-column><paragraph-text type="body">unmanned aerial vehicle, a drone</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">UV</paragraph-text></table-column><table-column><paragraph-text type="body">ultraviolet light, wavelengths shorter than that of visible light, but longer than X-rays, typically having a range of 10 nm - 400 nm</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">X-rays</paragraph-text></table-column><table-column><paragraph-text type="body">electromagnetic radiation in the range 10 pm &#8211; 10 nm</paragraph-text></table-column></table-row></table></section-body></glossary-of-terms></definition-item>
<definition-item date-revised="2024-08-01"><classification-symbol scheme="cpc">G06V10/20</classification-symbol><definition-title>Image preprocessing</definition-title><definition-statement><section-title>Definition statement</section-title><section-body><paragraph-text type="preamble">This place covers:</paragraph-text><paragraph-text type="body">Any kind of processing of acquired image or video data before the steps of feature extraction and recognition; devices configured to perform this processing.</paragraph-text><paragraph-text type="body">Processing to prepare an image for feature extraction.</paragraph-text><paragraph-text type="body">Processing to enhance image quality with the intent to emphasise structures in the image, which inform the automated recognition of objects or categories of objects.</paragraph-text><paragraph-text type="body">Processing to attenuate or discard elements of the image, which are unlikely to be useful for the pattern recognition process.</paragraph-text><paragraph-text type="body">Processing converts image to a standard format suitable for feature extraction and pattern recognition routines.</paragraph-text><paragraph-text type="subheading">Notes &#8211; other classification places</paragraph-text><paragraph-text type="body">Specific aspects of pre-processing are covered by the subgroups of group <class-ref scheme="cpc">G06V10/20</class-ref>; they particularly relate to aspects such as:</paragraph-text><list><list-item><paragraph-text type="body">Processes or devices for identifying regions of the image, which should be subjected to the pattern recognition process, or which are likely to contain image information that is relevant for an object recognition task &#8211; covered by group <class-ref scheme="cpc">G06V10/22</class-ref>; </paragraph-text></list-item><list-item><paragraph-text type="body">Correcting wrongly oriented images (e.g. changing the orientation from an erroneous portrait mode to landscape mode), compensating for the pose change of the object by performing affine transformations (translation, scaling, homothety, similarity, reflection, rotation, shear mapping and compositions of them in any combination and sequence), or for correcting geometrical distortions induced by the image capturing &#8211; covered by group <class-ref scheme="cpc">G06V10/24</class-ref>; </paragraph-text></list-item><list-item><paragraph-text type="body">Determination of a bounding box containing the pattern of interest, processing within a region-of-interest [ROI] or volume-of-interest [VOI] to emphasise the pattern for recognition &#8211; covered by group <class-ref scheme="cpc">G06V10/25</class-ref>; </paragraph-text></list-item><list-item><paragraph-text type="body">Devices or processes for separating a candidate object from other, non-interesting image regions or the background; image segmentation to the extent that it is adapted to support a subsequent recognition step &#8211; covered by group <class-ref scheme="cpc">G06V10/26</class-ref>; </paragraph-text></list-item><list-item><paragraph-text type="body">Adjusting the bit depth, e.g. conversion to black-and-white images, and setting thresholds therefor, e.g. by analysis of the histogram of the image grey levels; Converting the image data to a predetermined numerical range, e.g. by scaling pixel values &#8211; covered by group <class-ref scheme="cpc">G06V10/28</class-ref>; </paragraph-text></list-item><list-item><paragraph-text type="body">Techniques for improving the signal-to-noise [SNR] ratio or denoising the image for the purpose of improving the recognition &#8211; covered by group <class-ref scheme="cpc">G06V10/30</class-ref>; </paragraph-text></list-item><list-item><paragraph-text type="body">Adjusting the size or the resolution of the image to a standard format, e.g. by scaling; adjusting the size of the detected object to a certain format &#8211; covered by group <class-ref scheme="cpc">G06V10/32</class-ref>; </paragraph-text></list-item><list-item><paragraph-text type="body">Smoothing or thinning to obtain an alternative, less complex representation of the pattern; applying morphological operators (e.g. morphological dilation, erosion, opening or closing) for filling in gaps or merging elements, with the aim of emphasising the structures relevant for recognition; skeleton extraction for characterising the shape of a pattern &#8211; covered by group <class-ref scheme="cpc">G06V10/34</class-ref>; </paragraph-text></list-item><list-item><paragraph-text type="body">Enhancing the contrast by convolving the image with a filter mask or by applying a non-linear operator to local image patches &#8211; covered by group <class-ref scheme="cpc">G06V10/36</class-ref>.</paragraph-text></list-item></list><paragraph-text type="body">Illustrative example of subject matter classified in this place:</paragraph-text><paragraph-text type="body"><media id="media3.png" file-name="cpc-def-G06V-0003.png" type="png" preferred-width="16cm" preferred-height="9.2cm"/></paragraph-text><paragraph-text type="body">Alignment of the image of a face by affine transformations to obtain a pose-invariant image.</paragraph-text></section-body></definition-statement><relationship><section-title>Relationships with other classification places</section-title><section-body><paragraph-text type="body">Different image pre-processing in general is covered in groups as follows:</paragraph-text><list><list-item><paragraph-text type="body"><class-ref scheme="cpc">G06T3/00</class-ref> when geometric image transformations (e.g. image rotation) are involved;</paragraph-text></list-item><list-item><paragraph-text type="body"><class-ref scheme="cpc">G06T5/00</class-ref> when image enhancement or restoration (e.g. denoising) is performed.</paragraph-text></list-item></list></section-body></relationship><references><section-title>References</section-title><application-references><section-title>Application-oriented references</section-title><section-body><paragraph-text type="preamble">Examples of places where the subject matter of this place is covered when specially adapted, used for a particular purpose, or incorporated in a larger system:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Recognising scenes; Scene-specific elements</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V20/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Character recognition; Recognising digital ink; Document-oriented image-based pattern recognition</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V30/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Image or video recognition or understanding of human-related, animal-related or biometric patterns in image or video data</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V40/00</class-ref></paragraph-text></table-column></table-row></table></section-body></application-references><informative-references><section-title>Informative references</section-title><section-body><paragraph-text type="preamble">Attention is drawn to the following places, which may be of interest for search:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Filter operations to reveal edges, corners or other image features, which are used to characterise objects</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/44</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Image enhancement or restoration</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06T5/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Image segmentation</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06T7/10</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Morphological operators for image segmentation</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06T7/155</class-ref></paragraph-text></table-column></table-row></table></section-body></informative-references></references><glossary-of-terms><section-title>Glossary of terms</section-title><section-body><paragraph-text type="preamble">In this place, the following terms or expressions are used with the meaning indicated:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">DCT</paragraph-text></table-column><table-column><paragraph-text type="body">discrete cosine transform</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">FFT</paragraph-text></table-column><table-column><paragraph-text type="body">fast Fourier transform</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">FOV</paragraph-text></table-column><table-column><paragraph-text type="body">field of view, the region of the environment that an image sensor observes</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">ROI</paragraph-text></table-column><table-column><paragraph-text type="body">region of interest, an image patch that is likely to contain relevant information</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">skeletonisation</paragraph-text></table-column><table-column><paragraph-text type="body">process of shrinking a shape to a connected sequence of lines, which are equidistant to the boundaries of the shape</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">SNR</paragraph-text></table-column><table-column><paragraph-text type="body">signal-to-noise ratio</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">VOI</paragraph-text></table-column><table-column><paragraph-text type="body">volume of interest, a cuboid that encloses three-dimensional data points that are likely to represent relevant information</paragraph-text></table-column></table-row></table></section-body></glossary-of-terms></definition-item>
<definition-item date-revised="2024-08-01"><classification-symbol scheme="cpc">G06V10/22</classification-symbol><definition-title>by selection of a specific region containing or referencing a pattern; Locating or processing of specific regions to guide the detection or recognition</definition-title><definition-statement><section-title>Definition statement</section-title><section-body><paragraph-text type="preamble">This place covers:</paragraph-text><paragraph-text type="body">Guiding a pattern recognition process or device to a specific region of an image where the pattern recognition algorithm is to be applied, e.g. using fiducial markers.</paragraph-text><paragraph-text type="body">The use of reference points in images, e.g. patterns having unique combinations of colours or other image properties, which make them useful for guiding a pattern recognition process.</paragraph-text><paragraph-text type="body">Illustrative examples of subject matter classified in this place:</paragraph-text><paragraph-text type="body">1.</paragraph-text><paragraph-text type="body"><media id="media4.png" file-name="cpc-def-G06V-0004.png" type="png" preferred-width="8.3cm" preferred-height="8.49cm"/></paragraph-text><paragraph-text type="body">A fiducial marker placed in the centre of an object is used for its object detection and recognition.</paragraph-text><paragraph-text type="body">2.</paragraph-text><paragraph-text type="body"><media id="media5.png" file-name="cpc-def-G06V-0005.png" type="png" preferred-width="16cm" preferred-height="5.36cm"/></paragraph-text><paragraph-text type="body">3.</paragraph-text><paragraph-text type="body"><media id="media6.png" file-name="cpc-def-G06V-0006.png" type="png" preferred-width="9.46cm" preferred-height="8.87cm"/></paragraph-text><paragraph-text type="body">A pattern present on a marker gives additional information about the scene to be recognised.</paragraph-text></section-body></definition-statement><relationship><section-title>Relationships with other classification places</section-title><section-body><paragraph-text type="body">Determination of position or orientation of image objects using fiducial markers is covered by group <class-ref scheme="cpc">G06T7/70</class-ref>.</paragraph-text></section-body></relationship><references><section-title>References</section-title><application-references><section-title>Application-oriented references</section-title><section-body><paragraph-text type="preamble">Examples of places where the subject matter of this place is covered when specially adapted, used for a particular purpose, or incorporated in a larger system:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Devices for tracking or guiding surgical instruments</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">A61B34/20</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Fiducial marks and measuring scales in optical systems</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G02B27/32</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Marks applied to semiconductor devices</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">H01L23/544</class-ref></paragraph-text></table-column></table-row></table></section-body></application-references><informative-references><section-title>Informative references</section-title><section-body><paragraph-text type="preamble">Attention is drawn to the following places, which may be of interest for search:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Aligning, centring, orientation detection or correction of the image</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/24</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Image pre-processing for image or video recognition or understanding involving the determination of region of interest [ROI] or volume of interest [VOI]</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/25</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Image analysis for determining position or orientation of objects or cameras</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06T7/70</class-ref></paragraph-text></table-column></table-row></table></section-body></informative-references></references><glossary-of-terms><section-title>Glossary of terms</section-title><section-body><paragraph-text type="preamble">In this place, the following terms or expressions are used with the meaning indicated:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">AR</paragraph-text></table-column><table-column><paragraph-text type="body">augmented reality</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">ARTag</paragraph-text></table-column><table-column><paragraph-text type="body">fiducial marker system based on ARToolKit</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">ARToolKit</paragraph-text></table-column><table-column><paragraph-text type="body">open-source software library for augmented reality</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">FFT</paragraph-text></table-column><table-column><paragraph-text type="body">fast Fourier transform</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">fiducial marker</paragraph-text></table-column><table-column><paragraph-text type="body">an image element which is explicitly designed for serving as a visual landmark point. A fiducial marker can be as simple as a set of lines forming crosshairs or a rectangle, but it can also be a more elaborate pattern such as an augmented reality tag, which additionally conveys information encoded as a two-dimensional barcode. Fiducial markers generally provide information about the position and, often, the orientation or the three-dimensional arrangement of objects in images. Additionally, they can comprise unique identifiers to support the recognition process. Fiducial markers are designed for being easily distinguishable from other image elements; therefore, they commonly have sharp image contrasts (e.g. by limiting their colours to black and white), and they are often designed to generate sharp peaks in the frequency space, allowing them to be easily recognisable by a two-dimensional Fourier transform. Commonly known fiducial markers are those defined by the augmented reality toolkit (ARToolKit).</paragraph-text></table-column></table-row></table></section-body></glossary-of-terms></definition-item>
<definition-item date-revised="2024-08-01"><classification-symbol scheme="cpc">G06V10/24</classification-symbol><definition-title>Aligning, centring, orientation detection or correction of the image</definition-title><definition-statement><section-title>Definition statement</section-title><section-body><paragraph-text type="preamble">This place covers:</paragraph-text><paragraph-text type="body">Methods or arrangements for aligning or centring the image pattern so that it meets the requirements for successfully recognising it; for example, adjusting the camera&apos;s field of view such that a face, a person or another object of interest is located at the centre of the image.</paragraph-text><paragraph-text type="body">Adjusting the field of view such that the object is entirely visible without any parts of the object extending beyond the boundaries of the image</paragraph-text><paragraph-text type="body">Correcting the image alignment by changing from landscape to portrait mode.</paragraph-text><paragraph-text type="body">Detecting or correcting images that were flipped upside-down or left-right.</paragraph-text><paragraph-text type="body">Compensating for image skew.</paragraph-text><paragraph-text type="body">Illustrative examples of subject matter classified in this place:</paragraph-text><paragraph-text type="body">1A.</paragraph-text><paragraph-text type="body"><media id="media7.png" file-name="cpc-def-G06V-0007.png" type="png" preferred-width="10.14cm" preferred-height="19.83cm"/></paragraph-text><paragraph-text type="body">1B.</paragraph-text><paragraph-text type="body"><media id="media8.png" file-name="cpc-def-G06V-0008.png" type="png" preferred-width="9.99cm" preferred-height="20.7cm"/></paragraph-text><paragraph-text type="body">Compensation for the tilt angle of a face captured by a mobile phone by aligning the image.</paragraph-text></section-body></definition-statement><references><section-title>References</section-title><informative-references><section-title>Informative references</section-title><section-body><paragraph-text type="preamble">Attention is drawn to the following places, which may be of interest for search:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Image pre-processing by selection of a specific region containing or referencing a pattern; Locating or processing of specific regions to guide the detection or recognition</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/22</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Image pre-processing for image or video recognition or understanding involving the determination; Determining of region of interest [ROI] or volume of interest [VOI]</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/25</class-ref></paragraph-text></table-column></table-row></table></section-body></informative-references></references><glossary-of-terms><section-title>Glossary of terms</section-title><section-body><paragraph-text type="preamble">In this place, the following terms or expressions are used with the meaning indicated:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">FOV</paragraph-text></table-column><table-column><paragraph-text type="body">field of view, the region of the environment that an image sensor observes</paragraph-text></table-column></table-row></table></section-body></glossary-of-terms></definition-item>
<definition-item date-revised="2024-08-01"><classification-symbol scheme="cpc">G06V10/25</classification-symbol><definition-title>Determination of region of interest [ROI] or a volume of interest [VOI]</definition-title><definition-statement><section-title>Definition statement</section-title><section-body><paragraph-text type="preamble">This place covers:</paragraph-text><paragraph-text type="body">Methods or arrangements for identifying regions in two-dimensional images, or volumes in three-dimensional point cloud data sets, which contain information relevant for recognition.</paragraph-text><paragraph-text type="body">Identifying regions or volumes of interest in an image, point cloud or distance map which are likely to lead to successful object recognition.</paragraph-text><paragraph-text type="subheading">Notes &#8211; technical background</paragraph-text><paragraph-text type="body">These notes provide more information about the technical subject matter that is classified in this place:</paragraph-text><paragraph-text type="body">A region or volume of interest [RoI or VoI] could include, for example, a human face (in case of a CCTV system), a vehicle or a pedestrian (in case of a camera-based traffic monitoring system), an obstacle on the road (in case of an advanced driver assistance system) or an item on a conveyor belt (in case of an industrial automation system).</paragraph-text><paragraph-text type="body">The determination of a region or volume of interest is in essence a task of object detection, that is to say detecting the presence of a particular kind of object in images and localising the object(s).</paragraph-text><paragraph-text type="body">It is the necessity of localising an object and, in particular, of describing the position and the spatial extent of the object (e.g. by outputting a bounding box around it) that distinguishes &quot;object detection&quot; algorithms from &quot;object recognition&quot; algorithms. This is because an &quot;object detection&quot; algorithm will merely assess whether a given visual object exists at a given image location. It may automatically generate a bounding box (e.g. around weeds in a field of vegetables) without solving the problem of &quot;object classification&quot; (e.g. analysing an image of a weed to determine its species and to output its botanical name).</paragraph-text><paragraph-text type="body">Algorithms for detecting ROIs or VOIs in video sequences typically use frame differencing or more advanced optical flow methods for detecting moving objects.Algorithms that determine a region or volume of interest [ROI or VOI] may use visual cues to establish the location of a boundary box, e.g. by evaluating features such as colour distributions or local textures.</paragraph-text><paragraph-text type="body">The determination of a region or volume of interest may be facilitated by using special illumination, such as casting light in a specific direction where an object is to be expected in autonomous driving, or by treating the images of specimens with special staining, as is the case in classification of objects in microscopic imagery.</paragraph-text><paragraph-text type="body">More recently developed algorithms use neural networks [NN] which integrate object detection and recognition. An example is the region-based convolutional neural network [R-CNN] which uses segmentation algorithms for splitting the image into individual segments to find candidate ROIs, followed by inputting each ROI to a classifier for subsequent object recognition.</paragraph-text><paragraph-text type="body">Other solutions, such as the you only look once [YOLO], region-proposal networks [RPN] or single shot detector [SSD] networks integrate the ROI detection into the actual object recognition step.</paragraph-text><paragraph-text type="body">Illustrative example of subject matter classified in this place:</paragraph-text><paragraph-text type="body"><media id="media9.png" file-name="cpc-def-G06V-0009.png" type="png" preferred-width="14.33cm" preferred-height="9.36cm"/></paragraph-text><paragraph-text type="body">Using a mixed architecture based on region-proposal convolutional networks [R-CNN or RPN] to define a region of interest [ROI] and classifying it by another mixed convolutional neural network [CNN] using 2D and 3D information.</paragraph-text></section-body></definition-statement><relationship><section-title>Relationships with other classification places</section-title><section-body><paragraph-text type="body">Determination of a ROI for character recognition is classified in group <class-ref scheme="cpc">G06V30/146</class-ref>.</paragraph-text></section-body></relationship><references><section-title>References</section-title><application-references><section-title>Application-oriented references</section-title><section-body><paragraph-text type="preamble">Examples of places where the subject matter of this place is covered when specially adapted, used for a particular purpose, or incorporated in a larger system:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Devices for radiation diagnosis</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">A61B6/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Diagnostic systems using ultrasound, sound or infrasound</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">A61B8/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Computer-aided diagnosis systems</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G16H50/20</class-ref></paragraph-text></table-column></table-row></table></section-body></application-references><informative-references><section-title>Informative references</section-title><section-body><paragraph-text type="preamble">Attention is drawn to the following places, which may be of interest for search:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Region-based segmentation image analysis</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06T7/11</class-ref></paragraph-text></table-column></table-row></table></section-body></informative-references></references><glossary-of-terms><section-title>Glossary of terms</section-title><section-body><paragraph-text type="preamble">In this place, the following terms or expressions are used with the meaning indicated:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">AOI</paragraph-text></table-column><table-column><paragraph-text type="body">area of interest, synonym for ROI</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">FOV</paragraph-text></table-column><table-column><paragraph-text type="body">field of view, the region of the environment that an image sensor observes</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">R-CNN</paragraph-text></table-column><table-column><paragraph-text type="body">convolutional neural network using a region proposal algorithm for object detection (variants: fast R-CNN, faster R-CNN, cascade R-CNN)</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">ROI</paragraph-text></table-column><table-column><paragraph-text type="body">region of interest, an image region that is likely to contain relevant information concerning an object to be detected and recognised</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">RPN</paragraph-text></table-column><table-column><paragraph-text type="body">region proposal network, an artificial neural network architecture which defines a ROI</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">SSD</paragraph-text></table-column><table-column><paragraph-text type="body">single shot (multibox) detector, a neural network for object detection</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">VOI</paragraph-text></table-column><table-column><paragraph-text type="body">volume of interest, a cuboid that encloses three-dimensional data points that are likely to represent relevant information concerning an object to be detected and recognised</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">YOLO</paragraph-text></table-column><table-column><paragraph-text type="body">you only look once, an artificial neural network used for object detection (comes in various versions: YOLO v2, YOLO v3, etc.).</paragraph-text></table-column></table-row></table></section-body></glossary-of-terms></definition-item>
<definition-item date-revised="2024-08-01"><classification-symbol scheme="cpc">G06V10/26</classification-symbol><definition-title>Segmentation of patterns in the image field; Cutting or merging of image elements to establish the pattern region, e.g. clustering-based techniques; Detection of occlusion</definition-title><definition-statement><section-title>Definition statement</section-title><section-body><paragraph-text type="preamble">This place covers:</paragraph-text><paragraph-text type="body">Methods and arrangements for segmenting patterns in images or video frames, e.g. segmentation algorithms. Note: segmentation algorithms divide images or video frames into distinct regions, so that boundaries between neighbouring regions coincide with changes of some image properties.</paragraph-text><paragraph-text type="body">Segmentation algorithms which operate directly on the image by considering the pixel values and their neighbourhood relationships, e.g. mathematical-morphology based algorithms, such as region growing, watershed methods and level-set methods.</paragraph-text><paragraph-text type="body">Segmentation algorithms which generate a hierarchy of segmentations by starting with a coarse segmentation, which includes only few segments, and successively refine this coarse segmentation by splitting (possibly recursively) the coarse image segments into finer segments (coarse-to-fine approaches).</paragraph-text><paragraph-text type="body">Graph-cut algorithms such as normalised cuts or min-cut which use graph-based clustering algorithms for image segmentation.</paragraph-text><paragraph-text type="body">Region growing algorithms which start with few seed points and iteratively expand these into larger regions until some optimality criterion is fulfilled.</paragraph-text><paragraph-text type="body">The use of classifiers for foreground-background separation. Note: classifiers calculate a score function which expresses a probability (or belief) that a given region of the image is a foreground object or part of the background. The image is then segmented based on these score values.</paragraph-text><paragraph-text type="body">Deep learning models, in particular different encoder-decoder architectures based on convolutional neural networks [CNN&apos;s], applied to semantic image segmentation (a task which requires not only splitting the image into regions, but also consistently assigning labels to image object categories, e.g. &quot;sky&quot;, &quot;trees&quot;, &quot;road&quot;).</paragraph-text><paragraph-text type="body">Detection of occlusion. Note: Sometimes an object (e.g. a trunk of a tree) partly occludes another object, e.g. a dog behind the tree, which may cause the other object to be split into multiple disjoint segments; occlusion detection algorithms deal with such situations so as to join semantically linked segments into a single segment.</paragraph-text><paragraph-text type="body">Other algorithms (e.g. some active contour models) which start from an initial image region, which is large enough to surely enclose an object in the image, and they iteratively shrink this region until its boundary is tightly aligned with the contour of the object.</paragraph-text><paragraph-text type="subheading">Notes &#8211; technical background</paragraph-text><paragraph-text type="body">These notes provide more information about the technical subject matter that is classified in this place:</paragraph-text><paragraph-text type="body">Segmentation algorithms divide images or video frames into distinct regions, so that boundaries between neighbouring regions coincide with changes of some image properties.</paragraph-text><paragraph-text type="body">Segmentation algorithms may determine regions of homogeneous texture, regions having characteristic colours, regions enclosing individual objects, etc.</paragraph-text><paragraph-text type="body">Some segmentation algorithms are in essence clustering algorithms. They disregard the spatial arrangement of pixels in the image and compute clusters in a feature space (e.g. by running the k-means algorithm on all colour values in an image). They then group spatially-connected pixels belonging to the same cluster into a region (a &quot;segment&quot;).</paragraph-text><paragraph-text type="body">Illustrative examples of subject matter classified in this place:</paragraph-text><paragraph-text type="body">1.</paragraph-text><paragraph-text type="body"><media id="media10.png" file-name="cpc-def-G06V-0010.png" type="png" preferred-width="14.69cm" preferred-height="16.87cm"/></paragraph-text><paragraph-text type="body">Colour segmentation of a skin region of a face using the clustering in a colour space.</paragraph-text><paragraph-text type="body">2.</paragraph-text><paragraph-text type="body"><media id="media11.png" file-name="cpc-def-G06V-0011.png" type="png" preferred-width="16cm" preferred-height="6.3cm"/></paragraph-text><paragraph-text type="body">Example of a scene frequently encountered in autonomous driving and its semantic segmentation map with regions such as &quot;road&quot;, &quot;sky&quot;, &quot;trees&quot;, etc.</paragraph-text></section-body></definition-statement><relationship><section-title>Relationships with other classification places</section-title><section-body><paragraph-text type="body">Variational methods used for object recognition, such as active contour models [ACM, or &quot;snakes&quot;], active shape models [ASM] or active appearance models [AMM] are classified in group <class-ref scheme="cpc">G06V10/74</class-ref>.</paragraph-text></section-body></relationship><references><section-title>References</section-title><informative-references><section-title>Informative references</section-title><section-body><paragraph-text type="preamble">Attention is drawn to the following places, which may be of interest for search:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Clustering algorithms for image or video recognition or understanding</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/762</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Image segmentation in general</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06T7/10</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Region-based segmentation image analysis</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06T7/11</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Edge-based image segmentation in general</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06T7/12</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Motion-based image segmentation in general</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06T7/215</class-ref></paragraph-text></table-column></table-row></table></section-body></informative-references></references><glossary-of-terms><section-title>Glossary of terms</section-title><section-body><paragraph-text type="preamble">In this place, the following terms or expressions are used with the meaning indicated:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">BSD</paragraph-text></table-column><table-column><paragraph-text type="body">Berkeley segmentation data set, a collection of manually segmented images</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">K-Means</paragraph-text></table-column><table-column><paragraph-text type="body">clustering algorithm</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">NCUTS</paragraph-text></table-column><table-column><paragraph-text type="body">normalised cuts, a graph-based segmentation algorithm</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">PASCAL VOC</paragraph-text></table-column><table-column><paragraph-text type="body">collection of image data sets for evaluating the performance of computer vision algorithms; it includes a dedicated data set for evaluating segmentation algorithms.</paragraph-text></table-column></table-row></table></section-body></glossary-of-terms></definition-item>
<definition-item date-revised="2024-08-01"><classification-symbol scheme="cpc">G06V10/28</classification-symbol><definition-title>Quantising the image, e.g. histogram thresholding for discrimination between background and foreground patterns</definition-title><definition-statement><section-title>Definition statement</section-title><section-body><paragraph-text type="preamble">This place covers:</paragraph-text><paragraph-text type="body">Methods and arrangements for quantising the image with the effect that the number of possible pixel values does not exceed a predetermined maximum number.</paragraph-text><paragraph-text type="body">Note: In the limit, this quantisation generates a binary two-tone (black-and-white) image, e.g. an image in which foreground objects appear white and any objects in the background appear black. Quantisation to other numbers of pixel values is also possible.</paragraph-text><paragraph-text type="body">Quantisation algorithms which calculate a histogram of the grey value distribution and then use one or more thresholds to divide the grey values into different ranges, and then map grey values in the same range to the same target value.</paragraph-text><paragraph-text type="subheading">Notes &#8211; technical background</paragraph-text><paragraph-text type="body">These notes provide more information about the technical subject matter that is classified in this place:</paragraph-text><paragraph-text type="body">Image quantisation refers to a technique in which the number of possible pixel values is set so that it does not exceed a predetermined maximum number. The source image may be an analogue image, which is quantised for being storable in digital form, or a digital image, which is quantised to a smaller bit depth.</paragraph-text><paragraph-text type="body">Quantisation may be uniform or non-uniform. The subsequent encoding might adapt the number of bits to encode the representation, using more bits to represent those ranges of grey values which are considered to be particularly relevant for the subsequent image recognition step.</paragraph-text><paragraph-text type="body">Colour or grey value quantisation can cause artefacts in the resulting image, such as apparent edges or quantisation boundaries which did not exist in the original image (e.g. colour banding). These artefacts can be mitigated by dithering techniques (e.g. by using the Floyd-Steinberg algorithm).</paragraph-text><paragraph-text type="body">Quantisation may be performed globally (using the histogram of the whole image) or locally (using statistics of local image patches).</paragraph-text><paragraph-text type="body">Illustrative examples of subject matter classified in this place:</paragraph-text><paragraph-text type="body">1.</paragraph-text><paragraph-text type="body"><media id="media12.png" file-name="cpc-def-G06V-0012.png" type="png" preferred-width="13.1cm" preferred-height="20.32cm"/></paragraph-text><paragraph-text type="body">2.</paragraph-text><paragraph-text type="body"><media id="media13.png" file-name="cpc-def-G06V-0013.png" type="png" preferred-width="9.02cm" preferred-height="16.19cm"/></paragraph-text><paragraph-text type="body">Detection of faces in colour images by creating a single-channel image, e.g. a greyscale image, and subsequent binarisation by thresholding.</paragraph-text></section-body></definition-statement><relationship><section-title>Relationships with other classification places</section-title><section-body><paragraph-text type="body">Image enhancement or restoration by the use of histograms is covered by group <class-ref scheme="cpc">G06T5/40</class-ref>.</paragraph-text></section-body></relationship><references><section-title>References</section-title><informative-references><section-title>Informative references</section-title><section-body><paragraph-text type="preamble">Attention is drawn to the following places, which may be of interest for search:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Image coding</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06T9/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Circuits or arrangements for halftone screening</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">H04N1/52</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Systems for transmitting or storing colour picture signals</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">H04N1/64</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Quantisation for adaptive video coding</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">H04N19/124</class-ref></paragraph-text></table-column></table-row></table></section-body></informative-references></references><glossary-of-terms><section-title>Glossary of terms</section-title><section-body><paragraph-text type="preamble">In this place, the following terms or expressions are used with the meaning indicated:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">bit depth</paragraph-text></table-column><table-column><paragraph-text type="body">number of bits, which is available for indicating the grey level or colour of an individual pixel</paragraph-text></table-column></table-row></table></section-body></glossary-of-terms></definition-item>
<definition-item date-revised="2024-08-01"><classification-symbol scheme="cpc">G06V10/30</classification-symbol><definition-title>Noise filtering</definition-title><definition-statement><section-title>Definition statement</section-title><section-body><paragraph-text type="preamble">This place covers:</paragraph-text><paragraph-text type="body">Techniques for noise removal or filtering such as thresholding in the frequency domain (e.g. after a Fourier or wavelet transform), edge-preserving smoothing techniques such as anisotropic diffusion (also called Perona-Malik diffusion) or deep learning approaches to image denoising, e.g. using convolutional neural networks [CNN&apos;s].</paragraph-text><paragraph-text type="body">Linear smoothing filters (e.g. for convolving the original image with a low-pass filter such as a Gaussian kernel matrix or applying a Wiener filter) and non-linear filtering such as median filtering or bilateral filtering (see also group <class-ref scheme="cpc">G06V10/36</class-ref>), when applied for the purpose of noise removal.</paragraph-text><paragraph-text type="body">Noise estimation techniques based on a reference image, wherein the reference image may be:</paragraph-text><list><list-item><paragraph-text type="body">a previously captured image which was obtained with the same camera set-up;</paragraph-text></list-item><list-item><paragraph-text type="body">a previously captured image which was obtained with an optical system of higher quality, potentially downscaled or otherwise converted to match the expected performance parameters of a lower-quality system;</paragraph-text></list-item><list-item><paragraph-text type="body">artificially generated patterns, obtained, e.g. by blurring or smoothing the original image or by means of computer graphics techniques (e.g. rendered from a 3D model of an object).</paragraph-text></list-item></list><paragraph-text type="body">Estimation of noise parameters based on different noise models, e.g. additive white Gaussian noise, speckle noise, etc.</paragraph-text><paragraph-text type="body">Detection of blur or defocusing of the image pattern.</paragraph-text><paragraph-text type="body">Illustrative examples of subject matter classified in this place:</paragraph-text><paragraph-text type="body">1.</paragraph-text><paragraph-text type="body">Face image denoising</paragraph-text><paragraph-text type="body"><media id="media14.png" file-name="cpc-def-G06V-0014.png" type="png" preferred-width="14.69cm" preferred-height="3.45cm"/></paragraph-text><paragraph-text type="body">2.</paragraph-text><paragraph-text type="body"><media id="media15.png" file-name="cpc-def-G06V-0015.png" type="png" preferred-width="10.35cm" preferred-height="3.77cm"/></paragraph-text><paragraph-text type="body">Face denoising using an autoencoder convolutional neural network architecture (above), followed by face recognition using a discriminator architecture (below).</paragraph-text><paragraph-text type="body">3.</paragraph-text><paragraph-text type="body"><media id="media16.png" file-name="cpc-def-G06V-0016.png" type="png" preferred-width="10.2cm" preferred-height="4.42cm"/></paragraph-text></section-body></definition-statement><references><section-title>References</section-title><informative-references><section-title>Informative references</section-title><section-body><paragraph-text type="preamble">Attention is drawn to the following places, which may be of interest for search:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Aligning, centring, orientation detection or correction for image or video recognition or understanding</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/24</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Segmentation of patterns in the image field</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/26</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Local image operators for image or video recognition or understanding, e.g. median filtering</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/36</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Enhancement or restoration for general image processing</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06T5/00</class-ref></paragraph-text></table-column></table-row></table></section-body></informative-references></references><glossary-of-terms><section-title>Glossary of terms</section-title><section-body><paragraph-text type="preamble">In this place, the following terms or expressions are used with the meaning indicated:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">DCT</paragraph-text></table-column><table-column><paragraph-text type="body">discrete cosine transform</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">FFT</paragraph-text></table-column><table-column><paragraph-text type="body">fast Fourier transform</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">PDF</paragraph-text></table-column><table-column><paragraph-text type="body">probability density function</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">SNR</paragraph-text></table-column><table-column><paragraph-text type="body">signal to noise ratio</paragraph-text></table-column></table-row></table></section-body></glossary-of-terms></definition-item>
<definition-item date-revised="2024-08-01"><classification-symbol scheme="cpc">G06V10/32</classification-symbol><definition-title>Normalisation of the pattern dimensions</definition-title><definition-statement><section-title>Definition statement</section-title><section-body><paragraph-text type="preamble">This place covers:</paragraph-text><paragraph-text type="body">Processes and devices for bringing image or video data to a standard format, so that it may be compared with reference data (e.g. with images in an image database or gallery images serving as reference templates).</paragraph-text><paragraph-text type="body">Normalisation or standardisation of the size of images, e.g. by cropping, by reducing the image size via downscaling or sub-sampling, or by enlarging images via up-scaling and interpolation.</paragraph-text><paragraph-text type="subheading">Note &#8211; technical background</paragraph-text><paragraph-text type="body">These notes provide more information about the technical subject matter that is classified in this place:</paragraph-text><paragraph-text type="body">Normalisation can involve adjustments to guarantee that all objects to be recognised have similar size or appearance (e.g. by rescaling facial images so that they are centred and that the area of the face covers a predetermined fraction of the image, or by only selecting frontal images).</paragraph-text><paragraph-text type="body">Illustrative example of subject matter classified in this place:</paragraph-text><paragraph-text type="body"><media id="media17.png" file-name="cpc-def-G06V-0017.png" type="png" preferred-width="16cm" preferred-height="7.28cm"/></paragraph-text><paragraph-text type="body">Correction of the region detected for the face image by cropping around the face region.</paragraph-text></section-body></definition-statement><references><section-title>References</section-title><informative-references><section-title>Informative references</section-title><section-body><paragraph-text type="preamble">Attention is drawn to the following places, which may be of interest for search:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Image enhancement or restoration in general, e.g. dynamic range modification</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06T5/00</class-ref></paragraph-text></table-column></table-row></table></section-body></informative-references></references></definition-item>
<definition-item date-revised="2024-08-01"><classification-symbol scheme="cpc">G06V10/34</classification-symbol><definition-title>Smoothing or thinning of the pattern; Morphological operations; Skeletonisation</definition-title><definition-statement><section-title>Definition statement</section-title><section-body><paragraph-text type="preamble">This place covers:</paragraph-text><paragraph-text type="body">Techniques for binary and grayscale morphological analysis of image patterns. These techniques include:</paragraph-text><list><list-item><paragraph-text type="body">Basic morphological operators: erosion, dilatation, opening, closing, watershed analysis, etc.;</paragraph-text></list-item><list-item><paragraph-text type="body">Detection of patterns or arrangements of pixels in a binary or a greyscale image, e.g. by using the hit-or-miss transform;</paragraph-text></list-item><list-item><paragraph-text type="body">Finding the outline or contour of a foreground object by morphological processing (morphological edge detection, watershed processing, etc.);</paragraph-text></list-item><list-item><paragraph-text type="body">Finding the skeleton of a foreground object, e.g. by thinning, medial axis transformation, contour-based erosion, etc.;</paragraph-text></list-item><list-item><paragraph-text type="body">Distance transformation.</paragraph-text></list-item></list><paragraph-text type="body">Illustrative example of subject matter classified in this place:</paragraph-text><paragraph-text type="body"><media id="media18.png" file-name="cpc-def-G06V-0018.png" type="png" preferred-width="10.1cm" preferred-height="9.36cm"/></paragraph-text><paragraph-text type="body">Extraction of the skeleton representation of a human body by applying morphological operations.</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Restoration for general image processing</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06T5/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Erosion or dilatation, e.g. thinning</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06T5/30</class-ref></paragraph-text></table-column></table-row></table></section-body></definition-statement><glossary-of-terms><section-title>Glossary of terms</section-title><section-body><paragraph-text type="preamble">In this place, the following terms or expressions are used with the meaning indicated:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">skeletonisation</paragraph-text></table-column><table-column><paragraph-text type="body">process of shrinking a shape to a connected sequence of lines, which are equidistant to the boundaries of the shape</paragraph-text></table-column></table-row></table></section-body></glossary-of-terms></definition-item>
<definition-item date-revised="2024-08-01"><classification-symbol scheme="cpc">G06V10/36</classification-symbol><definition-title>Applying a local operator, i.e. means to operate on image points situated in the vicinity of a given point; Non-linear local filtering operations, e.g. median filtering</definition-title><definition-statement><section-title>Definition statement</section-title><section-body><paragraph-text type="preamble">This place covers:</paragraph-text><paragraph-text type="body">Image or video pre-processing techniques which examine a local neighbourhood around a pixel and assign a value to the pixel, which is a function of the values (e.g. colour values or luminance values) of the pixels in this local neighbourhood.</paragraph-text><paragraph-text type="body">The application of local operators in the spatial domain (e.g. by convolving the image with a predefined kernel matrix) or in the frequency domain (e.g. by calculating the Fourier transform and performing a point-wise multiplication in the frequency domain).</paragraph-text><paragraph-text type="subheading">Notes &#8211; technical background</paragraph-text><paragraph-text type="body">These notes provide more information about the technical subject matter that is classified in this place:</paragraph-text><paragraph-text type="body">1. Usually the local neighbourhood is defined as a rectangular region of pixels with the pixel of interest placed at its centre, e.g. a 3*3 pixels neighbourhood or a 5*5 pixels neighbourhood; neighbourhoods having other shapes are also possible, but they are less common.</paragraph-text><paragraph-text type="body">2. Local operators include:</paragraph-text><list><list-item><paragraph-text type="body">Linear operators, e.g. convolutions with low-pass filter matrices (such as a Gaussian kernel or a boxcar function; and convolutions with high-pass filter matrices for sharpening the image, or convolutions with spatial band-pass filters (such as the difference of Gaussians filter);</paragraph-text></list-item><list-item><paragraph-text type="body">Non-linear operators, e.g. median filters and more complex operators such as those for evaluating local luminance differences in order to detect sparkle points which are significantly brighter than their immediate surroundings;</paragraph-text></list-item><list-item><paragraph-text type="body">Non-linear operators, e.g. the Sobel operator and the Marr Hildreth operator are also frequently used for emphasising object boundaries or elongated structures in images;</paragraph-text></list-item><list-item><paragraph-text type="body">Differential operators such as the Laplace operator and filter matrices for calculating image gradients.</paragraph-text></list-item></list><paragraph-text type="subheading">Notes &#8211; other classification places</paragraph-text><paragraph-text type="body">Use of low-pass filter matrices for noise removal &#8211; group <class-ref scheme="cpc">G06V10/30</class-ref>.</paragraph-text><paragraph-text type="body">Use of median filters for noise removal &#8211; group <class-ref scheme="cpc">G06V10/30</class-ref>.</paragraph-text><paragraph-text type="body">Use of the Sobel operator and the Marr Hildreth operator for edge detection &#8211; group <class-ref scheme="cpc">G06V10/44</class-ref>.</paragraph-text><paragraph-text type="body">Illustrative example of subject matter classified in this place:</paragraph-text><paragraph-text type="body"><media id="media19.png" file-name="cpc-def-G06V-0019.png" type="png" preferred-width="13.61cm" preferred-height="4.78cm"/></paragraph-text><paragraph-text type="body">Analysis of local image patches of a face image using a local operator and encoding the representation for subsequent face recognition.</paragraph-text></section-body></definition-statement><references><section-title>References</section-title><informative-references><section-title>Informative references</section-title><section-body><paragraph-text type="preamble">Attention is drawn to the following places, which may be of interest for search:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Noise removal for image or video recognition or understanding</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/30</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Detecting edges or corners for image or video recognition or understanding</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/44</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Extracting features from image blocks</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/50</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Local operators for general image enhancement</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06T5/20</class-ref></paragraph-text></table-column></table-row></table></section-body></informative-references></references><glossary-of-terms><section-title>Glossary of terms</section-title><section-body><paragraph-text type="preamble">In this place, the following terms or expressions are used with the meaning indicated:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">BPF</paragraph-text></table-column><table-column><paragraph-text type="body">band-pass filter</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">DCT</paragraph-text></table-column><table-column><paragraph-text type="body">discrete cosine transform</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">DoG</paragraph-text></table-column><table-column><paragraph-text type="body">difference of Gaussians</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">DWT</paragraph-text></table-column><table-column><paragraph-text type="body">discrete wavelet transform</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">FFT</paragraph-text></table-column><table-column><paragraph-text type="body">fast Fourier transform</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">HPF</paragraph-text></table-column><table-column><paragraph-text type="body">high-pass filter</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Kernel</paragraph-text></table-column><table-column><paragraph-text type="body">filter kernel, a matrix which an image is convolved with</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">LPF</paragraph-text></table-column><table-column><paragraph-text type="body">low-pass filter</paragraph-text></table-column></table-row></table></section-body></glossary-of-terms></definition-item>
<definition-item date-revised="2024-08-01"><classification-symbol scheme="cpc">G06V10/40</classification-symbol><definition-title>Extraction of image or video features</definition-title><definition-statement><section-title>Definition statement</section-title><section-body><paragraph-text type="preamble">This place covers:</paragraph-text><paragraph-text type="body">Methods and arrangements for extracting visual features which are subsequently input to an object recognition algorithm.</paragraph-text><paragraph-text type="subheading">Notes &#8211; technical background</paragraph-text><paragraph-text type="body">These notes provide more information about the technical subject matter that is classified in this place:</paragraph-text><paragraph-text type="body">Formerly, the choice of suitable feature extraction algorithms was a crucial design choice in the art of pattern recognition algorithms. It had a strong influence of the overall performance. With the advent of deep learning, particularly in convolutional neural networks, the need for the hand-picked design of dedicated feature extraction algorithms has decreased to some extent, because the inner layers of the neural networks are trained to automatically find suitable features from the training data.</paragraph-text><paragraph-text type="subheading">Notes &#8211; other classification places</paragraph-text><paragraph-text type="body">Subgroups of group <class-ref scheme="cpc">G06V10/40</class-ref> focus on specific kinds of feature extraction techniques. These include:</paragraph-text><list><list-item><paragraph-text type="body">Features which describe characteristics of the entire image or an entire object (group <class-ref scheme="cpc">G06V10/42</class-ref>);</paragraph-text></list-item></list><paragraph-text type="body">Note: Global feature extraction techniques often involve domain transformations, such as frequency domain transformation. The global descriptors contain numerical data, such as vectors or matrices, but they can also represent the image or object in an abstract form as a string of symbols from a predetermined alphabet, which are integrated using a grammar (covered by group <class-ref scheme="cpc">G06V10/424</class-ref>).</paragraph-text><list><list-item><paragraph-text type="body">Graph structures having vertices and edges (e.g. directed attributed graphs or trees) are another way of representing patterns in images; the vertices of such graph structures represent qualitative or quantitative feature measurements; the edges represent relations between them (covered by group <class-ref scheme="cpc">G06V10/426</class-ref>);</paragraph-text></list-item><list-item><paragraph-text type="body">Local features (covered by group <class-ref scheme="cpc">G06V10/44</class-ref>) build representations of the local image content. Examples of local features include luminance values or colour characteristics, potentially from more than three colour channels, local edges, corners, gradients and texture. Edges can be extracted by convolutions with specially designed filter masks (e.g. Prewitt, Sobel) or by convolutions with a numerical filter, e.g. wavelet filters (Haar, Daubechies), or by difference of Gaussians, Laplacian of Gaussians, Gabor filters etc. Local features such as edges and corners, which can be extracted by applying a pre-defined image operator, are also referred to as low-level features to distinguish them from features such as objects or events, which are extracted using a machine learning algorithm;</paragraph-text></list-item><list-item><paragraph-text type="body">Higher-level features, obtained e.g. by detecting silhouettes of shapes and describing them, e.g. using a chain code, by a Fourier expansion of the contour, by curvature scale-space analysis or by sampling points along object boundaries and quantifying their relative locations;</paragraph-text></list-item><list-item><paragraph-text type="body">Algorithms for evaluating the saliency of local image regions; selecting salient points as key points (covered by group <class-ref scheme="cpc">G06V10/46</class-ref>);</paragraph-text></list-item><list-item><paragraph-text type="body">For the purpose of feature extraction, techniques for converting image or video data to a different parameter space, e.g. using a Hough transform for detecting linear structures in images, or performing a conversion from the spatial domain to the frequency domain or vice versa (group <class-ref scheme="cpc">G06V10/48</class-ref>);</paragraph-text></list-item><list-item><paragraph-text type="body">Techniques for combining individual low-level features into feature vectors by first calculating local statistics of low-level image features in a block of pixels and subsequently generating histograms or deriving other statistical measures in a local neighbourhood (group <class-ref scheme="cpc">G06V10/50</class-ref>);</paragraph-text></list-item><list-item><paragraph-text type="body"> Multi-scale feature extraction algorithms for analysing image or video data at different resolutions; scale space analysis, e.g. wavelet decompositions (group <class-ref scheme="cpc">G06V10/52</class-ref>);</paragraph-text></list-item><list-item><paragraph-text type="body">Techniques for describing textures, such as convolution with Gabor wavelets, grey-level co-occurrence matrices or edge histograms (group <class-ref scheme="cpc">G06V10/54</class-ref>);</paragraph-text></list-item><list-item><paragraph-text type="body">Descriptors which capture colour properties of the image, such as colour histograms, possibly after conversion to a suitable colour space (group <class-ref scheme="cpc">G06V10/56</class-ref>);</paragraph-text></list-item><list-item><paragraph-text type="body">Descriptors which are specially designed for more than three colour channels, in particular for hyperspectral images which contain sensor readings in a multitude of different wavelengths not limited to the visual spectrum (group <class-ref scheme="cpc">G06V10/58</class-ref>);</paragraph-text></list-item><list-item><paragraph-text type="body">Descriptors obtained by integrating information about the imaging conditions, such as the position, the orientation and the spectral properties of light sources, diffuse or specular reflections at object surfaces, etc. (group <class-ref scheme="cpc">G06V10/60</class-ref>);</paragraph-text></list-item><list-item><paragraph-text type="body">Temporal descriptors derived from object movements, e.g. optical flow (group <class-ref scheme="cpc">G06V10/62</class-ref>).</paragraph-text></list-item></list><paragraph-text type="body">Illustrative examples of subject matter classified in this place:</paragraph-text><paragraph-text type="body">1.</paragraph-text><paragraph-text type="body"><media id="media20.png" file-name="cpc-def-G06V-0020.png" type="png" preferred-width="11.6cm" preferred-height="10.96cm"/></paragraph-text><paragraph-text type="body">2.</paragraph-text><paragraph-text type="body"><media id="media21.png" file-name="cpc-def-G06V-0021.png" type="png" preferred-width="11.75cm" preferred-height="10.82cm"/></paragraph-text><paragraph-text type="body">Quantifying local image properties, in particular the local gradient, using a local probe.</paragraph-text><paragraph-text type="body">3.</paragraph-text><paragraph-text type="body"><media id="media22.png" file-name="cpc-def-G06V-0022.png" type="png" preferred-width="16cm" preferred-height="7.65cm"/></paragraph-text><paragraph-text type="body">Different types of features used for object recognition, e.g. contours, line segments, continuous lines.</paragraph-text></section-body></definition-statement><references><section-title>References</section-title><application-references><section-title>Application-oriented references</section-title><section-body><paragraph-text type="preamble">Examples of places where the subject matter of this place is covered when specially adapted, used for a particular purpose, or incorporated in a larger system:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Recognition of scene and scene-specific elements</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V20/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Character recognition; Recognising digital ink; Document-oriented image-based pattern recognition</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V30/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Image or video recognition or understanding of human-related, animal-related or biometric patterns in image or video data</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V40/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Recognition of fingerprints or palmprints</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V40/12</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Recognition of vascular patterns</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V40/14</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Recognition of human faces, e.g. facial parts, sketches or expressions within images or video data</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V40/16</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Recognition of eye characteristics within image or video data, e.g. of the iris</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V40/18</class-ref></paragraph-text></table-column></table-row></table></section-body></application-references><informative-references><section-title>Informative references</section-title><section-body><paragraph-text type="preamble">Attention is drawn to the following places, which may be of interest for search:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Spectrometry, measurement of colour</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G01J3/46</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Image analysis using feature-based methods, in particular, for determination of transform parameters for the alignment of images</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06T7/33</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Image analysis for depth or shape recovery</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06T7/50</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Image contour coding, e.g. using detection of edges</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06T9/20</class-ref></paragraph-text></table-column></table-row></table></section-body></informative-references></references><glossary-of-terms><section-title>Glossary of terms</section-title><section-body><paragraph-text type="preamble">In this place, the following terms or expressions are used with the meaning indicated:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">BoW</paragraph-text></table-column><table-column><paragraph-text type="body">bag of words, a model originally developed for natural language processing; when applied to images, it represents an image by a histogram of visual words, each visual word representing a specific part of the feature space.</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">edge</paragraph-text></table-column><table-column><paragraph-text type="body">region in the image, at which the image exhibits a strong luminance gradient</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">GLCM</paragraph-text></table-column><table-column><paragraph-text type="body">grey-level co-occurrence matrix</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">HOG</paragraph-text></table-column><table-column><paragraph-text type="body">histogram of oriented gradients, a feature descriptor described by N. Dalal and B. Triggs</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">SIFT</paragraph-text></table-column><table-column><paragraph-text type="body">scale-invariant feature transform, a feature detection algorithm</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">SURF</paragraph-text></table-column><table-column><paragraph-text type="body">speeded up robust features, a feature descriptor</paragraph-text></table-column></table-row></table></section-body></glossary-of-terms></definition-item>
<definition-item date-revised="2024-08-01"><classification-symbol scheme="cpc">G06V10/46</classification-symbol><definition-title>Descriptors for shape, contour or point-related descriptors, e.g. scale invariant feature transform [SIFT] or bags of words [BoW]; Salient regional features  (colour feature extraction <class-ref scheme="cpc">G06V10/56</class-ref>)</definition-title><definition-statement><section-title>Definition statement</section-title><section-body><paragraph-text type="preamble">This place covers:</paragraph-text><paragraph-text type="body">Feature extraction techniques in which additional (invariant) information is calculated from certain image regions or patches or at certain points, which are visually more relevant in the process of comparison or matching.</paragraph-text><paragraph-text type="body">Feature extraction techniques in which information from multiple local image patches can be combined into a joint descriptor by using an approach called &quot;bag of features&quot; (from its origin in text document matching), &quot;bag of visual features&quot; or &quot;bag of visual words&quot;.</paragraph-text><paragraph-text type="subheading">Notes &#8211; technical background</paragraph-text><paragraph-text type="body">These notes provide more information about the technical subject matter that is classified in this place:</paragraph-text><paragraph-text type="body">1. The image regions referred to in this place are called &quot;salient regions&quot;, and the points are called &quot;keypoints&quot;, &quot;interest points&quot; or &quot;salient points&quot;. The information assigned to these regions or points is referred to as a local descriptor due to the inherent aspect of locality in the image analysis.</paragraph-text><paragraph-text type="body">A local descriptor aims to be invariant to transformations of the depicted image object (e.g., invariant to affine transforms, object deformations or changes in image capturing conditions such as contrast or scene illumination, etc.).</paragraph-text><paragraph-text type="body">A local descriptor may capture image characteristics across different scales for reliably detecting objects at different sizes, distances or resolutions. Typical descriptors of this kind include:</paragraph-text><list><list-item><paragraph-text type="body">Blob detectors (e.g. SIFT, SURF);</paragraph-text></list-item><list-item><paragraph-text type="body">Region detectors (e.g. MSER, SuperPixels).</paragraph-text></list-item></list><paragraph-text type="body">At a salient point, the pixels in its immediate neighbourhood have visual characteristics, which are different from those of the vast majority of the other pixels. The visual appearance of patches around a salient point is, therefore, somewhat unique; this uniqueness increases the chance of finding a similar patch in other images showing the same object.</paragraph-text><paragraph-text type="body">Generally, salient points can be expected to be located at boundaries of objects and at other image regions having a strong contrast.</paragraph-text><paragraph-text type="body">2. A &quot;bag of visual words&quot; is a histogram, which indicates the frequencies of patches with particular visual properties; these visual properties are expressed by a codebook, which is commonly obtained by clustering a collection of typical feature descriptors (e.g. SIFT features) in the feature space; each bin of the histogram corresponds to one specific cluster in the codebook.</paragraph-text><paragraph-text type="body">The process of generating a bag of features typically involves:</paragraph-text><paragraph-text type="body">A training phase comprising:</paragraph-text><list><list-item><paragraph-text type="body">Extracting local features (e.g. SIFT) from a set of training images;</paragraph-text></list-item><list-item><paragraph-text type="body">Clustering these features into visual words (e.g. with k-means).</paragraph-text></list-item></list><paragraph-text type="body">And an operating phase comprising:</paragraph-text><list><list-item><paragraph-text type="body">Extracting local features from a target image;</paragraph-text></list-item><list-item><paragraph-text type="body">Associating each feature with its closest visual word;</paragraph-text></list-item><list-item><paragraph-text type="body">Building a histogram of visual words over the whole image and match them with templates using a statistical distance (e.g. Mahalanobis distance).</paragraph-text></list-item></list><paragraph-text type="body">Illustrative example of subject matter classified in this place:</paragraph-text><paragraph-text type="body"><media id="media23.png" file-name="cpc-def-G06V-0023.png" type="png" preferred-width="15.18cm" preferred-height="5.4cm"/></paragraph-text><paragraph-text type="body">Defining key-patches for different object classes from a training set, computing features from them and using a set of support vector machine [SVM] classifiers to detect those objects in new images.</paragraph-text></section-body></definition-statement><references><section-title>References</section-title><limiting-references><section-title>Limiting references</section-title><section-body><paragraph-text type="preamble">This place does not cover:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Colour feature extraction</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/56</class-ref></paragraph-text></table-column></table-row></table></section-body></limiting-references><informative-references><section-title>Informative references</section-title><section-body><paragraph-text type="preamble">Attention is drawn to the following places, which may be of interest for search:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Image preprocessing for image or video recognition or understanding involving the determination of a region or volume of interest [ROI, VOI]</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/25</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Global feature extraction, global invariant features (e.g. GIST)</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/42</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Local feature extraction; Extracting of specific shape primitives, e.g. corners, intersections; Computing saliency maps with interactions such as reinforcement or inhibition</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/44</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Local feature extraction, descriptors computed by performing operations within image blocks (e.g. HOG, LBP)</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/50</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Organisation of the matching process; Coarse-fine approaches, e.g. multi-scale approaches; using context analysis; Selection of dictionaries</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/75</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Obtaining sets of training patterns, e.g. bagging</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/774</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Extracting salient feature points for character recognition</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V30/18</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Image retrieval systems using metadata</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06F16/583</class-ref></paragraph-text></table-column></table-row></table></section-body></informative-references></references><special-rules><section-title>Special rules of classification</section-title><section-body><paragraph-text type="body">The present group does not cover biologically inspired approaches of feature extraction based on modelling the receptive fields of visual neurons, such as Gabor filters, and convolutional neural networks [CNN].</paragraph-text><paragraph-text type="body">The use of neural networks for image or video pattern recognition or understanding is classified in group <class-ref scheme="cpc">G06V10/82</class-ref>.</paragraph-text><paragraph-text type="body">When a document presents details on a sampling technique and a clustering technique (bagging), then it should also be classified in group <class-ref scheme="cpc">G06V10/774</class-ref>.</paragraph-text><paragraph-text type="body">Classical &quot;bag of words&quot; techniques remove most image localisation information (geometry).</paragraph-text><paragraph-text type="body">When local features are matched directly from one image to another without involving a bagging technique (and thereby retaining geometric information), e.g. when triplets of features are matched using a geometric transformation with a RANSAC algorithm, then the document should also be classified in group <class-ref scheme="cpc">G06V10/75</class-ref>.</paragraph-text></section-body></special-rules><glossary-of-terms><section-title>Glossary of terms</section-title><section-body><paragraph-text type="preamble">In this place, the following terms or expressions are used with the meaning indicated:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">BOF</paragraph-text></table-column><table-column><paragraph-text type="body">bag of features, see BOW</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">BOVF</paragraph-text></table-column><table-column><paragraph-text type="body">bag of visual features, see BOVF</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">BOVW</paragraph-text></table-column><table-column><paragraph-text type="body">bag of visual words, see BOW</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">BOW</paragraph-text></table-column><table-column><paragraph-text type="body">bag of words, a model originally developed for natural language processing; when applied to images, it represents an image by a histogram of visual words, each visual word representing a specific part of the feature space.</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">MSER</paragraph-text></table-column><table-column><paragraph-text type="body">maximally stable extremal regions, a technique used for blob detection</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">RANSAC</paragraph-text></table-column><table-column><paragraph-text type="body">random sample consensus, a popular regression algorithm</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">SIFT</paragraph-text></table-column><table-column><paragraph-text type="body">scale-invariant feature transform</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">superpixels</paragraph-text></table-column><table-column><paragraph-text type="body">sets of pixels obtained by partitioning a digital image for saliency assessment</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">SURF</paragraph-text></table-column><table-column><paragraph-text type="body">speeded up robust features</paragraph-text></table-column></table-row></table></section-body></glossary-of-terms></definition-item>
<definition-item date-revised="2024-08-01"><classification-symbol scheme="cpc">G06V10/48</classification-symbol><definition-title>by mapping characteristic values of the pattern into a parameter space, e.g. Hough transformation</definition-title><definition-statement><section-title>Definition statement</section-title><section-body><paragraph-text type="preamble">This place covers:</paragraph-text><paragraph-text type="body">Techniques that map the image space into a parameter space using a transformation, such as the Hough transform.</paragraph-text><paragraph-text type="subheading">Notes &#8211; technical background</paragraph-text><paragraph-text type="body">These notes provide more information about the technical subject matter that is classified in this place:</paragraph-text><paragraph-text type="body">The object of the transformations classified in this place is to allow better interpretation and increase the separability between the pattern classes. Each dimension of the parameter space may be linked to a specific feature parameter of an object, e.g. its distance from the origin of the image coordinate system and its orientation. The function which performs the mapping to the parameter space may be invertible, i.e. the original representation could be recovered from the representation in the parameter space.</paragraph-text><paragraph-text type="body">In case of the Hough transform, the parameter space is partitioned into individual bins, which form a so-called accumulator array (a two-dimensional histogram). A voting process maps features in images to individual bins of the accumulator array to finally determine the most probably parameter configuration by retrieving the bin, which has received the maximum bin count.</paragraph-text><paragraph-text type="body">The generalised Hough transform can be applied for recognising arbitrary shapes, e.g. analytic curves such as lines and circles, or binary or grey-value pattern templates.</paragraph-text><paragraph-text type="body">Other examples are the generalised Radon transform, the Trace transform, etc.</paragraph-text><paragraph-text type="body">Illustrative examples of subject matter classified in this place:</paragraph-text><paragraph-text type="body">1.</paragraph-text><paragraph-text type="body"><media id="media24.jpeg" file-name="cpc-def-G06V-0024.jpeg" type="jpeg" preferred-width="7.73cm" preferred-height="4.11cm"/></paragraph-text><paragraph-text type="body">A line in the plane is described by the parameters &quot;d&quot; and &quot;&quot; (distance to the origin and angle).</paragraph-text><paragraph-text type="body">2A.</paragraph-text><paragraph-text type="body"><media id="media25.png" file-name="cpc-def-G06V-0025.png" type="png" preferred-width="8.34cm" preferred-height="9.27cm"/></paragraph-text><paragraph-text type="body">2B.</paragraph-text><paragraph-text type="body"><media id="media26.jpeg" file-name="cpc-def-G06V-0026.jpeg" type="jpeg" preferred-width="10.94cm" preferred-height="8.93cm"/></paragraph-text><paragraph-text type="body">The two lines in the input image (fig. 2A) are mapped by the Hough transform in the parameter space (d,), and the representation leads to two distinct corresponding bright spots (fig. 2B).</paragraph-text><paragraph-text type="body">3.</paragraph-text><paragraph-text type="body"><media id="media27.jpeg" file-name="cpc-def-G06V-0027.jpeg" type="jpeg" preferred-width="10.75cm" preferred-height="14.99cm"/></paragraph-text><paragraph-text type="body">Detection of the visible edges of a cube as points in the Hough parameter space.</paragraph-text></section-body></definition-statement><references><section-title>References</section-title><informative-references><section-title>Informative references</section-title><section-body><paragraph-text type="preamble">Attention is drawn to the following places, which may be of interest for search:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Global feature extraction by analysis of the whole pattern</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/42</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Local feature extraction by analysis of parts of the pattern</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/44</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Descriptors for shape, contour or point-related descriptors, e.g. SIFT</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/46</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Image analysis in general</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06T7/00</class-ref></paragraph-text></table-column></table-row></table></section-body></informative-references></references><special-rules><section-title>Special rules of classification</section-title><section-body><paragraph-text type="body">Global feature extraction for image or video recognition or understanding is classified in group <class-ref scheme="cpc">G06V10/42</class-ref>.</paragraph-text><paragraph-text type="body">Fourier-transform based representations, scale-space representations or wavelet-based representations have a different aim than improving the discriminability in the representation space. The Fourier transform is usually chosen for its geometric invariance properties in the Fourier space (e.g. translation invariance), while the scale-space and wavelet-based representations aim at capturing the variability of the pattern at multiple representation scales. For this reason, the latter two representations are classified as global feature extraction (group <class-ref scheme="cpc">G06V10/42</class-ref>) and, respectively, local feature extraction by scale-space analysis (group <class-ref scheme="cpc">G06V10/52</class-ref>).</paragraph-text></section-body></special-rules></definition-item>
<definition-item date-revised="2024-08-01"><classification-symbol scheme="cpc">G06V10/50</classification-symbol><definition-title>by performing operations within image blocks; by using histograms, e.g. histogram of oriented gradients [HoG]; by summing image-intensity values; Projection analysis</definition-title><definition-statement><section-title>Definition statement</section-title><section-body><paragraph-text type="preamble">This place covers:</paragraph-text><paragraph-text type="body">Feature extraction techniques that perform operations within image blocks or by using histograms.</paragraph-text><paragraph-text type="body">Summation of image intensity values and projection along an axis, e.g. by binning the values into a histogram, to arrive at a more compact feature representation.</paragraph-text><paragraph-text type="subheading">Notes &#8211; technical background</paragraph-text><paragraph-text type="body">These notes provide more information about the technical subject matter that is classified in this place:</paragraph-text><paragraph-text type="body">The processing classified in this group might involve:</paragraph-text><paragraph-text type="body">Block-based arithmetic or logical operations (including non-linear operators such as &quot;max&quot;, &quot;min&quot;, etc.);</paragraph-text><paragraph-text type="body">Histograms of various measurements computed on a block-basis, e.g. histogram of oriented gradients [HOG];</paragraph-text><paragraph-text type="body">Quantification of local geometric arrangements of features by block-based analysis, e.g. local binary patterns [LBP].</paragraph-text><paragraph-text type="body">The blocks need not necessarily be arranged in a form of a grid. They can overlap or can be arranged in different geometrical patterns.</paragraph-text><paragraph-text type="body">Frequently used local feature descriptors which are classified in this group include:</paragraph-text><paragraph-text type="body">- Histogram of oriented gradients [HOG];</paragraph-text><paragraph-text type="body">- Edge oriented histogram [EOH];</paragraph-text><paragraph-text type="body">- Local binary pattern [LBP] and its refinements:</paragraph-text><list><list-item><paragraph-text type="body">Local Gabor binary pattern [LGBP];</paragraph-text></list-item><list-item><paragraph-text type="body">Local edge pattern [LEP];</paragraph-text></list-item><list-item><paragraph-text type="body">Heat kernel local binary pattern [HKLBP];</paragraph-text></list-item><list-item><paragraph-text type="body">Oriented local binary pattern [OLBP];</paragraph-text></list-item><list-item><paragraph-text type="body">Elliptical binary patterns [EBP];</paragraph-text></list-item><list-item><paragraph-text type="body">Local ternary Patterns [LTP];</paragraph-text></list-item><list-item><paragraph-text type="body">Probabilistic LBP [PLBP];</paragraph-text></list-item><list-item><paragraph-text type="body">Elongated quinary patterns [EQP];</paragraph-text></list-item><list-item><paragraph-text type="body">Thee-patch local binary patterns [TPLBP], four-patch local binary patterns [FPLBP];</paragraph-text></list-item><list-item><paragraph-text type="body">Local line binary patterns, etc.;</paragraph-text></list-item></list><paragraph-text type="body">- Shape context;</paragraph-text><paragraph-text type="body">- Gradient location and orientation histogram [GLOH];</paragraph-text><paragraph-text type="body">- Local energy-based shape histogram [LESH];</paragraph-text><paragraph-text type="body">- Oriented histogram of flows [OHF];</paragraph-text><paragraph-text type="body">- Binary robust independent elementary features [BRIEF];</paragraph-text><paragraph-text type="body">- Spin image.</paragraph-text><paragraph-text type="body">Illustrative examples of subject matter classified in this place:</paragraph-text><paragraph-text type="body">1.</paragraph-text><paragraph-text type="body"><media id="media28.png" file-name="cpc-def-G06V-0028.png" type="png" preferred-width="15.37cm" preferred-height="9.38cm"/></paragraph-text><paragraph-text type="body">The local oriented histograms of the gradients or HOG descriptor.</paragraph-text><paragraph-text type="body">2.</paragraph-text><paragraph-text type="body"><media id="media29.png" file-name="cpc-def-G06V-0029.png" type="png" preferred-width="10.92cm" preferred-height="7.68cm"/></paragraph-text><paragraph-text type="body">The &quot;shape context&quot;, a representation which performs binning of the contours of the shape in a circular-like pattern.</paragraph-text></section-body></definition-statement><references><section-title>References</section-title><informative-references><section-title>Informative references</section-title><section-body><paragraph-text type="preamble">Attention is drawn to the following places, which may be of interest for search:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Global feature extraction by analysis of the whole pattern</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/42</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Local feature extraction by analysis of parts of the pattern</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/44</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Descriptors for shape, contour or point-related descriptors, e.g. SIFT</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/46</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Image analysis in general</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06T7/00</class-ref></paragraph-text></table-column></table-row></table></section-body></informative-references></references><glossary-of-terms><section-title>Glossary of terms</section-title><section-body><paragraph-text type="preamble">In this place, the following terms or expressions are used with the meaning indicated:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">BRIEF</paragraph-text></table-column><table-column><paragraph-text type="body">binary robust independent elementary features</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">EOH</paragraph-text></table-column><table-column><paragraph-text type="body">edge oriented histogram</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">GLOH</paragraph-text></table-column><table-column><paragraph-text type="body">gradient location and orientation histogram</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">HOG</paragraph-text></table-column><table-column><paragraph-text type="body">histogram of oriented gradients</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">LBP</paragraph-text></table-column><table-column><paragraph-text type="body">local binary pattern</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">LESH</paragraph-text></table-column><table-column><paragraph-text type="body">local energy-based shape histogram</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">OHF</paragraph-text></table-column><table-column><paragraph-text type="body">oriented histogram of flows</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">OLBP</paragraph-text></table-column><table-column><paragraph-text type="body">oriented local binary pattern</paragraph-text></table-column></table-row></table></section-body></glossary-of-terms></definition-item>
<definition-item date-revised="2024-08-01"><classification-symbol scheme="cpc">G06V10/52</classification-symbol><definition-title>Scale-space analysis, e.g. wavelet analysis  (multi-scale boundary representations <class-ref scheme="cpc">G06V10/42</class-ref>)</definition-title><definition-statement><section-title>Definition statement</section-title><section-body><paragraph-text type="preamble">This place covers:</paragraph-text><paragraph-text type="body">Scale-space representations which allow analysis of the image or video at multiple scales.</paragraph-text><paragraph-text type="subheading">Notes &#8211; technical background</paragraph-text><paragraph-text type="body">These notes provide more information about the technical subject matter that is classified in this place:</paragraph-text><paragraph-text type="body">One primary goal of scale space methods is to achieve scale-invariance, i.e. being able to detect and recognise objects regardless of their size in the image. The scale is usually selected by convolving the image with a parametric &quot;size function&quot;, also called a kernel. After the convolution, which typically blurs the fine-scale structures to a certain degree and which is often followed by a suitable sub-sampling of the blurred image, the actual feature extraction can take place at the selected scale.</paragraph-text><paragraph-text type="body">A very common example of a kernel is the Gaussian kernel:</paragraph-text><paragraph-text type="body"><media id="media30.png" file-name="cpc-def-G06V-0030.png" type="png" preferred-width="4.53cm" preferred-height="0.87cm"/></paragraph-text><paragraph-text type="body">Given the input image f, the scale-space representation is obtained by convolving it with the Gaussian kernel: <media id="media31.png" file-name="cpc-def-G06V-0031.png" type="png" preferred-width="4.51cm" preferred-height="0.49cm"/> where t is the scale of analysis.</paragraph-text><paragraph-text type="body">Scale space approaches can also use Gaussian derivatives, Laplacians of Gaussians, difference of Gaussians (DoG&apos;s), Gabor functions, wavelets (in continuous or discrete form, e.g. Haar, Daubechies).</paragraph-text><paragraph-text type="body">Other alternatives to constructing a scale space which do not use a kernel exist, for instance, by applying to the image a diffusion equation <media id="media32.png" file-name="cpc-def-G06V-0032.png" type="png" preferred-width="2.37cm" preferred-height="0.87cm"/> starting with the initial condition <media id="media33.png" file-name="cpc-def-G06V-0033.png" type="png" preferred-width="3.2cm" preferred-height="0.49cm"/> . In more general terms, these techniques analyse the differential intrinsic structure of the image in order to construct scale-space representations.</paragraph-text><paragraph-text type="body">Techniques based on morphological scale-space construct representations at different scales using mathematical morphology methods, e.g. erosion, dilation, opening, closing.</paragraph-text><paragraph-text type="body">Some other techniques construct multi-scale temporal representations based on the analysis of optical flow for feature extraction in video.</paragraph-text><paragraph-text type="body">Multi-resolution methods implicitly provide representations at multiple scales; such methods are also classified in the present group in as far as they concern image or video feature extraction.</paragraph-text><paragraph-text type="body">Illustrative examples of subject matter classified in this place:</paragraph-text><paragraph-text type="body">1.</paragraph-text><paragraph-text type="body"><media id="media34.png" file-name="cpc-def-G06V-0034.png" type="png" preferred-width="15.24cm" preferred-height="17.04cm"/></paragraph-text><paragraph-text type="body">2.</paragraph-text><paragraph-text type="body"><media id="media35.png" file-name="cpc-def-G06V-0035.png" type="png" preferred-width="12.02cm" preferred-height="18.63cm"/></paragraph-text><paragraph-text type="body">Wavelets applied at different scales for the extraction of facial features.</paragraph-text></section-body></definition-statement><references><section-title>References</section-title><limiting-references><section-title>Limiting references</section-title><section-body><paragraph-text type="preamble">This place does not cover:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Multi-scale boundary representations</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/42</class-ref></paragraph-text></table-column></table-row></table></section-body></limiting-references><informative-references><section-title>Informative references</section-title><section-body><paragraph-text type="preamble">Attention is drawn to the following places, which may be of interest for search:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Descriptors for shape, contour or point-related descriptors, e.g. SIFT</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/46</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Image analysis in general</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06T7/00</class-ref></paragraph-text></table-column></table-row></table></section-body></informative-references></references><glossary-of-terms><section-title>Glossary of terms</section-title><section-body><paragraph-text type="preamble">In this place, the following terms or expressions are used with the meaning indicated:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">CWT</paragraph-text></table-column><table-column><paragraph-text type="body">continuous wavelet transform</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">DoG</paragraph-text></table-column><table-column><paragraph-text type="body">difference of Gaussians</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">DWT</paragraph-text></table-column><table-column><paragraph-text type="body">discrete wavelet transform</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">LoG</paragraph-text></table-column><table-column><paragraph-text type="body">Laplacian of Gaussian</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Haar wavelets</paragraph-text></table-column><table-column><paragraph-text type="body">family of wavelets constructed from rescaled square-shaped functions</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">steerable filter</paragraph-text></table-column><table-column><paragraph-text type="body">class of orientation-selective convolution kernels used for feature extraction that can be expressed via a linear combination of a small set of rotated versions of themselves. As an example, the oriented first derivative of a 2D Gaussian is a steerable filter</paragraph-text></table-column></table-row></table></section-body></glossary-of-terms></definition-item>
<definition-item date-revised="2024-08-01"><classification-symbol scheme="cpc">G06V10/54</classification-symbol><definition-title>relating to texture</definition-title><definition-statement><section-title>Definition statement</section-title><section-body><paragraph-text type="preamble">This place covers:</paragraph-text><paragraph-text type="body">Texture feature extraction for image or video recognition or understanding, either by identifying the boundaries of texture regions, or by analysing the content of the regions themselves.</paragraph-text><paragraph-text type="subheading">Notes &#8211; technical background</paragraph-text><paragraph-text type="body">These notes provide more information about the technical subject matter that is classified in this place:</paragraph-text><paragraph-text type="body">Examples of algorithms used for feature extraction include:</paragraph-text><list><list-item><paragraph-text type="body">Statistical approaches which characterise the texture by local statistical measures such as &quot;edgeness&quot; (local variation of the image gradient), co-occurrence matrices and Haralick features, Laws texture energy, local histogram-based measures, autocorrelation, power spectrum, etc.;</paragraph-text></list-item><list-item><paragraph-text type="body">Structural approaches based on primitives, morphological operations or representations derived from them, or graph-based methods in which image quantities (e.g. pixels or local patches) are represented as graph nodes and are clustered together using graph-based clustering algorithms (e.g. graph-cuts) to identify texture regions;</paragraph-text></list-item><list-item><paragraph-text type="body">Model-based approaches such as auto-regressive models, fractal models, random fields, texton model;</paragraph-text></list-item><list-item><paragraph-text type="body">Transform methods such as Fourier (spectral) analysis, Gabor filters, wavelets, curvelet transform.</paragraph-text></list-item></list><paragraph-text type="body">Illustrative example of subject matter classified in this place:</paragraph-text><paragraph-text type="body"><media id="media36.jpeg" file-name="cpc-def-G06V-0036.jpeg" type="jpeg" preferred-width="8.13cm" preferred-height="5.42cm"/></paragraph-text><paragraph-text type="body">Texture feature extraction allows identification of an animal (zebra) in natural images.</paragraph-text></section-body></definition-statement><references><section-title>References</section-title><informative-references><section-title>Informative references</section-title><section-body><paragraph-text type="preamble">Attention is drawn to the following places, which may be of interest for search:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Global feature extraction by analysis of the whole pattern</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/42</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Local feature extraction by analysis of the parts of the pattern, e.g. by detecting edges, contours, loops, corners, intersections; Connectivity analysis, e.g. connected component analysis</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/44</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Descriptors for shape, contour or point-related descriptors, e.g. SIFT</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/46</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Colour feature extraction</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/56</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Feature extraction related to illumination properties</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/60</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Pattern recognition or image understanding, using clustering</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/762</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Analysis of texture in general</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06T7/40</class-ref></paragraph-text></table-column></table-row></table></section-body></informative-references></references><glossary-of-terms><section-title>Glossary of terms</section-title><section-body><paragraph-text type="preamble">In this place, the following terms or expressions are used with the meaning indicated:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">GLCH</paragraph-text></table-column><table-column><paragraph-text type="body">grey-level co-occurrence histogram (synonym of GLCM)</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">GLCM</paragraph-text></table-column><table-column><paragraph-text type="body">grey-level co-occurrence matrix (Haralick invariant texture features)</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Texton</paragraph-text></table-column><table-column><paragraph-text type="body">basic component of an image that may be recognised visually before the entire image is recognised, and that repeats itself to generate a texture region</paragraph-text></table-column></table-row></table></section-body></glossary-of-terms></definition-item>
<definition-item date-revised="2024-08-01"><classification-symbol scheme="cpc">G06V10/56</classification-symbol><definition-title>relating to colour</definition-title><definition-statement><section-title>Definition statement</section-title><section-body><paragraph-text type="preamble">This place covers:</paragraph-text><paragraph-text type="body">Colour feature extraction for image or video recognition or understanding.</paragraph-text><paragraph-text type="body">Colour feature extraction based on colour invariance.</paragraph-text><paragraph-text type="body">Colour feature extraction based on colour descriptors.</paragraph-text><paragraph-text type="subheading">Notes &#8211; technical background</paragraph-text><paragraph-text type="body">These notes provide more information about the technical subject matter that is classified in this place:</paragraph-text><paragraph-text type="body">1. Colour invariance or, conversely, compensation of colour variations, is important for increasing the robustness in image matching or object recognition. Colour variations are often caused by changing lighting conditions (e.g. the colour of an object typically looks different under ambient light or when the object is being illuminated by an incandescent light bulb). They can also be caused by other factors (e.g. sun-tanned skin has a different colour than pale skin).</paragraph-text><paragraph-text type="body">2. Colour descriptors associate colour information with various image structures such as points, contours or blobs/regions. Colour descriptors (e.g. colour histograms, average colour values etc.) are frequently used in image recognition or image understanding. Typical applications include feature detection based on a model of skin colour, traffic sign detection based on colour information, colour image object detection or video analysis for finding objects with a special colour (e.g. nudity detection).</paragraph-text><paragraph-text type="body">Illustrative examples of subject matter classified in this place:</paragraph-text><paragraph-text type="body">1.</paragraph-text><paragraph-text type="body"><media id="media37.png" file-name="cpc-def-G06V-0037.png" type="png" preferred-width="10.08cm" preferred-height="12.02cm"/></paragraph-text><paragraph-text type="body">Colour histograms used to detect vegetation in natural scenes.</paragraph-text><paragraph-text type="body">2.</paragraph-text><paragraph-text type="body"><media id="media38.jpeg" file-name="cpc-def-G06V-0038.jpeg" type="jpeg" preferred-width="16cm" preferred-height="3.47cm"/></paragraph-text><paragraph-text type="body">Detection of a person based on his/her skin colour.</paragraph-text><paragraph-text type="body">3.</paragraph-text><paragraph-text type="body"><media id="media39.jpeg" file-name="cpc-def-G06V-0039.jpeg" type="jpeg" preferred-width="8.68cm" preferred-height="12.04cm"/></paragraph-text><paragraph-text type="body">Discrimination between skin image regions and nail image regions by clustering in a three-dimensional colour space.</paragraph-text></section-body></definition-statement><references><section-title>References</section-title><informative-references><section-title>Informative references</section-title><section-body><paragraph-text type="preamble">Attention is drawn to the following places, which may be of interest for search:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Global feature extraction by analysis of the whole pattern</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/42</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Descriptors for shape, contour or point-related descriptors, e.g. SIFT</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/46</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Local feature extraction by performing operations within image blocks or by using histograms</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/50</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Image analysis for determination of colour characteristics</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06T7/90</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Colour picture communication systems</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">H04N1/46</class-ref></paragraph-text></table-column></table-row></table></section-body></informative-references></references><glossary-of-terms><section-title>Glossary of terms</section-title><section-body><paragraph-text type="preamble">In this place, the following terms or expressions are used with the meaning indicated:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">CIELAB, L*a*b*</paragraph-text></table-column><table-column><paragraph-text type="body">colour space representation using a lightness value L*, a value a* on a red-green axis and a value b* on a blue yellow axis; these axes reflect human perception</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">CMYK</paragraph-text></table-column><table-column><paragraph-text type="body">colour space representation using cyan, magenta, yellow and black</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">HSB</paragraph-text></table-column><table-column><paragraph-text type="body">colour space representation using separate channels for hue, saturation and brightness (also called HSV)</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">HSL</paragraph-text></table-column><table-column><paragraph-text type="body">colour space representation using separate channels for hue, saturation and lightness</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">HSV</paragraph-text></table-column><table-column><paragraph-text type="body">colour space representation using separate channels for hue, saturation and value (also called HSB)</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">RGB</paragraph-text></table-column><table-column><paragraph-text type="body">colour space representation using red, green and blue colour channels</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">YCbCr</paragraph-text></table-column><table-column><paragraph-text type="body">colour space representation using separate channels for a luminance component Y, a blue-difference component Cb, and a red-difference component Cr, respectively</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">YUV</paragraph-text></table-column><table-column><paragraph-text type="body">colour space representation using separate channels for a luminance component Y and two chrominance components U and V</paragraph-text></table-column></table-row></table></section-body></glossary-of-terms></definition-item>
<definition-item date-revised="2024-08-01"><classification-symbol scheme="cpc">G06V10/58</classification-symbol><definition-title>relating to hyperspectral data</definition-title><definition-statement><section-title>Definition statement</section-title><section-body><paragraph-text type="preamble">This place covers:</paragraph-text><paragraph-text type="body">Techniques for feature extraction in hyperspectral image data.</paragraph-text><paragraph-text type="subheading">Notes &#8211; technical background</paragraph-text><paragraph-text type="body">These notes provide more information about the technical subject matter that is classified in this place:</paragraph-text><paragraph-text type="body">The goal of feature extraction in hyperspectral imaging is to obtain a representation of the relevant features captured by the spectral content of a scene, with the purpose of finding relevant objects and identifying materials. The data can be visualised as a 3D cube, also called a hyperspectral cube, where 2D images corresponding to different spectral wavelengths are superposed. Typical examples of applications are in astronomy, microscopy and satellite image analysis.</paragraph-text><paragraph-text type="body">Depending on the number of spectral bands, one often distinguishes between multispectral imaging (e.g. 3 to 15 bands) and hyperspectral imaging (often several hundred spectral bands). Group <class-ref scheme="cpc">G06V10/58</class-ref> encompasses both alternatives.</paragraph-text><paragraph-text type="body">Illustrative example of subject matter classified in this place:</paragraph-text><paragraph-text type="body"><media id="media40.png" file-name="cpc-def-G06V-0040.png" type="png" preferred-width="13.97cm" preferred-height="16.3cm"/></paragraph-text><paragraph-text type="body">Example of a 3D representation containing hyperspectral features or representations derived from them.</paragraph-text></section-body></definition-statement><relationship><section-title>Relationships with other classification places</section-title><section-body><paragraph-text type="body">Feature extraction in the visible spectrum using colour representations is not regarded as pertaining to this group and it is provided under colour feature extraction &#8211; group <class-ref scheme="cpc">G06V10/56</class-ref>.</paragraph-text></section-body></relationship><references><section-title>References</section-title><application-references><section-title>Application-oriented references</section-title><section-body><paragraph-text type="preamble">Examples of places where the subject matter of this place is covered when specially adapted, used for a particular purpose, or incorporated in a larger system:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Scenes; Scene-specific elements, terrestrial scenes taken from satellites</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V20/13</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Scenes; Scene-specific elements, microscopic objects</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V20/69</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Geographic models</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06T17/05</class-ref></paragraph-text></table-column></table-row></table></section-body></application-references><informative-references><section-title>Informative references</section-title><section-body><paragraph-text type="preamble">Attention is drawn to the following places, which may be of interest for search:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Global feature extraction by analysis of the whole pattern</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/42</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Descriptors for shape, contour or point-related descriptors, e.g. SIFT</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/46</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Local feature extraction by performing operations within image blocks or by using histograms</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/50</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Feature extraction related to colour</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/56</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Geographic information databases</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06F16/29</class-ref></paragraph-text></table-column></table-row></table></section-body></informative-references></references><glossary-of-terms><section-title>Glossary of terms</section-title><section-body><paragraph-text type="preamble">In this place, the following terms or expressions are used with the meaning indicated:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">hyperspectral images</paragraph-text></table-column><table-column><paragraph-text type="body">images in which one continuous spectrum is measured for each pixel. Generally, the spectral resolution is given in nanometres or wave numbers.</paragraph-text></table-column></table-row></table></section-body></glossary-of-terms></definition-item>
<definition-item date-revised="2024-08-01"><classification-symbol scheme="cpc">G06V10/60</classification-symbol><definition-title>relating to illumination properties, e.g. using a reflectance or lighting model</definition-title><definition-statement><section-title>Definition statement</section-title><section-body><paragraph-text type="preamble">This place covers:</paragraph-text><paragraph-text type="body">Techniques in which a model of illumination or reflectance of the image object is relevant for performing feature extraction or for object detection/recognition.</paragraph-text><paragraph-text type="subheading">Notes &#8211; technical background</paragraph-text><paragraph-text type="body">These notes provide more information about the technical subject matter that is classified in this place:</paragraph-text><paragraph-text type="body">Information relating to the object in terms of its surface or geometry and other information relating to the scene (e.g. camera and illumination sources) may be used to compensate for, or to eliminate, the effect of changes in illumination.</paragraph-text><paragraph-text type="body">When involved in the process of image or video recognition or understanding, the techniques covered include:</paragraph-text><list><list-item><paragraph-text type="body">analysing the scene and changing the acquisition to eliminate for undesired illumination conditions (e.g. reflections, albedo);</paragraph-text></list-item><list-item><paragraph-text type="body">evaluating the amount of illumination in the scene, adapting the processing according to this amount;</paragraph-text></list-item><list-item><paragraph-text type="body">estimating the position of the illumination source(s);</paragraph-text></list-item><list-item><paragraph-text type="body">estimating or modelling other properties of the illumination source;</paragraph-text></list-item><list-item><paragraph-text type="body">estimating the amount of front or back light;</paragraph-text></list-item><list-item><paragraph-text type="body">illumination invariant representations for object recognition (e.g. using local/global transforms);</paragraph-text></list-item><list-item><paragraph-text type="body">computing an illumination or reflectance map of the image scene and taking this map into account in object detection/recognition, e.g. &quot;de-lighting&quot; or &quot;re-lighting&quot; techniques;</paragraph-text></list-item><list-item><paragraph-text type="body">extracting features in presence of shadows, estimating shadows;</paragraph-text></list-item><list-item><paragraph-text type="body">representing objects using shape-illumination manifolds.</paragraph-text></list-item></list><paragraph-text type="body">Illustrative examples of subject matter classified in this place:</paragraph-text><paragraph-text type="body">1.</paragraph-text><paragraph-text type="body"><media id="media41.png" file-name="cpc-def-G06V-0041.png" type="png" preferred-width="7.62cm" preferred-height="6.12cm"/></paragraph-text><paragraph-text type="body">Modelling the reflection of human skin by considering its light scattering properties.</paragraph-text><paragraph-text type="body">2. <media id="media42.png" file-name="cpc-def-G06V-0042.png" type="png" preferred-width="9.46cm" preferred-height="6.5cm"/></paragraph-text><paragraph-text type="body">Light source direction determination by modelling the albedo and the shape of an object.</paragraph-text><paragraph-text type="body">3.</paragraph-text><paragraph-text type="body"><media id="media43.jpeg" file-name="cpc-def-G06V-0043.jpeg" type="jpeg" preferred-width="10.86cm" preferred-height="12.53cm"/></paragraph-text><paragraph-text type="body">Person identification in different illumination (lighting) conditions by grouping the images pertaining to a certain illumination condition.</paragraph-text></section-body></definition-statement><references><section-title>References</section-title><informative-references><section-title>Informative references</section-title><section-body><paragraph-text type="preamble">Attention is drawn to the following places, which may be of interest for search:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Global feature extraction by analysis of the whole pattern</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/42</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Descriptors for shape, contour or point-related descriptors, e.g. SIFT</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/46</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Local feature extraction by performing operations within image blocks or by using histograms</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/50</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Feature extraction related to colour</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/56</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Image analysis for depth or shape recovery</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06T7/50</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Image analysis for determining position or orientation of objects or cameras</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06T7/70</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Colour picture communication systems</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">H04N1/46</class-ref></paragraph-text></table-column></table-row></table></section-body></informative-references></references><glossary-of-terms><section-title>Glossary of terms</section-title><section-body><paragraph-text type="preamble">In this place, the following terms or expressions are used with the meaning indicated:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Albedo</paragraph-text></table-column><table-column><paragraph-text type="body">the proportion of the incident light or radiation that is reflected by a surface</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">diffuse reflection</paragraph-text></table-column><table-column><paragraph-text type="body">reflection having the property that incident light rays are scattered in many different directions</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">illumination cone</paragraph-text></table-column><table-column><paragraph-text type="body">representation of a set of all possible images of a convex Lambertian surface created by varying the strength and direction of an arbitrary number of light sources at infinity</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Lambertian model</paragraph-text></table-column><table-column><paragraph-text type="body">model according to which the radiant intensity or luminous intensity observed from an ideal diffusely reflecting surface or ideal diffuse radiator is directly proportional to the cosine of the angle &#952; between the direction of the incident light and the surface normal (I = I<sub>0</sub> cos(&#952;))</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">reflectance</paragraph-text></table-column><table-column><paragraph-text type="body">effectiveness of a surface in reflecting radiant energy, component of the response of the electronic structure of the material to the electromagnetic field of light, and is in general a function of the frequency, or wavelength, of the light, its polarisation, and the angle of incidence</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">specular reflection</paragraph-text></table-column><table-column><paragraph-text type="body">mirror-like reflection</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">spherical harmonics</paragraph-text></table-column><table-column><paragraph-text type="body">special functions defined on the surface of a sphere, generally used to model the reflectance properties of a 3D surface</paragraph-text></table-column></table-row></table></section-body></glossary-of-terms></definition-item>
<definition-item date-revised="2024-08-01"><classification-symbol scheme="cpc">G06V10/62</classification-symbol><definition-title>relating to a temporal dimension, e.g. time-based feature extraction; Pattern tracking</definition-title><definition-statement><section-title>Definition statement</section-title><section-body><paragraph-text type="preamble">This place covers:</paragraph-text><paragraph-text type="body">Techniques involving time-related feature extraction and pattern tracking for image or video recognition or understanding. Such techniques include:</paragraph-text><list><list-item><paragraph-text type="body">generative methods, such as kernel-based tracking [KT], Kalman filtering [KF], particle filtering [PF];</paragraph-text></list-item><list-item><paragraph-text type="body">discriminative tracking methods, such as joint probability data association filtering (JPDAF), multiple-hypothesis tracking [MHT], flow network framework [FNF].</paragraph-text></list-item></list><paragraph-text type="subheading">Notes &#8211; technical background</paragraph-text><paragraph-text type="body">These notes provide more information about the technical subject matter that is classified in this place:</paragraph-text><paragraph-text type="body">1. Tracking may be implemented using a single camera or a system with multiple cameras, with possibly overlapping field of views [FOV].</paragraph-text><paragraph-text type="body">2. In time-related feature extraction and pattern tracking, the features extracted from the video can be low-level (e.g. pixel colours, gradient, motion cues), mid-level (e.g. edges, corners, interest points, regions, etc.) or high-level (e.g. geometrical arrangements of parts of an object). The tracking often involves the foreground-background segmentation or background modelling in order to focus only on the objects of interest and reduce the overall complexity. Target representations are models of the objects of interest which rely on visual cues such as shape, texture, colour. There are rigid models (e.g. regions or volumes of interest), articulated models (e.g. kinematic chains) or deformable models (e.g. fluid models, point-distributions, appearance models).</paragraph-text><paragraph-text type="body">An inherent problem during tracking is that of localisation, which is usually solved:</paragraph-text><list><list-item><paragraph-text type="body">using single-hypothesis localisation in which only one track candidate estimate is evaluated over time, e.g. gradient-based trackers such as Kanade-Lucas-Tomasi [KLT], mean-shift [MS] tracker, Bayes tracker, Kalman filtering; or</paragraph-text></list-item><list-item><paragraph-text type="body">a multiple-hypothesis localisation where multiple tracks are evaluated simultaneously, e.g. grid sampling, particle filter, hybrid methods such as hybrid particle mean shift tracker.</paragraph-text></list-item></list><paragraph-text type="body">Models employed during tracking include graphical models (e.g. Markov models), graph-matching based tracking, camera-link model [CLM] or statistical models such as maximum a-posteriori estimation (MAP).</paragraph-text><paragraph-text type="body">Problems frequently occurring are that of context modelling (e.g. changes in background, clutter, duration of the tracking events), or in the case of a multiple camera system, that of re-identification, i.e. detection of the same object in the field of view of these cameras.</paragraph-text><paragraph-text type="body">Neural networks have been more recently applied to the problem of tracking, examples of architectures include: generic object tracking using regression networks [GOTURN], multi-domain network [MDNet], long short-term memory [LSTM] networks, recurrent you only look once [ROLO] networks.</paragraph-text><paragraph-text type="body">Illustrative example of subject matter classified in this place: <media id="media44.png" file-name="cpc-def-G06V-0044.png" type="png" preferred-width="10.8cm" preferred-height="7.2cm"/></paragraph-text><paragraph-text type="body">Tracking, person re-identification in a multiple camera system.</paragraph-text></section-body></definition-statement><references><section-title>References</section-title><informative-references><section-title>Informative references</section-title><section-body><paragraph-text type="preamble">Attention is drawn to the following places, which may be of interest for search:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Image preprocessing for image or video recognition or understanding involving the determination of a region or volume of interest [ROI, VOI]</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/25</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Global feature extraction by analysis of the whole pattern</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/42</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Descriptors for shape, contour or point-related descriptors, e.g. SIFT</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/46</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Local feature extraction by performing operations within image blocks or by using histograms</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/50</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Feature extraction related to texture</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/54</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Feature extraction related to colour</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/56</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Pattern recognition or machine learning for image or video recognition or understanding using probabilistic graphical models</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/84</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Analysis of motion in images</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06T7/20</class-ref></paragraph-text></table-column></table-row></table></section-body></informative-references></references><glossary-of-terms><section-title>Glossary of terms</section-title><section-body><paragraph-text type="preamble">In this place, the following terms or expressions are used with the meaning indicated:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">CLM</paragraph-text></table-column><table-column><paragraph-text type="body">camera link model</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">FOV</paragraph-text></table-column><table-column><paragraph-text type="body">field of view</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">GM</paragraph-text></table-column><table-column><paragraph-text type="body">graph matching</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">KF</paragraph-text></table-column><table-column><paragraph-text type="body">Kalman filter</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">KT</paragraph-text></table-column><table-column><paragraph-text type="body">kernel tracking</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">MAP</paragraph-text></table-column><table-column><paragraph-text type="body">maximum a-posteriori estimation</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">MHT</paragraph-text></table-column><table-column><paragraph-text type="body">multiple hypothesis tracking</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">PF</paragraph-text></table-column><table-column><paragraph-text type="body">particle filtering</paragraph-text></table-column></table-row></table></section-body></glossary-of-terms></definition-item>
<definition-item date-revised="2024-08-01"><classification-symbol scheme="cpc">G06V10/70</classification-symbol><definition-title>using pattern recognition or machine learning  (optical pattern recognition or electronic computations therefor <class-ref scheme="cpc">G06V10/88</class-ref>)</definition-title><definition-statement><section-title>Definition statement</section-title><section-body><paragraph-text type="preamble">This place covers:</paragraph-text><paragraph-text type="body">Methods and arrangements for pattern recognition or machine learning in image or video data.</paragraph-text><paragraph-text type="subheading">Notes &#8211; technical background</paragraph-text><paragraph-text type="body">These notes provide more information about the technical subject matter that is classified in this place:</paragraph-text><paragraph-text type="body">Pattern recognition algorithms try to identify or discover regularities in data (such as a collection of representative features derived from images) through the use of computer algorithms. These regularities are used to take actions such as classifying the data into different categories. Modern approaches include the use of techniques from machine learning for this purpose.</paragraph-text><paragraph-text type="body">Pattern recognition and machine learning algorithms can operate in a supervised fashion, an unsupervised fashion or in hybrid forms (e.g. semi-supervised). Supervised methods require not only exemplary feature patterns for training the model, but also a-priori knowledge in the form of associated class labels that indicate a respective category or class. Using labelled inputs and outputs, the accuracy can be measured and the method can adapt/learn over time. In contrast, unsupervised learning may discover hidden patterns in data without the need for human intervention, with the goal of, e.g. clustering unlabelled data sets.</paragraph-text><paragraph-text type="subheading">Notes &#8211; other classification places</paragraph-text><paragraph-text type="body">Specific aspects of the pattern recognition or machine learning in the recognition or understanding of images or video are classified in subgroups as follows:</paragraph-text><list><list-item><paragraph-text type="body">Preparation of data items for being fed into a pattern recognition or machine learning algorithm (e.g. complementing missing data, statistical pre-processing, discarding feature vectors which have been identified as outliers), is classified in group <class-ref scheme="cpc">G06V10/72</class-ref>; </paragraph-text></list-item><list-item><paragraph-text type="body">Pattern matching based on a measure of (dis)similarity, e.g. template matching, is classified in group <class-ref scheme="cpc">G06V10/74</class-ref>. The definition of suitable criteria (e.g. similarity thresholds) for deciding whether a match is successful or not is also classified in group <class-ref scheme="cpc">G06V10/74</class-ref>; </paragraph-text></list-item><list-item><paragraph-text type="body">Clustering algorithms are classified in group <class-ref scheme="cpc">G06V10/762</class-ref>; </paragraph-text></list-item><list-item><paragraph-text type="body">Classification algorithms are classified in group <class-ref scheme="cpc">G06V10/764</class-ref>; </paragraph-text></list-item><list-item><paragraph-text type="body">Regression algorithms are classified in group <class-ref scheme="cpc">G06V10/766</class-ref>; </paragraph-text></list-item><list-item><paragraph-text type="body">The processing of image or video features in feature spaces is classified in group <class-ref scheme="cpc">G06V10/77</class-ref>; also classified there are techniques of data integration or data reduction, e.g. principal component analysis [PCA], independent component analysis [ICA], self-organising maps [SOM] or blind source separation. Feature selection methods which pick the most informative vectors/dimensions of high-dimensional feature vectors during model training, and which disregard the others, are classified in group <class-ref scheme="cpc">G06V10/771</class-ref>; </paragraph-text></list-item><list-item><paragraph-text type="body">Generating sets of training patterns and bootstrap methods (e.g. bagging, boosting) are classified in group <class-ref scheme="cpc">G06V10/774</class-ref>; </paragraph-text></list-item><list-item><paragraph-text type="body">Validation and performance evaluation of the methods of pattern recognition and machine learning are classified in group <class-ref scheme="cpc">G06V10/776</class-ref>; </paragraph-text></list-item><list-item><paragraph-text type="body">Active pattern learning, e.g. online learning of features, is classified in group <class-ref scheme="cpc">G06V10/778</class-ref>; </paragraph-text></list-item><list-item><paragraph-text type="body">Fusion, i.e. combining data from various sources, is classified in group <class-ref scheme="cpc">G06V10/80</class-ref>; </paragraph-text></list-item><list-item><paragraph-text type="body">Artificial neural networks [ANN] are classified in group <class-ref scheme="cpc">G06V10/82</class-ref>; </paragraph-text></list-item><list-item><paragraph-text type="body">Graphical models, e.g. Markov models or Bayesian networks, are classified in group <class-ref scheme="cpc">G06V10/84</class-ref>; </paragraph-text></list-item><list-item><paragraph-text type="body">Syntactic or structural representations and graph matching are classified in group <class-ref scheme="cpc">G06V10/86</class-ref>.</paragraph-text></list-item></list></section-body></definition-statement><references><section-title>References</section-title><limiting-references><section-title>Limiting references</section-title><section-body><paragraph-text type="preamble">This place does not cover:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Pattern recognition performed by an arrangement of optical devices rather than by machine learning</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/88</class-ref></paragraph-text></table-column></table-row></table></section-body></limiting-references><application-references><section-title>Application-oriented references</section-title><section-body><paragraph-text type="preamble">Examples of places where the subject matter of this place is covered when specially adapted, used for a particular purpose, or incorporated in a larger system:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Scenes; Scene-specific elements</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V20/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Character recognition</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V30/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Image or video recognition or understanding of human-related, animal-related or biometric patterns in image or video data</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V40/00</class-ref></paragraph-text></table-column></table-row></table></section-body></application-references><informative-references><section-title>Informative references</section-title><section-body><paragraph-text type="preamble">Attention is drawn to the following places, which may be of interest for search:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Neural network models per se, not specially adapted to a particular data modality</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06N3/02</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Genetic algorithms per se, not specially adapted to a particular data modality</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06N3/12</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Machine learning in general</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06N20/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Identification of individual speakers or sound sources by multimodal pattern matching</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G10L17/10</class-ref></paragraph-text></table-column></table-row></table></section-body></informative-references></references><glossary-of-terms><section-title>Glossary of terms</section-title><section-body><paragraph-text type="preamble">In this place, the following terms or expressions are used with the meaning indicated:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">AE</paragraph-text></table-column><table-column><paragraph-text type="body">auto-encoder network</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">AlexNet</paragraph-text></table-column><table-column><paragraph-text type="body">CNN designed by Alex Krizhevsky et al.</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Backprop</paragraph-text></table-column><table-column><paragraph-text type="body">backpropagation, an algorithm for adjusting the weights of an artificial neural network</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">BERT</paragraph-text></table-column><table-column><paragraph-text type="body">bidirectional encoder representations from transformers, a transformer based artificial neural network</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">C4.5</paragraph-text></table-column><table-column><paragraph-text type="body">an algorithm for learning decision trees</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">CART</paragraph-text></table-column><table-column><paragraph-text type="body">classification and regression trees</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">CNN</paragraph-text></table-column><table-column><paragraph-text type="body">convolutional neural network, an artificial neural network that includes convolutional layers</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">CPD</paragraph-text></table-column><table-column><paragraph-text type="body">coherent point drift, an algorithm for matching point clouds</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">DAG</paragraph-text></table-column><table-column><paragraph-text type="body">directed acyclic graph</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">DBSCAN</paragraph-text></table-column><table-column><paragraph-text type="body">density-based spatial clustering of applications with noise, a non-parametric clustering algorithm which does not require specifying the number of clusters in advance</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">DNN</paragraph-text></table-column><table-column><paragraph-text type="body">deep neural network</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">EMD</paragraph-text></table-column><table-column><paragraph-text type="body">earth mover&apos;s distance/Wasserstein metric</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">FCL</paragraph-text></table-column><table-column><paragraph-text type="body">fully connected layer of an artificial neural network</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">FCNN</paragraph-text></table-column><table-column><paragraph-text type="body">fully convolutional neural network</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">GAN</paragraph-text></table-column><table-column><paragraph-text type="body">generative adversarial network</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">GMM</paragraph-text></table-column><table-column><paragraph-text type="body">Gaussian mixture model</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">GoogLeNet</paragraph-text></table-column><table-column><paragraph-text type="body">deep convolutional neural network</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">ICA</paragraph-text></table-column><table-column><paragraph-text type="body">independent component analysis</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">ICP</paragraph-text></table-column><table-column><paragraph-text type="body">iterative closest point, an algorithm for matching point clouds</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">ID3</paragraph-text></table-column><table-column><paragraph-text type="body">iterative Dichotomiser 3, an algorithm for learning decision trees</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Inception</paragraph-text></table-column><table-column><paragraph-text type="body">convolutional neural network which concatenates several filters of different sizes at the same level of the network</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">IoU</paragraph-text></table-column><table-column><paragraph-text type="body">intersection over union, a measure for quantifying the accuracy of an object detection algorithm</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">KDE</paragraph-text></table-column><table-column><paragraph-text type="body">kernel density estimation, an algorithm for estimating the probability density function of a random variable</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">kernel</paragraph-text></table-column><table-column><paragraph-text type="body">function which expresses an inner product of two inputs in another feature space</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">KLT</paragraph-text></table-column><table-column><paragraph-text type="body">Karhunen-Lo&#232;ve transform</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">K-Means</paragraph-text></table-column><table-column><paragraph-text type="body">data clustering algorithm</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">KNN</paragraph-text></table-column><table-column><paragraph-text type="body">K-nearest neighbour: a classification algorithm which, for a given data sample, chooses the k most similar samples from a training set, retrieves their respective class labels, and assigns a class label to the data sample by majority decision. Variant - 1NN, which is KNN for k=1</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">LASSO</paragraph-text></table-column><table-column><paragraph-text type="body">least absolute shrinkage and selection operator</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">LDA</paragraph-text></table-column><table-column><paragraph-text type="body">linear discriminant analysis</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">LeNet</paragraph-text></table-column><table-column><paragraph-text type="body">early CNN that firstly demonstrated the performance of CNNs on handwritten character recognition</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">LSTM</paragraph-text></table-column><table-column><paragraph-text type="body">long short-term memory, a recurrent neural network</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">LVQ</paragraph-text></table-column><table-column><paragraph-text type="body">learning vector quantisation</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">MDS</paragraph-text></table-column><table-column><paragraph-text type="body">multi-dimensional scaling</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">MLP</paragraph-text></table-column><table-column><paragraph-text type="body">multi-layer perceptron</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">MRF</paragraph-text></table-column><table-column><paragraph-text type="body">Markov random field</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">MS COCO</paragraph-text></table-column><table-column><paragraph-text type="body">annotated image data set</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">overfitting</paragraph-text></table-column><table-column><paragraph-text type="body">trained model suffers from overfitting if it performs well on the training data, but generalises poorly on new test data</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">PASCAL VOC</paragraph-text></table-column><table-column><paragraph-text type="body">collection of data sets for object detection</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">PCA</paragraph-text></table-column><table-column><paragraph-text type="body">principal component analysis</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">PDF</paragraph-text></table-column><table-column><paragraph-text type="body">probability density function</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Perceptron</paragraph-text></table-column><table-column><paragraph-text type="body">simple feed-forward neural network</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">RANSAC</paragraph-text></table-column><table-column><paragraph-text type="body">random sample consensus, a popular regression algorithm</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">RBF</paragraph-text></table-column><table-column><paragraph-text type="body">radial basis function</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Res-Net</paragraph-text></table-column><table-column><paragraph-text type="body">residual neural network, an artificial neural network having shortcuts / skip connections between different layers</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">R-CNN</paragraph-text></table-column><table-column><paragraph-text type="body">convolutional neural network using a region proposal algorithm for object detection (variants: fast R-CNN, faster R-CNN, cascade R-CNN)</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">ROC</paragraph-text></table-column><table-column><paragraph-text type="body">receiver-operating characteristics</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">RPM</paragraph-text></table-column><table-column><paragraph-text type="body">robust point matching, an algorithm for matching point clouds</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">RVM</paragraph-text></table-column><table-column><paragraph-text type="body">relevance vector machine</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">SOM</paragraph-text></table-column><table-column><paragraph-text type="body">self-organising maps, an algorithm for generating a low-dimensional representation of data while preserving the topological structure of the data</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">SSD</paragraph-text></table-column><table-column><paragraph-text type="body">single shot (multibox) detector, a neural network for object detection</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">SVD</paragraph-text></table-column><table-column><paragraph-text type="body">singular value decomposition</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">SVM</paragraph-text></table-column><table-column><paragraph-text type="body">support vector machine</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">test data</paragraph-text></table-column><table-column><paragraph-text type="body">data set different from the training data, used for testing the performance of a trained model</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">training data</paragraph-text></table-column><table-column><paragraph-text type="body">data set used for adjusting the parameters of the model during training</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">transformer</paragraph-text></table-column><table-column><paragraph-text type="body">deep learning model that uses attention to give different weights to individual parts of the input data</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">U-Net</paragraph-text></table-column><table-column><paragraph-text type="body">neural network having a specific layer structure</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">validation data</paragraph-text></table-column><table-column><paragraph-text type="body">data set used for testing the performance of the model during training</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">YOLO</paragraph-text></table-column><table-column><paragraph-text type="body">you only look once, an artificial neural network used for object detection (comes in various versions: YOLO v2, YOLO v3, etc.)</paragraph-text></table-column></table-row></table></section-body></glossary-of-terms></definition-item>
<definition-item date-revised="2024-08-01"><classification-symbol scheme="cpc">G06V10/72</classification-symbol><definition-title>Data preparation, e.g. statistical preprocessing of image or video features</definition-title><definition-statement><section-title>Definition statement</section-title><section-body><paragraph-text type="preamble">This place covers:</paragraph-text><paragraph-text type="body">Techniques that handle data quality issues such as data accuracy (obtaining the correct data entries), data completeness and data consistency (data written to a database must be valid according to all defined rules) in the context of image or video recognition or understanding.</paragraph-text><paragraph-text type="body">Examples of techniques classified here include:</paragraph-text><list><list-item><paragraph-text type="body">data cleaning, e.g. by filling in missing values, smoothing noisy data, identifying or removing outliers, resolving inconsistencies, etc.;</paragraph-text></list-item><list-item><paragraph-text type="body">compensating for missing data by supplying alternative default values;</paragraph-text></list-item><list-item><paragraph-text type="body">eliminating unreliable samples/outliers;</paragraph-text></list-item><list-item><paragraph-text type="body">data reduction, i.e. building a reduced representation of a data set through a reduction technique (e.g. PCA) or a numerosity reduction technique such as data aggregation;</paragraph-text></list-item><list-item><paragraph-text type="body">data normalisation, so that all attributes have an equal weight, e.g. min-max normalisation, z-scores, normalisation by decimal scaling, etc.</paragraph-text></list-item></list><paragraph-text type="body">Illustrative examples of subject matter classified in this place:</paragraph-text><paragraph-text type="body">1.</paragraph-text><paragraph-text type="body"><media id="media45.png" file-name="cpc-def-G06V-0045.png" type="png" preferred-width="6.84cm" preferred-height="10.16cm"/></paragraph-text><paragraph-text type="body">2.</paragraph-text><paragraph-text type="body"><media id="media46.png" file-name="cpc-def-G06V-0046.png" type="png" preferred-width="14.86cm" preferred-height="13.42cm"/></paragraph-text><paragraph-text type="body">Selection of vectors in a multi-dimensional space by considering the median of their subsets and discarding those above a certain distance range from the median.</paragraph-text></section-body></definition-statement><references><section-title>References</section-title><informative-references><section-title>Informative references</section-title><section-body><paragraph-text type="preamble">Attention is drawn to the following places, which may be of interest for search:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Pattern recognition or machine learning, using clustering</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/762</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Pattern recognition or machine learning, using classification</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/764</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Pattern recognition or machine learning, using regression</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/766</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Pattern recognition or machine learning, processing image or video features in feature spaces</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/77</class-ref></paragraph-text></table-column></table-row></table></section-body></informative-references></references></definition-item>
<definition-item date-revised="2024-08-01"><classification-symbol scheme="cpc">G06V10/74</classification-symbol><definition-title>Image or video pattern matching; Proximity measures in feature spaces</definition-title><definition-statement><section-title>Definition statement</section-title><section-body><paragraph-text type="preamble">This place covers:</paragraph-text><paragraph-text type="body">Matching, which involves comparison of pixels values, combinations thereof or features derived from them, in which one entity is considered as a template pattern and the other is the input pattern (template matching). The matching process may involve shifting, deforming or transforming patterns to accommodate distortions or positional errors.</paragraph-text><paragraph-text type="body">Histogram-based matching, wherein a histogram can be regarded as a quantised representation of the grey-level probability distribution function of pixels into intervals, called bins. Other statistical measures may be used for matching include probabilities, confidence intervals, etc.</paragraph-text><paragraph-text type="body">Variational techniques such as active contour models [ACM, or &quot;snakes&quot;], active shapes models [ASM] or active appearance models [AAM] in which a contour or a shape of the object is obtained by iterative matching.</paragraph-text><paragraph-text type="subheading">Notes &#8211; technical background</paragraph-text><paragraph-text type="body">These notes provide more information about the technical subject matter that is classified in this place:</paragraph-text><paragraph-text type="body">Aside pixels, other types of entities that may be involved in matching processes include lines, edges, object contours, object shapes, corners, key-points and statistical measures computed in a defined image neighbourhood.</paragraph-text><paragraph-text type="body">The matching may be performed in a different representation space than the image space, e.g. using an eigenspace representation of the image object, using shape manifolds, using a Hough transform, using a Fourier transform etc., which implies applying a transformation from the image to this representation space prior to matching. The transformation is usually chosen due to the invariant properties sought by the matching process (e.g. Fourier transformation offers invariance to translation of the pattern in the image).</paragraph-text><paragraph-text type="body">The proximity measures used during matching may include classical distances, such as Euclidian distances, or more involved distances, divergences or other measures between probability distribution functions or other statistical representations (e.g. mean, standard deviation, moments, kurtosis, Chi-square distance), for instance:</paragraph-text><list><list-item><paragraph-text type="body">Kullback-Leibler divergence;</paragraph-text></list-item><list-item><paragraph-text type="body">Mutual information;</paragraph-text></list-item><list-item><paragraph-text type="body">Bhattacharyya distance;</paragraph-text></list-item><list-item><paragraph-text type="body"> Hamming distance;</paragraph-text></list-item><list-item><paragraph-text type="body">Earth mover, Wasserstein distance;</paragraph-text></list-item><list-item><paragraph-text type="body">Chi-square distance;</paragraph-text></list-item><list-item><paragraph-text type="body">Hellinger distance.</paragraph-text></list-item></list><paragraph-text type="subheading">Notes &#8211; other classification places</paragraph-text><paragraph-text type="body">Group <class-ref scheme="cpc">G06V10/75</class-ref> covers more detailed aspects of the matching process, such as:</paragraph-text><list><list-item><paragraph-text type="body">its organisation: e.g. sequential, parallel, an initial matching with a small set of patterns each representing an entire set of patterns can be followed by a subsequent matching with all patterns in the most relevant sub-set; or matching in a randomised order, or in a predetermined order of relevance;</paragraph-text></list-item><list-item><paragraph-text type="body">precision-related aspects, e.g. rough matching with a large set of templates can be followed by a more elaborate matching with a few candidate matches; coarse-to-fine approaches at different scales of analysis, e.g. starting with a rough image resolution and then refining it to more precise resolutions;</paragraph-text></list-item><list-item><paragraph-text type="body">organisation of templates in dictionaries according to their properties in order to speed-up the matching process;</paragraph-text></list-item><list-item><paragraph-text type="body">matching using context, i.e. by taking into account secondary aspects not necessarily related to the intrinsic properties of the pattern, e.g. its proximity to other patterns, co-occurrences, etc.</paragraph-text></list-item></list><paragraph-text type="body">Illustrative examples of subject matter classified in this place:</paragraph-text><paragraph-text type="body">1.</paragraph-text><paragraph-text type="body"><media id="media47.png" file-name="cpc-def-G06V-0047.png" type="png" preferred-width="13.38cm" preferred-height="6.27cm"/></paragraph-text><paragraph-text type="body">2.</paragraph-text><paragraph-text type="body"><media id="media48.png" file-name="cpc-def-G06V-0048.png" type="png" preferred-width="15.9cm" preferred-height="6.56cm"/></paragraph-text><paragraph-text type="body">Eye detection by matching a circle/ellipse to the iris using a 2D projection onto a 3D representation of the eye.</paragraph-text><paragraph-text type="body">3.</paragraph-text><paragraph-text type="body"><media id="media49.png" file-name="cpc-def-G06V-0049.png" type="png" preferred-width="9.46cm" preferred-height="5.5cm"/></paragraph-text><paragraph-text type="body">Fitting an active appearance model [AAM] to the face using key points detected for prominent facial features.</paragraph-text></section-body></definition-statement><references><section-title>References</section-title><informative-references><section-title>Informative references</section-title><section-body><paragraph-text type="preamble">Attention is drawn to the following places, which may be of interest for search:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Global feature extraction by analysis of the whole pattern</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/42</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Descriptors for shape, contour or point-related descriptors, e.g. SIFT</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/46</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Local feature extraction by performing operations within image blocks or by using histograms</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/50</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Feature extraction related to texture</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/54</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Feature extraction related to colour</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/56</class-ref></paragraph-text></table-column></table-row></table></section-body></informative-references></references></definition-item>
<definition-item date-revised="2024-08-01"><classification-symbol scheme="cpc">G06V10/762</classification-symbol><definition-title>using clustering, e.g. of similar faces in social networks</definition-title><definition-statement><section-title>Definition statement</section-title><section-body><paragraph-text type="preamble">This place covers:</paragraph-text><paragraph-text type="body">Techniques of grouping patterns together in order to reveal a certain structure or a meaning in images or video.</paragraph-text><paragraph-text type="subheading">Notes &#8211; technical background</paragraph-text><paragraph-text type="body">These notes provide more information about the technical subject matter that is classified in this place:</paragraph-text><paragraph-text type="body">The object of techniques classified here is to identify groups of similar entities and to assign entities to a group (cluster) according to a measure of their similarity.</paragraph-text><paragraph-text type="body">Separability is determined by measuring the similarity or dissimilarity. Such techniques are usually performed in a high-dimensional feature space constructed by extracting features from the image or video, but can also be performed in the original domain, e.g. in the image domain in the case of image segmentation.</paragraph-text><paragraph-text type="body">Regarding the grouping of patterns, any pattern may belong exclusively to a single cluster (hard clustering) or it may belong simultaneously to more than one cluster up to a certain degree (fuzzy clustering) according to a similarity (or proximity) measure. In addition, depending on the clustering method used, proximity may be defined (a) between vectors, (b) between a vector and a set of vectors (or a cluster), and (c) between sets of vectors (or different clusters).</paragraph-text><paragraph-text type="body">Examples of proximity measures are: dissimilarity measures (based on l<sub>1</sub>, l<sub>2, </sub>and l<sub> </sub>norms), similarity measures (inner product, cosine, Pearson&apos;s correlation coefficient, Tanimoto distance, etc.).</paragraph-text><paragraph-text type="body">Clustering algorithms include:</paragraph-text><paragraph-text type="body">a) clustering based on statistical measures (which mainly employ numerical data) which adopt a cost function J related to possible groupings which is subject to a global or local optimisation criterion, and return a clustering that optimises J. Examples of such algorithms are:</paragraph-text><list><list-item><paragraph-text type="body">Hard clustering algorithms, where a vector belongs exclusively to a specific cluster, e.g. k-means, k-medoids, Linde-Buzo-Gray, ISODATA, DBSCAN, Neural Gas;</paragraph-text></list-item><list-item><paragraph-text type="body">Fuzzy clustering algorithms, where a vector belongs to a specific cluster up to a certain degree, e.g. fuzzy c-means, adaptive fuzzy C-shells [AFCS], fuzzy C quadric shells [FCQS], modified fuzzy C quadric shells [MFCQS];</paragraph-text></list-item><list-item><paragraph-text type="body">Probabilistic clustering algorithms, which follow Bayesian classification arguments and in which each vector is assigned to the cluster according to a probabilistic set-up, e.g. expectation maximisation [EM], Gaussian mixture model [GMM], mean-shift;</paragraph-text></list-item><list-item><paragraph-text type="body">b) Graph-based clustering algorithms, e.g. minimum spanning tree [MST] clustering, clustering based on directed trees, spectral clustering, graph-cut optimisation;</paragraph-text></list-item><list-item><paragraph-text type="body">c) Competitive learning algorithms for clustering, in which a set of representatives is selected and the goal is to move all representatives to regions of a vector space that are &quot;dense&quot; in terms of other vectors. Examples are leaky learning algorithms, self-organising maps [SOM], learning vector quantisation [LVQ].</paragraph-text></list-item></list><paragraph-text type="body">Hierarchical clustering is a popular technique in the class of graph-based clustering, with its agglomerative or divisive variants. Various criteria can be used for determining the groupings, such as those based on matrix theory involving dissimilarity matrices. </paragraph-text><paragraph-text type="body">Algorithms included in this scheme are:</paragraph-text><list><list-item><paragraph-text type="body">Single link algorithm;</paragraph-text></list-item><list-item><paragraph-text type="body">Complete link algorithm;</paragraph-text></list-item><list-item><paragraph-text type="body">Weighted pair group method average [WPGMA];</paragraph-text></list-item><list-item><paragraph-text type="body">Unweighted pair group method average [UPGMA];</paragraph-text></list-item><list-item><paragraph-text type="body">Weighted pair group method centroid [WPGMC];</paragraph-text></list-item><list-item><paragraph-text type="body">Ward or minimum variance algorithm.</paragraph-text></list-item></list><paragraph-text type="body">Illustrative examples of subject matter classified in this place:</paragraph-text><paragraph-text type="body">1.</paragraph-text><paragraph-text type="body"><media id="media50.png" file-name="cpc-def-G06V-0050.png" type="png" preferred-width="14.99cm" preferred-height="6.6cm"/></paragraph-text><paragraph-text type="body">2.</paragraph-text><paragraph-text type="body"><media id="media51.png" file-name="cpc-def-G06V-0051.png" type="png" preferred-width="15.16cm" preferred-height="5.76cm"/></paragraph-text><paragraph-text type="body">Clustering face images to detect affinity between persons using a graph-based clustering algorithm.</paragraph-text></section-body></definition-statement><references><section-title>References</section-title><informative-references><section-title>Informative references</section-title><section-body><paragraph-text type="preamble">Attention is drawn to the following places, which may be of interest for search:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Pattern recognition or machine learning, using classification</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/764</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Pattern recognition or machine learning, using regression</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/766</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Pattern recognition or machine learning, processing image features in feature spaces</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/77</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Pattern recognition or machine learning, fusion</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/80</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Information retrieval of still images; Clustering; Classification</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06F16/55</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Information retrieval of video data; Clustering; Classification</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06F16/75</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Image analysis; Segmentation; Edge detection</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06T7/10</class-ref></paragraph-text></table-column></table-row></table></section-body></informative-references></references><glossary-of-terms><section-title>Glossary of terms</section-title><section-body><paragraph-text type="preamble">In this place, the following terms or expressions are used with the meaning indicated:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">AFC</paragraph-text></table-column><table-column><paragraph-text type="body">adaptive fuzzy clustering</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">alternating cluster Estimation [ACE]</paragraph-text></table-column><table-column><paragraph-text type="body">when a partitioning with a specific shape is to be obtained, the user can define membership functions U(V, X) and prototype functions V(U, X). The clustering will be estimated as follows: <media id="media52.png" file-name="cpc-def-G06V-0052.png" type="png" preferred-width="7.15cm" preferred-height="5.97cm"/></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">AO</paragraph-text></table-column><table-column><paragraph-text type="body">alternative optimisation</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">CCM</paragraph-text></table-column><table-column><paragraph-text type="body">compatible cluster merging</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">clustering by graph partitioning</paragraph-text></table-column><table-column><paragraph-text type="body">a weighted graph is partitioned into disjoint subgraphs by removing a set of edges (cut). The basic objective function is to minimise the size of the cut, which is calculated as the sum of the weights of all edges belonging to the cut.</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">compatible cluster merging [CCM]compatible cluster merging</paragraph-text></table-column><table-column><paragraph-text type="body">starts with a sufficiently large number of clusters, and successively reduces the number by merging similar (compatible) clusters with respect to some criteria such as: <media id="media53.jpeg" file-name="cpc-def-G06V-0053.jpeg" type="jpeg" preferred-width="6.03cm" preferred-height="0.89cm"/>where: <media id="media54.jpeg" file-name="cpc-def-G06V-0054.jpeg" type="jpeg" preferred-width="2.33cm" preferred-height="0.64cm"/> the set of eigenvectors of the ith cluster.</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">DBSCAN</paragraph-text></table-column><table-column><paragraph-text type="body">density-based spatial clustering of applications with noise, a non-parametric clustering algorithm which does not require specifying the number of clusters in advance.</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">FACE</paragraph-text></table-column><table-column><paragraph-text type="body">Fast-ACE</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">FCQS (Fuzzy C-quadric shells)FCQSFuzzy C-quadric shells</paragraph-text></table-column><table-column><paragraph-text type="body">in case of quadric shaped clusters, FCQS can be employed for recovering them. For the estimation of the clusters the following cost function is minimised: <media id="media55.jpeg" file-name="cpc-def-G06V-0055.jpeg" type="jpeg" preferred-width="8.38cm" preferred-height="1.63cm"/></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">FCSS</paragraph-text></table-column><table-column><paragraph-text type="body">fuzzy C-spherical shells</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">FCV</paragraph-text></table-column><table-column><paragraph-text type="body">fuzzy C-varieties</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">FHV</paragraph-text></table-column><table-column><paragraph-text type="body">fuzzy hyper volume</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">fuzzy c-means clustering</paragraph-text></table-column><table-column><paragraph-text type="body">Choose a number of clusters.Assign randomly to each point coefficients for being in the clusters using the formula. <media id="media56.png" file-name="cpc-def-G06V-0056.png" type="png" preferred-width="4.36cm" preferred-height="1.35cm"/>Repeat until the algorithm has converged:Compute the centroid for each cluster, using the formula; <media id="media57.png" file-name="cpc-def-G06V-0057.png" type="png" preferred-width="3.26cm" preferred-height="1.02cm"/> For each point, compute its coefficients of being in the clusters.</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Gustafson-Kessel [GK]</paragraph-text></table-column><table-column><paragraph-text type="body">the GK algorithm associates each cluster with the cluster centre and its covariance. The main feature of GK clustering is the local adaptation of distance matrix in order to identify ellipsoidal clusters. The objective function of GK is: <media id="media58.png" file-name="cpc-def-G06V-0058.png" type="png" preferred-width="4.11cm" preferred-height="1.5cm"/>where: <media id="media59.png" file-name="cpc-def-G06V-0059.png" type="png" preferred-width="5.4cm" preferred-height="0.89cm"/></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">HCM</paragraph-text></table-column><table-column><paragraph-text type="body">hard c-Means</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">K-means clustering</paragraph-text></table-column><table-column><paragraph-text type="body"><media id="media60.jpeg" file-name="cpc-def-G06V-0060.jpeg" type="jpeg" preferred-width="10.82cm" preferred-height="5.93cm"/></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">KNN</paragraph-text></table-column><table-column><paragraph-text type="body">K-nearest neighbour; a classification algorithm which, for a given data sample, chooses the k most similar samples from a training set, retrieves their respective class labels, and assigns a class label to the data sample by majority decision; variant: 1NN, which is KNN for k=1.</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">LVQ</paragraph-text></table-column><table-column><paragraph-text type="body">learning vector quantisation</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">partitioning around medoids [PAM] &#8211; the most common realisation of k-medoid type algorithms partitioning around medoids PAM</paragraph-text></table-column><table-column><paragraph-text type="body">1. Initialise: randomly select k of the n data points as the medoids. 2. Associate each data point to the closest medoid. (&quot;closest&quot; here usually in a Euclidean/Manhattan distance sense). 3. For each medoid m.- For each non-medoid data point x. *Swap m and x and compute the total cost of the configuration. 4. Select the configuration with the lowest cost. 5. Repeat steps 2 to 4 until there is no change in the medoid.</paragraph-text></table-column></table-row></table></section-body></glossary-of-terms></definition-item>
<definition-item date-revised="2024-08-01"><classification-symbol scheme="cpc">G06V10/764</classification-symbol><definition-title>using classification, e.g. of video objects</definition-title><definition-statement><section-title>Definition statement</section-title><section-body><paragraph-text type="preamble">This place covers:</paragraph-text><paragraph-text type="body">Classification of images or videos to identify the category or set of categories (classes) to which a new observation belongs, on the basis of a training set of data containing observations (or instances) whose category membership is known.</paragraph-text><paragraph-text type="body">Novelty detection (e.g. classification of &quot;unseen&quot; observations), anomaly detection or outlier detection.</paragraph-text><paragraph-text type="subheading">Notes &#8211; technical background</paragraph-text><paragraph-text type="body">These notes provide more information about the technical subject matter that is classified in this place:</paragraph-text><paragraph-text type="body">Individual observations may be analysed into a set of quantifiable properties, known as explanatory variables or features. These properties may be categorical, ordinal, integer-valued, real-valued, etc. Other classifiers perform a class assignment by comparing current observations to previous observations by means of a similarity or distance function.</paragraph-text><paragraph-text type="body">A classifier can be parametric or non-parametric depending on the type of model adopted for the observations.</paragraph-text><paragraph-text type="body">Classification algorithms include those:</paragraph-text><list><list-item><paragraph-text type="body">based on the distance between a decision surface and training patterns, e.g. support vector machines [SVM];</paragraph-text></list-item><list-item><paragraph-text type="body">based on the distance between the pattern to be recognised and a reference, where the reference can be a prototype, a centroid of samples of the same class or the closest patterns from the same class or different classes, e.g. nearest-neighbour classification;</paragraph-text></list-item><list-item><paragraph-text type="body">based on a parametric, probabilistic model, where the model uses the Neyman-Pearson lemma, likelihood ratios, receiver operating characteristics [ROC], plotting the false acceptance rate [FAR] versus the false rejection rate [FRR], Bayesian classification, etc.;</paragraph-text></list-item><list-item><paragraph-text type="body">based on a graph-like or tree-like model, e.g. decision trees, random forests, etc. Examples are the classification and regression trees [CART], ID3 [Iterative Dichotomiser 3], C4.5, etc.</paragraph-text></list-item></list><paragraph-text type="body">The decision surface of the classifier may be a linear classifier or a non-linear classifier. Linear classifiers model the boundaries between different classes in the feature space as hyperplanes. Non-linear classifiers use, e.g. quadratic, polynomial or hyperbolic functions instead.</paragraph-text><paragraph-text type="body">Illustrative examples of subject matter classified in this place:</paragraph-text><paragraph-text type="body">1.</paragraph-text><paragraph-text type="body"><media id="media61.png" file-name="cpc-def-G06V-0061.png" type="png" preferred-width="13.63cm" preferred-height="12.13cm"/></paragraph-text><paragraph-text type="body">2.</paragraph-text><paragraph-text type="body"><media id="media62.jpeg" file-name="cpc-def-G06V-0062.jpeg" type="jpeg" preferred-width="13.19cm" preferred-height="11.96cm"/></paragraph-text><paragraph-text type="body">A linear support vector machine classifier which attempts to define a linear boundary between two classes (205, 210) of feature vectors originating from images containing &quot;persons&quot; and &quot;non-persons&quot;, such as to separate them into two different classes.</paragraph-text><paragraph-text type="body">3.</paragraph-text><paragraph-text type="body"><media id="media63.png" file-name="cpc-def-G06V-0063.png" type="png" preferred-width="11.56cm" preferred-height="7.49cm"/></paragraph-text><paragraph-text type="body">Decision tree classifying objects in the image data using an efficient hardware implementation with FIFO buffers.</paragraph-text></section-body></definition-statement><references><section-title>References</section-title><informative-references><section-title>Informative references</section-title><section-body><paragraph-text type="preamble">Attention is drawn to the following places, which may be of interest for search:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Pattern recognition or machine learning, using clustering</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/762</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Pattern recognition or machine learning, using regression</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/766</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Pattern recognition or machine learning, processing image features in feature spaces</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/77</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Pattern recognition or machine learning, fusion</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/80</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Information retrieval of still images; Clustering; Classification</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06F16/55</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Information retrieval of video data; Clustering; Classification</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06F16/75</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Image analysis; Segmentation; Edge detection</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06T7/10</class-ref></paragraph-text></table-column></table-row></table></section-body></informative-references></references><glossary-of-terms><section-title>Glossary of terms</section-title><section-body><paragraph-text type="preamble">In this place, the following terms or expressions are used with the meaning indicated:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">C4.5</paragraph-text></table-column><table-column><paragraph-text type="body">classification algorithm using a decision tree</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">CART</paragraph-text></table-column><table-column><paragraph-text type="body">classification and regression Tree</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">FAR</paragraph-text></table-column><table-column><paragraph-text type="body">false acceptance rate</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">FRR</paragraph-text></table-column><table-column><paragraph-text type="body">false rejection rate</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Gini impurity</paragraph-text></table-column><table-column><paragraph-text type="body">measure of how often a randomly chosen element from the set would be incorrectly labelled if it was randomly labelled according to the distribution of labels in the subset; usually used at the level of the nodes of tree-based classifiers.</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">ID3</paragraph-text></table-column><table-column><paragraph-text type="body">iterative Dichotomiser 3, a precursor of C4.5</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">ROC</paragraph-text></table-column><table-column><paragraph-text type="body">receiver operating characteristics</paragraph-text></table-column></table-row></table></section-body></glossary-of-terms></definition-item>
<definition-item date-revised="2024-08-01"><classification-symbol scheme="cpc">G06V10/766</classification-symbol><definition-title>using regression, e.g. by projecting features on hyperplanes</definition-title><definition-statement><section-title>Definition statement</section-title><section-body><paragraph-text type="preamble">This place covers:</paragraph-text><paragraph-text type="body">Techniques for image or video recognition or understanding using regression.</paragraph-text><paragraph-text type="subheading">Notes &#8211; technical background</paragraph-text><paragraph-text type="body">These notes provide more information about the technical subject matter that is classified in this place:</paragraph-text><paragraph-text type="body">The term &quot;regression&quot; refers to statistical techniques for estimating the relationships between a dependent variable (often called the &quot;outcome&quot; or &quot;response&quot; variable) and one or more independent variables (often called &quot;predictors&quot;, &quot;covariates&quot; or &quot;explanatory variables&quot;), where the variables model the underlying image or video data.</paragraph-text><paragraph-text type="body">Common forms of regression are:</paragraph-text><list><list-item><paragraph-text type="body">Linear regression - the model specification is that the dependent variables are a linear combination of the parameters (but need not be linear in the independent variables). The goal is to find a line (or a more complex linear combination) that most closely fits the data according to a specific mathematical criterion (e.g. by minimising the least-mean-squares criterion). For example, the method of ordinary least squares computes the unique line (or hyperplane) that minimises the sum of squared differences between the true data and that line (or hyperplane);</paragraph-text></list-item><list-item><paragraph-text type="body">Non-linear regression, e.g. polynomial, binomial, binary, logistic, multinomial logistic, etc.</paragraph-text></list-item></list><paragraph-text type="body">Illustrative example of subject matter classified in this place:</paragraph-text><paragraph-text type="body"><media id="media64.png" file-name="cpc-def-G06V-0064.png" type="png" preferred-width="11.62cm" preferred-height="4.93cm"/></paragraph-text><paragraph-text type="body">Example of adaptive regression analysis for classification.</paragraph-text></section-body></definition-statement><references><section-title>References</section-title><informative-references><section-title>Informative references</section-title><section-body><paragraph-text type="preamble">Attention is drawn to the following places, which may be of interest for search:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Pattern recognition or machine learning, using clustering</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/762</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Pattern recognition or machine learning, using classification</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/764</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Pattern recognition or machine learning, processing image features in feature spaces</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/77</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Pattern recognition or machine learning, fusion</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/80</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Digital computing; Complex mathematical operations</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06F17/10</class-ref></paragraph-text></table-column></table-row></table></section-body></informative-references></references><glossary-of-terms><section-title>Glossary of terms</section-title><section-body><paragraph-text type="preamble">In this place, the following terms or expressions are used with the meaning indicated:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">LMS</paragraph-text></table-column><table-column><paragraph-text type="body">least mean squares</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">RANSAC</paragraph-text></table-column><table-column><paragraph-text type="body">RANdom SAmple Consensus &#8211; an iterative algorithm for fitting a linear mathematical model such as a line or a plane through a set of points by eliminating the influence of outliers</paragraph-text></table-column></table-row></table></section-body></glossary-of-terms></definition-item>
<definition-item date-revised="2024-08-01"><classification-symbol scheme="cpc">G06V10/77</classification-symbol><definition-title>Processing image or video features in feature spaces; using data integration or data reduction, e.g. principal component analysis [PCA] or independent component analysis [ICA] or self-organising maps [SOM]; Blind source separation</definition-title><definition-statement><section-title>Definition statement</section-title><section-body><paragraph-text type="preamble">This place covers:</paragraph-text><paragraph-text type="body">Techniques which deal with the problem of reducing the dimensionality of a representation of features in high-dimensional feature spaces.</paragraph-text><paragraph-text type="subheading">Notes &#8211; technical background</paragraph-text><paragraph-text type="body">These notes provide more information about the technical subject matter that is classified in this place:</paragraph-text><paragraph-text type="body">The problem of reducing the dimensionality of a representation of features in high-dimensional feature spaces is sometimes referred to as &quot;the curse of dimensionality&quot;. Generally, having to consider too many features increases the requirements regarding processing power and memory capacity; moreover, available data samples may be too sparsely distributed in a high-dimensional feature space for reliably recognising patterns and the number of training samples, which are necessary to obtain a good estimate of the actual data distribution, increases exponentially. The distances between randomly chosen pairs of training samples can be expected to exhibit little differences, causing a nearest neighbour search to become unreliable the more dimensions the feature space has.</paragraph-text><paragraph-text type="body">Different types of analysis can be considered:</paragraph-text><list><list-item><paragraph-text type="body">based on a discrimination criterion, e.g. discriminant analysis such as linear discriminant analysis [LDA];</paragraph-text></list-item><list-item><paragraph-text type="body">based on evaluating a naturality criterion, non-negative matrix factorisation;</paragraph-text></list-item><list-item><paragraph-text type="body">based on an approximation criterion, e.g. principal component analysis [PCA];</paragraph-text></list-item><list-item><paragraph-text type="body">based on a separation criterion, e.g. independent component analysis [ICA];</paragraph-text></list-item><list-item><paragraph-text type="body">measuring the statistical independence, e.g. mutual information;</paragraph-text></list-item><list-item><paragraph-text type="body">decorrelating the data in the feature space;</paragraph-text></list-item><list-item><paragraph-text type="body">enforcing sparsity or performing a domain transformation, or evaluating a sparsity criterion, e.g. representations with an overcomplete basis;</paragraph-text></list-item><list-item><paragraph-text type="body">based on topology preservation, e.g. multidimensional scaling, self-organising maps [SOM].</paragraph-text></list-item></list><paragraph-text type="body">Another way of dealing with the problem is to integrate or reduce data by deriving representatives through clustering.</paragraph-text><paragraph-text type="body">A further concept covered by this group is the blind source separation [BSS] which involves estimating individual source components from mixtures of multiple sources, e.g. blended images that are obtained by superimposing one image onto another, or an image that is deteriorated by a noise process.</paragraph-text><paragraph-text type="body">The reduction of the representation by principal component analysis [PCA] has been extensively applied in various application-related contexts, one example being face recognition (&quot;Eigenfaces&quot;).</paragraph-text><paragraph-text type="subheading">Notes &#8211; other classification places</paragraph-text><paragraph-text type="body">Various subgroups cover further aspects relating to processing features in high-dimensional feature spaces.</paragraph-text><paragraph-text type="body">In particular, group <class-ref scheme="cpc">G06V10/771</class-ref> covers techniques relating to feature selection, e.g. selecting those features which are the most representative from a multi-dimensional feature space. Well-known ways to carry out feature selection are:</paragraph-text><list><list-item><paragraph-text type="body">by ranking or filtering the set of features, e.g. using a statistical measure such as variance or cross-correlation;</paragraph-text></list-item><list-item><paragraph-text type="body">by evaluating different subsets according to an optimisation criterion such as class separability in forward selection or backward elimination;</paragraph-text></list-item><list-item><paragraph-text type="body">using evolutionary computational techniques, such as genetic algorithms.</paragraph-text></list-item></list><paragraph-text type="body">Group <class-ref scheme="cpc">G06V10/772</class-ref> covers techniques for determining representative reference patterns, e.g. by averaging or distorting patterns, or for generating dictionaries, i.e. sets of templates which are usually organised efficiently for different purposes, such as matching.</paragraph-text><paragraph-text type="body">Group <class-ref scheme="cpc">G06V10/776</class-ref> covers techniques for validation and performance evaluation. They usually involve considerations of partitioning the available data into a training set to be used for training a classification model, and a validation set used to assess the validity of the classification and to evaluate its performance.</paragraph-text><paragraph-text type="body">Group <class-ref scheme="cpc">G06V10/778</class-ref> covers techniques for active pattern learning, e.g. online learning.</paragraph-text><paragraph-text type="body">Group <class-ref scheme="cpc">G06V10/80</class-ref> covers techniques for fusion, i.e. combining data from various sources at the sensor level, pre-processing, feature extraction or classification level, mainly to improve the performance of a pattern recognition system for images or video.</paragraph-text></section-body></definition-statement><references><section-title>References</section-title><informative-references><section-title>Informative references</section-title><section-body><paragraph-text type="preamble">Attention is drawn to the following places, which may be of interest for search:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Pattern recognition or machine learning, using clustering</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/762</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Pattern recognition or machine learning, using classification</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/764</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Pattern recognition or machine learning, using regression</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/766</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Pattern recognition or machine learning, fusion</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/80</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Information retrieval of still images; Clustering; Classification</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06F16/55</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Information retrieval of video data; Clustering; Classification</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06F16/75</class-ref></paragraph-text></table-column></table-row></table></section-body></informative-references></references><glossary-of-terms><section-title>Glossary of terms</section-title><section-body><paragraph-text type="preamble">In this place, the following terms or expressions are used with the meaning indicated:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">BSS</paragraph-text></table-column><table-column><paragraph-text type="body">blind source separation</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">eigenface</paragraph-text></table-column><table-column><paragraph-text type="body">name given to a set of eigenvectors obtained by principal component analysis when used in face recognition.</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">ICA</paragraph-text></table-column><table-column><paragraph-text type="body">independent component analysis</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">LDA</paragraph-text></table-column><table-column><paragraph-text type="body">linear discriminant analysis</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">MDS</paragraph-text></table-column><table-column><paragraph-text type="body">multidimensional scaling</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">PCA</paragraph-text></table-column><table-column><paragraph-text type="body">principal component analysis</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">SOM</paragraph-text></table-column><table-column><paragraph-text type="body">self-organising map</paragraph-text></table-column></table-row></table></section-body></glossary-of-terms></definition-item>
<definition-item date-revised="2024-08-01"><classification-symbol scheme="cpc">G06V10/776</classification-symbol><definition-title>Validation; Performance evaluation</definition-title><definition-statement><section-title>Definition statement</section-title><section-body><paragraph-text type="preamble">This place covers:</paragraph-text><paragraph-text type="body">Techniques for validation and performance evaluation of algorithms for image or video recognition or understanding.</paragraph-text><paragraph-text type="subheading">Notes &#8211; technical background</paragraph-text><paragraph-text type="body">These notes provide more information about the technical subject matter that is classified in this place:</paragraph-text><paragraph-text type="body">Validation and performance evaluation of algorithms for image or video recognition or understanding normally involve:</paragraph-text><list><list-item><paragraph-text type="body">a training data set which is a set of examples used to fit the parameters of a pattern recognition or machine learning model;</paragraph-text></list-item><list-item><paragraph-text type="body">a validation data set which provides an unbiased evaluation of the model fit on the training data set (while optionally tuning the model&apos;s parameters); and</paragraph-text></list-item><list-item><paragraph-text type="body">a test data set used to provide an unbiased evaluation of a final model. If the data in the test data set has never been used in training (for example in cross-validation), the test data set is also called a holdout data set.</paragraph-text></list-item></list><paragraph-text type="body">Common classification metrics to evaluate the models are true positive rate [TPR] or sensitivity, false positives rate [FPR] or fall-out, true negatives rate [TNR] or specificity, false negative rate [FNR] or miss rate, receiver operating characteristic [ROC] curves (TP rate divided by the FP rate), z-score, accuracy, precision (or positive predictive value), recall, negative predictive value, intersection over union [IoU], the Jaccard index (also referred to as Tanimoto index), etc. Other metrics are also possible, for instance regression metrics, explained variance, validation curves, detection error trade-off etc. In case of decision-tree learning, the compactness of a cluster, the purity of a cluster in terms of class labels, the minimum distance of samples from the class boundary, a calculated likelihood score, etc.</paragraph-text><paragraph-text type="body">In order to get more stable results and use all valuable data for training, a data set can be repeatedly split into several training and validation data sets. This strategy is known as cross-validation.</paragraph-text><paragraph-text type="body">The performance can be measured automatically, e.g. by a stochastic process such as when using bootstrapping, or by a human operator in the case of relevance feedback.</paragraph-text><paragraph-text type="body">Illustrative examples of subject matter classified in this place:</paragraph-text><paragraph-text type="body">1.</paragraph-text><paragraph-text type="body"><media id="media65.png" file-name="cpc-def-G06V-0065.png" type="png" preferred-width="16cm" preferred-height="10.98cm"/></paragraph-text><paragraph-text type="body">2.</paragraph-text><paragraph-text type="body"><media id="media66.png" file-name="cpc-def-G06V-0066.png" type="png" preferred-width="16cm" preferred-height="12.34cm"/></paragraph-text><paragraph-text type="body">Example of an iterative &quot;loss function&quot; calculation for four different recognition models trained with different subsets of images, which is indicative of the performance of the classification of each model.</paragraph-text></section-body></definition-statement><references><section-title>References</section-title><informative-references><section-title>Informative references</section-title><section-body><paragraph-text type="preamble">Attention is drawn to the following places, which may be of interest for search:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Pattern recognition or machine learning, using clustering</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/762</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Pattern recognition or machine learning, using classification</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/764</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Pattern recognition or machine learning, using regression</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/766</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Pattern recognition or machine learning, processing image features in feature spaces</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/77</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Pattern recognition or machine learning, fusion</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/80</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Digital computing; Complex mathematical operations</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06F17/10</class-ref></paragraph-text></table-column></table-row></table></section-body></informative-references></references><glossary-of-terms><section-title>Glossary of terms</section-title><section-body><paragraph-text type="preamble">In this place, the following terms or expressions are used with the meaning indicated:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">FNR</paragraph-text></table-column><table-column><paragraph-text type="body">false negative rate or miss rate</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">FPR</paragraph-text></table-column><table-column><paragraph-text type="body">false positive rate or fall-out rate</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">IoU</paragraph-text></table-column><table-column><paragraph-text type="body">intersection over union</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">ROC</paragraph-text></table-column><table-column><paragraph-text type="body">receiver operating characteristic</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">TNR</paragraph-text></table-column><table-column><paragraph-text type="body">true negative rate or specificity</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">TPR</paragraph-text></table-column><table-column><paragraph-text type="body">true positive rate or sensitivity</paragraph-text></table-column></table-row></table></section-body></glossary-of-terms></definition-item>
<definition-item date-revised="2024-08-01"><classification-symbol scheme="cpc">G06V10/778</classification-symbol><definition-title>Active pattern-learning, e.g. online learning of image or video features</definition-title><definition-statement><section-title>Definition statement</section-title><section-body><paragraph-text type="preamble">This place covers:</paragraph-text><paragraph-text type="body">Techniques for active pattern learning for image or video recognition or understanding which dynamically adapt a learning algorithm (e.g. a neural network), either by interactively querying a supervisor (user) or from some other information source, like a teacher module, to classify or learn new data. Examples of techniques classified here include:</paragraph-text><list><list-item><paragraph-text type="body">Membership query synthesis, where the learner generates its own instance from an underlying data set. For example, if the data set is pictures of humans and animals, the learner could send a clipped image of a leg to the teacher and query if this appendage belongs to an animal or human;</paragraph-text></list-item><list-item><paragraph-text type="body">Pool-based sampling, where instances are drawn from the entire data pool and assigned a confidence score, a measurement of how well the learner &quot;understands&quot; the data. The system then selects the instances for which it is the least confident and queries the teacher for the labels;</paragraph-text></list-item><list-item><paragraph-text type="body">Stream-based selective sampling, where unlabelled data samples are examined one at a time with the machine evaluating the informativeness of each item against its query parameters. The learner decides for itself whether to assign a label or query the teacher for each sample.</paragraph-text></list-item></list><paragraph-text type="body">Illustrative example of subject matter classified in this place:</paragraph-text><paragraph-text type="body"><media id="media67.png" file-name="cpc-def-G06V-0067.png" type="png" preferred-width="13.91cm" preferred-height="8.13cm"/></paragraph-text><paragraph-text type="body">Active learning of weights and biases at different stages of a convolutional neural network for image classification.</paragraph-text></section-body></definition-statement><references><section-title>References</section-title><informative-references><section-title>Informative references</section-title><section-body><paragraph-text type="preamble">Attention is drawn to the following places, which may be of interest for search:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Pattern recognition or machine learning, using clustering</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/762</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Pattern recognition or machine learning, using classification</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/764</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Pattern recognition or machine learning, using regression</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/766</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Pattern recognition or machine learning, processing image features in feature spaces</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/77</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Pattern recognition or machine learning, fusion</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/80</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Digital computing; Complex mathematical operations</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06F17/10</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Machine learning in general</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06N20/00</class-ref></paragraph-text></table-column></table-row></table></section-body></informative-references></references></definition-item>
<definition-item date-revised="2024-08-01"><classification-symbol scheme="cpc">G06V10/80</classification-symbol><definition-title>Fusion, i.e. combining data from various sources at the sensor level, preprocessing level, feature extraction level or classification level  (multimodal speaker identification or verification <class-ref scheme="cpc">G10L17/10</class-ref>)</definition-title><definition-statement><section-title>Definition statement</section-title><section-body><paragraph-text type="preamble">This place covers:</paragraph-text><paragraph-text type="body">Combining the information from several sources in order to form a unified representation for image or video recognition or understanding.</paragraph-text><paragraph-text type="subheading">Notes &#8211; technical background</paragraph-text><paragraph-text type="body">These notes provide more information about the technical subject matter that is classified in this place:</paragraph-text><paragraph-text type="body">A simple fusion process combines raw data from several sensors or several sensor modalities (e.g. fusing spatial and temporal data). Besides fusing the raw sensor data, it is also possible to first process the sensor data to extract features and then combine the extracted features into a joint feature vector. Alternatively, it is possible to fuse classification results, e.g. inputting the features from different sensor modalities to separate classifiers, receiving respective classification scores from each classifier, and combining the individual scores into a final classification result.</paragraph-text><paragraph-text type="body">Examples are probabilistic fusion, statistic fusion, fuzzy reasoning fusion, fusion based on evidence and belief theory, e.g. Dempster-Shafer, fusion by voting.</paragraph-text><paragraph-text type="body">Fusion can also be applied at different stages of a recognition system for different purposes, e.g. for dimensionality reduction, computing robustness, improving precision and certainty in the classification decisions, etc.</paragraph-text><paragraph-text type="body">Illustrative examples of subject matter classified in this place:</paragraph-text><paragraph-text type="body">1.</paragraph-text><paragraph-text type="body"><media id="media68.jpeg" file-name="cpc-def-G06V-0068.jpeg" type="jpeg" preferred-width="16cm" preferred-height="11.16cm"/></paragraph-text><paragraph-text type="body">Sensor-level fusion followed by classification.</paragraph-text><paragraph-text type="body">2.</paragraph-text><paragraph-text type="body"><media id="media69.jpeg" file-name="cpc-def-G06V-0069.jpeg" type="jpeg" preferred-width="11.66cm" preferred-height="12.7cm"/></paragraph-text><paragraph-text type="body">Feature-level fusion by combining colour, shape and texture representations.</paragraph-text></section-body></definition-statement><references><section-title>References</section-title><limiting-references><section-title>Limiting references</section-title><section-body><paragraph-text type="preamble">This place does not cover:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Multimodal speaker identification or verification</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G10L17/10</class-ref></paragraph-text></table-column></table-row></table></section-body></limiting-references><informative-references><section-title>Informative references</section-title><section-body><paragraph-text type="preamble">Attention is drawn to the following places, which may be of interest for search:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Pattern recognition or machine learning, using clustering</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/762</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Pattern recognition or machine learning, using classification</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/764</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Pattern recognition or machine learning, using regression</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/766</class-ref></paragraph-text></table-column></table-row></table></section-body></informative-references></references><glossary-of-terms><section-title>Glossary of terms</section-title><section-body><paragraph-text type="preamble">In this place, the following terms or expressions are used with the meaning indicated:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Dempster-Shafer</paragraph-text></table-column><table-column><paragraph-text type="body">general framework for reasoning with uncertainty which combines evidence from different sources and arrives at a degree of belief (represented by a mathematical object called belief function) that takes into account all the available evidence.</paragraph-text></table-column></table-row></table></section-body></glossary-of-terms></definition-item>
<definition-item date-revised="2024-08-01"><classification-symbol scheme="cpc">G06V10/82</classification-symbol><definition-title>using neural networks</definition-title><definition-statement><section-title>Definition statement</section-title><section-body><paragraph-text type="preamble">This place covers:</paragraph-text><paragraph-text type="body">Neural networks [NN] specially adapted for image or video recognition or understanding, in particular specific architectures and specific learning tasks for this purpose.</paragraph-text><paragraph-text type="subheading">Notes &#8211; technical background</paragraph-text><paragraph-text type="body">These notes provide more information about the technical subject matter that is classified in this place:</paragraph-text><paragraph-text type="body">Examples of architectures are:</paragraph-text><list><list-item><paragraph-text type="body">Attention based neural networks such as transformer architectures;</paragraph-text></list-item><list-item><paragraph-text type="body">Autoencoders consisting of encoder and decoder blocks, where the output has the same form as the input, i.e. input and outputs are both images for example;</paragraph-text></list-item><list-item><paragraph-text type="body">Convolutional neural networks consisting of repetitive convolutional and pooling layers;</paragraph-text></list-item><list-item><paragraph-text type="body">Pyramidal or multi-scale neural network, mostly of the convolutional type that process either differently scaled input images, have convolutional kernels of varying sizes, and/or contain skip connections from lower-level layers to higher level layers or the output layer;</paragraph-text></list-item><list-item><paragraph-text type="body">Recurrent neural networks, where the input data is sequential by nature. That is either the pixels of the input image are processed sequentially, or a plurality of image frames such as in videos are processed sequentially. Long-short-term-memory [LSTM] and gated recurrent units [GRU] are some specific examples of recurrent neural networks;</paragraph-text></list-item><list-item><paragraph-text type="body">Region proposal networks, where the main task is not only to correctly classify objects in an input image but also provide an indication where a specific object has been found. Example architectures are R-CNN and YOLO;</paragraph-text></list-item><list-item><paragraph-text type="body">Residual neural networks [ResNet] containing skip connections or shortcuts to jump over some layers;</paragraph-text></list-item><list-item><paragraph-text type="body">Siamese neural networks, that work on input pairs and consist of two identical neural networks that process each pair of the input and then merges the output to provide a judgement about the input pair, such as if they are belonging to the same class or not.</paragraph-text></list-item></list><paragraph-text type="body">Examples of learning tasks are:</paragraph-text><list><list-item><paragraph-text type="body">Adversarial learning such as in generative adversarial networks (GANs);</paragraph-text></list-item><list-item><paragraph-text type="body">Meta learning;</paragraph-text></list-item><list-item><paragraph-text type="body">Metric learning, learning a distant metric between two input objects mostly done with a Siamese neural network;</paragraph-text></list-item><list-item><paragraph-text type="body">Reinforcement learning, learning how to take optimal actions for performing a task, e.g. deep reinforcement learning for robotics, self-driving vehicles etc.;</paragraph-text></list-item><list-item><paragraph-text type="body">Representation or feature learning, learning representations or features from raw input, mostly done with some form of encoder-decoder architecture or simply by using intermediate representations of a classification network;</paragraph-text></list-item><list-item><paragraph-text type="body">Transfer or multitask learning, reusing a network trained on task A for task B or jointly training a neural network on multiple tasks.</paragraph-text></list-item></list><paragraph-text type="body">Illustrative examples of subject matter classified in this place:</paragraph-text><paragraph-text type="body">1A.</paragraph-text><paragraph-text type="body"><media id="media70.png" file-name="cpc-def-G06V-0070.png" type="png" preferred-width="5.84cm" preferred-height="7.15cm"/></paragraph-text><paragraph-text type="body">1B.</paragraph-text><paragraph-text type="body"><media id="media71.png" file-name="cpc-def-G06V-0071.png" type="png" preferred-width="5.1cm" preferred-height="7.22cm"/></paragraph-text><paragraph-text type="body">Siamese network showing not similar (left) and similar (right) input pairs.</paragraph-text><paragraph-text type="body">2.</paragraph-text><paragraph-text type="body"><media id="media72.png" file-name="cpc-def-G06V-0072.png" type="png" preferred-width="15.6cm" preferred-height="7.07cm"/></paragraph-text><paragraph-text type="body">Recurrent neural network for action recognition.</paragraph-text><paragraph-text type="body">3.</paragraph-text><paragraph-text type="body"><media id="media73.png" file-name="cpc-def-G06V-0073.png" type="png" preferred-width="10.16cm" preferred-height="10.58cm"/></paragraph-text><paragraph-text type="body">Region proposal neural network for region of interest (ROI) detection.</paragraph-text><paragraph-text type="body">4.</paragraph-text><paragraph-text type="body"><media id="media74.png" file-name="cpc-def-G06V-0074.png" type="png" preferred-width="12.42cm" preferred-height="8.04cm"/></paragraph-text><paragraph-text type="body">Adversarial learning with a generative adversarial neural network for object recognition on different backgrounds.</paragraph-text></section-body></definition-statement><references><section-title>References</section-title><informative-references><section-title>Informative references</section-title><section-body><paragraph-text type="preamble">Attention is drawn to the following places, which may be of interest for search:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Feature extraction related to a temporal dimension; Pattern tracking</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/62</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Pattern recognition or machine learning, using clustering</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/762</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Pattern recognition or machine learning, using classification</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/764</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Pattern recognition or machine learning, using regression</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/766</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Pattern recognition or machine learning, fusion</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/80</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Information retrieval of video data; Clustering; Classification</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06F16/75</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Computer systems based on biological models using neural networks</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06N3/02</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Computer systems using knowledge-based models; Inference methods</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06N5/04</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Machine learning</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06N20/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Motion image analysis</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06T7/20</class-ref></paragraph-text></table-column></table-row></table></section-body></informative-references></references><glossary-of-terms><section-title>Glossary of terms</section-title><section-body><paragraph-text type="preamble">In this place, the following terms or expressions are used with the meaning indicated:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">AE</paragraph-text></table-column><table-column><paragraph-text type="body">auto-encoder network</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">AlexNet</paragraph-text></table-column><table-column><paragraph-text type="body">CNN designed by Alex Krizhevsky et al.</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Backprop</paragraph-text></table-column><table-column><paragraph-text type="body">backpropagation, an algorithm for computing the gradient of the weights of an artificial neural network</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">BERT</paragraph-text></table-column><table-column><paragraph-text type="body">bidirectional encoder representations from transformers, a transformer based artificial neural network</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">CNN</paragraph-text></table-column><table-column><paragraph-text type="body">convolutional neural network, an artificial neural network that includes convolutional layers</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">DNN</paragraph-text></table-column><table-column><paragraph-text type="body">deep neural network</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">FCL</paragraph-text></table-column><table-column><paragraph-text type="body">fully connected layer of an artificial neural network</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">FCNN</paragraph-text></table-column><table-column><paragraph-text type="body">fully convolutional neural network</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">GAN</paragraph-text></table-column><table-column><paragraph-text type="body">generative adversarial network</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">GoogLeNet</paragraph-text></table-column><table-column><paragraph-text type="body">deep convolutional neural network</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Inception</paragraph-text></table-column><table-column><paragraph-text type="body">convolutional neural network which concatenates several filters of different sizes at the same level of the network</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">LeNet</paragraph-text></table-column><table-column><paragraph-text type="body">early CNN that firstly demonstrated the performance of CNNs on handwritten character recognition</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">LSTM</paragraph-text></table-column><table-column><paragraph-text type="body">long short-term memory, a recurrent neural network</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">MLP</paragraph-text></table-column><table-column><paragraph-text type="body">multi-layer perceptron</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">MS COCO</paragraph-text></table-column><table-column><paragraph-text type="body">annotated image data set</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Perceptron</paragraph-text></table-column><table-column><paragraph-text type="body">simple feed-forward neural network</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">RBF</paragraph-text></table-column><table-column><paragraph-text type="body">radial basis function</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">R-CNN</paragraph-text></table-column><table-column><paragraph-text type="body">convolutional neural network using a region proposal algorithm for object detection (variants: fast R-CNN, faster R-CNN, cascade R-CNN)</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Res-Net</paragraph-text></table-column><table-column><paragraph-text type="body">residual neural network, an artificial neural network having shortcuts / skip connections between different layers</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">SOM</paragraph-text></table-column><table-column><paragraph-text type="body">self-organising maps, an algorithm for generating a low-dimensional representation of data while preserving the topological structure of the data</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">SSD</paragraph-text></table-column><table-column><paragraph-text type="body">single shot (multibox) detector, a neural network for object detection</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">U-Net</paragraph-text></table-column><table-column><paragraph-text type="body">neural network having a specific layer structure</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">YOLO</paragraph-text></table-column><table-column><paragraph-text type="body">you only look once, an artificial neural network used for object detection (comes in various versions: YOLO v2, YOLO v3, etc.)</paragraph-text></table-column></table-row></table></section-body></glossary-of-terms></definition-item>
<definition-item date-revised="2024-08-01"><classification-symbol scheme="cpc">G06V10/84</classification-symbol><definition-title>using probabilistic graphical models from image or video features, e.g. Markov models or Bayesian networks</definition-title><definition-statement><section-title>Definition statement</section-title><section-body><paragraph-text type="preamble">This place covers:</paragraph-text><paragraph-text type="body">Graphical models for image or video recognition or understanding, with states modelled as nodes in a graph and transitions between states as graph edges, and where it is assumed that the future state of a system depends on the present state.</paragraph-text><paragraph-text type="subheading">Notes &#8211; technical background</paragraph-text><paragraph-text type="body">These notes provide more information about the technical subject matter that is classified in this place:</paragraph-text><paragraph-text type="body">Examples of graphical models include:</paragraph-text><list><list-item><paragraph-text type="body">probabilistic models such as state machines, Bayesian networks, dynamic Bayesian networks, tree-structured models, probabilistic latent semantic analysis (PLSA), conditional random fields, Markov models and variations, e.g. hidden Markov models, Markov random fields, partially observable Markov models, Markov decision processes, or variable length Markov models;</paragraph-text></list-item><list-item><paragraph-text type="body">inference using graphical models, e.g. by the junction tree algorithm, factor graphs, belief propagation, message passing, Gibbs sampling, variational inference, Monte Carlo inference, Markov chains;</paragraph-text></list-item><list-item><paragraph-text type="body">learning using graphical models, e.g. by expectation maximisation, latent variable methods, Baum-Welch algorithm, Viterbi training, forward-backward propagation, Monte Carlo methods;</paragraph-text></list-item><list-item><paragraph-text type="body">learning the graphical structure of the model itself.</paragraph-text></list-item></list><paragraph-text type="body">Applications include learning spatial context for object detection, learning spatio-temporal events for activity recognition, gesture recognition, video segmentation and understanding, etc.</paragraph-text><paragraph-text type="body">Illustrative examples of subject matter classified in this place:</paragraph-text><paragraph-text type="body">1.</paragraph-text><paragraph-text type="body"><media id="media75.png" file-name="cpc-def-G06V-0075.png" type="png" preferred-width="4.87cm" preferred-height="6.41cm"/></paragraph-text><paragraph-text type="body">2.</paragraph-text><paragraph-text type="body"><media id="media76.png" file-name="cpc-def-G06V-0076.png" type="png" preferred-width="8.87cm" preferred-height="7.13cm"/></paragraph-text><paragraph-text type="body">Human activity recognition using a hidden Markov model [HMM].</paragraph-text></section-body></definition-statement><references><section-title>References</section-title><informative-references><section-title>Informative references</section-title><section-body><paragraph-text type="preamble">Attention is drawn to the following places, which may be of interest for search:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Feature extraction related to a temporal dimension; Pattern tracking</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/62</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Pattern recognition or machine learning, using clustering</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/762</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Pattern recognition or machine learning, using classification</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/764</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Pattern recognition or machine learning, using regression</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/766</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Pattern recognition or machine learning, fusion</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/80</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Information retrieval of video data; Clustering; Classification</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06F16/75</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Motion image analysis</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06T7/20</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Speaker identification and verification; Hidden Markov models [HMM]</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G10L17/16</class-ref></paragraph-text></table-column></table-row></table></section-body></informative-references></references><glossary-of-terms><section-title>Glossary of terms</section-title><section-body><paragraph-text type="preamble">In this place, the following terms or expressions are used with the meaning indicated:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">EM</paragraph-text></table-column><table-column><paragraph-text type="body">expectation maximisation, iterative method to find (local) maximum likelihood or maximum a posteriori [MAP] estimates of parameters in statistical models, where the model depends on unobserved latent variables.</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">HMM</paragraph-text></table-column><table-column><paragraph-text type="body">hidden Markov model, statistical Markov model in which the system being modelled is assumed to be a Markov process with unobservable (&quot;hidden&quot;) states.</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">PLSA</paragraph-text></table-column><table-column><paragraph-text type="body">probabilistic latent semantic analysis, a representation model in which the probability of co-occurrence of data is modelled as a mixture of conditionally independent multinomial distributions.</paragraph-text></table-column></table-row></table></section-body></glossary-of-terms></definition-item>
<definition-item date-revised="2024-08-01"><classification-symbol scheme="cpc">G06V10/86</classification-symbol><definition-title>using syntactic or structural representations of the image or video pattern, e.g. symbolic string recognition; using graph matching</definition-title><definition-statement><section-title>Definition statement</section-title><section-body><paragraph-text type="preamble">This place covers:</paragraph-text><paragraph-text type="body">Methods and arrangements which use syntactic or structural representations of the image or video patterns for recognition or understanding where objects can be represented by a variable-cardinality set of symbolic, nominal features.</paragraph-text><paragraph-text type="body">Syntactic pattern recognition which represents structures by means of strings of symbols and formal language analysis algorithms, such as parsing with grammars.Recognition based on graph matching to find relations between patterns.</paragraph-text><paragraph-text type="subheading">Notes &#8211; technical background</paragraph-text><paragraph-text type="body">These notes provide more information about the technical subject matter that is classified in this place:</paragraph-text><paragraph-text type="body">These methods allow the representation of structures by taking into account interrelationships between patterns or their attributes.</paragraph-text><paragraph-text type="body">Illustrative examples of subject matter classified in this place:</paragraph-text><paragraph-text type="body">1.</paragraph-text><paragraph-text type="body"><media id="media77.png" file-name="cpc-def-G06V-0077.png" type="png" preferred-width="6.56cm" preferred-height="5.46cm"/></paragraph-text><paragraph-text type="body">2.</paragraph-text><paragraph-text type="body"><media id="media78.png" file-name="cpc-def-G06V-0078.png" type="png" preferred-width="5.8cm" preferred-height="5.82cm"/></paragraph-text><paragraph-text type="body">Face recognition by elastic bunch graph matching.</paragraph-text></section-body></definition-statement><references><section-title>References</section-title><informative-references><section-title>Informative references</section-title><section-body><paragraph-text type="preamble">Attention is drawn to the following places, which may be of interest for search:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Pattern recognition or machine learning, using clustering</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/762</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Pattern recognition or machine learning, using classification</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/764</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Complex mathematical operations</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06F17/10</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Handling natural language data</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06F40/00</class-ref></paragraph-text></table-column></table-row></table></section-body></informative-references></references><glossary-of-terms><section-title>Glossary of terms</section-title><section-body><paragraph-text type="preamble">In this place, the following terms or expressions are used with the meaning indicated:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Hopcroft-Karp algorithm</paragraph-text></table-column><table-column><paragraph-text type="body">graph matching algorithm that takes as input a bipartite graph and produces as output a maximum cardinality matching &#8211; a set of as many edges as possible with the property that no two edges share an endpoint.</paragraph-text></table-column></table-row></table></section-body></glossary-of-terms></definition-item>
<definition-item date-revised="2024-08-01"><classification-symbol scheme="cpc">G06V10/88</classification-symbol><definition-title>Image or video recognition using optical means, e.g. reference filters, holographic masks, frequency domain filters or spatial domain filters</definition-title><definition-statement><section-title>Definition statement</section-title><section-body><paragraph-text type="preamble">This place covers:</paragraph-text><paragraph-text type="body">Optical devices which are specially adapted for recognising patterns; methods making use of these devices.</paragraph-text><paragraph-text type="subheading">Notes &#8211; technical background</paragraph-text><paragraph-text type="body">These notes provide more information about the technical subject matter that is classified in this place:</paragraph-text><paragraph-text type="body">Detection and recognition of an object usually involves optical correlation between an input (optical) image and either a reference optical mask containing the object of interest, or an optical image of the reference object. The convolution is efficiently performed as superposition of these images (dot products) in the Fourier domain, the latter representation being obtained by a lens or a system of lenses.</paragraph-text><paragraph-text type="body">At least one element of a processing chain for recognising patterns in image and video data may be an optical hardware component, e.g. a ring-wedge detector, or an optical correlator to determine the similarity between two patterns. The effect of the optical element may be present in both spatial domain and in the frequency domain.</paragraph-text><paragraph-text type="body">Typically, the optical elements used in these approaches are: specially designed filter masks, spatial light modulators [SLM], holographic masks, phase-only filters, acousto-optic cells, waveguides, polarisers, etc.</paragraph-text><paragraph-text type="body">Illustrative example of subject matter classified in this place:</paragraph-text><paragraph-text type="body"><media id="media79.png" file-name="cpc-def-G06V-0079.png" type="png" preferred-width="12.23cm" preferred-height="7.15cm"/></paragraph-text><paragraph-text type="body">Example of an optical correlator.</paragraph-text></section-body></definition-statement><references><section-title>References</section-title><informative-references><section-title>Informative references</section-title><section-body><paragraph-text type="preamble">Attention is drawn to the following places, which may be of interest for search:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Arrangements for image or video recognition or understanding using pattern recognition or machine learning</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/70</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Optical elements per se</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G02B</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Diffraction optics, systems using spatial filters</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G02B27/46</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Spatial light modulators per se</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G02F</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Optical or electro-optical devices for carrying out mathematical operations</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06E3/00</class-ref></paragraph-text></table-column></table-row></table></section-body></informative-references></references><glossary-of-terms><section-title>Glossary of terms</section-title><section-body><paragraph-text type="preamble">In this place, the following terms or expressions are used with the meaning indicated:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">SLM</paragraph-text></table-column><table-column><paragraph-text type="body">spatial light modulator</paragraph-text></table-column></table-row></table></section-body></glossary-of-terms></definition-item>
<definition-item date-revised="2024-08-01"><classification-symbol scheme="cpc">G06V10/94</classification-symbol><definition-title>Hardware or software architectures specially adapted for image or video understanding</definition-title><definition-statement><section-title>Definition statement</section-title><section-body><paragraph-text type="preamble">This place covers:</paragraph-text><paragraph-text type="body">Hardware solutions (e.g. individual electronic circuits or networks of interacting electronic devices) or software architectures (e.g. data structures or software libraries), which are specially adapted for pattern recognition or image or video understanding.</paragraph-text><paragraph-text type="subheading">Notes &#8211; technical background</paragraph-text><paragraph-text type="body">These notes provide more information about the technical subject matter that is classified in this place:</paragraph-text><paragraph-text type="body">An example of special adaptation of software might include software arranged to perform a sequence of mathematical operations, which can run particularly efficiently on a particular graphical processing unit [GPU].</paragraph-text><paragraph-text type="body">An example of special adaptation of hardware might be a processor which is designed to perform operations, that are particularly relevant for pattern recognition (e.g. convolutions), in a power-efficient manner. Another example might be a hardware interface, which makes it possible to communicate an extracted visual pattern very efficiently (in terms of speed or bandwidth) to a server for further processing.</paragraph-text><paragraph-text type="body">Illustrative example of subject matter classified in this place:</paragraph-text><paragraph-text type="body"><media id="media80.png" file-name="cpc-def-G06V-0080.png" type="png" preferred-width="8.34cm" preferred-height="5.88cm"/></paragraph-text><paragraph-text type="body">Distributed pattern recognition system in which different resources are placed at different geographical locations.</paragraph-text></section-body></definition-statement><references><section-title>References</section-title><application-references><section-title>Application-oriented references</section-title><section-body><paragraph-text type="preamble">Examples of places where the subject matter of this place is covered when specially adapted, used for a particular purpose, or incorporated in a larger system:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Optical devices for pattern recognition</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/88</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Sensors specially adapted for fingerprint or palmprint recognition</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V40/13</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Sensors specially adapted for recognising vascular patterns</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V40/145</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Sensors specially adapted for eye recognition</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V40/19</class-ref></paragraph-text></table-column></table-row></table></section-body></application-references><informative-references><section-title>Informative references</section-title><section-body><paragraph-text type="preamble">Attention is drawn to the following places, which may be of interest for search:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Processor architectures for image data processing</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06T1/20</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Hardware or software architectures for video coding</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">H04N19/42</class-ref></paragraph-text></table-column></table-row></table></section-body></informative-references></references><glossary-of-terms><section-title>Glossary of terms</section-title><section-body><paragraph-text type="preamble">In this place, the following terms or expressions are used with the meaning indicated:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Core</paragraph-text></table-column><table-column><paragraph-text type="body">CPU cores in an individual physical CPU</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">CPU</paragraph-text></table-column><table-column><paragraph-text type="body">central processing unit</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">DSP</paragraph-text></table-column><table-column><paragraph-text type="body">digital signal processor</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">edge device</paragraph-text></table-column><table-column><paragraph-text type="body">device that provides an entry point to a digital communication network, such as routers, switches, etc.</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">GPU</paragraph-text></table-column><table-column><paragraph-text type="body">graphical processing unit</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">LAN</paragraph-text></table-column><table-column><paragraph-text type="body">local area network</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">WAN</paragraph-text></table-column><table-column><paragraph-text type="body">wide area network</paragraph-text></table-column></table-row></table></section-body></glossary-of-terms></definition-item>
<definition-item date-revised="2024-08-01"><classification-symbol scheme="cpc">G06V10/96</classification-symbol><definition-title>Management of image or video recognition tasks</definition-title><definition-statement><section-title>Definition statement</section-title><section-body><paragraph-text type="preamble">This place covers:</paragraph-text><paragraph-text type="body">Processes and devices which control the execution of pattern recognition algorithms.</paragraph-text><paragraph-text type="subheading">Notes &#8211; technical background</paragraph-text><paragraph-text type="body">These notes provide more information about the technical subject matter that is classified in this place:</paragraph-text><paragraph-text type="body">The control may cause the result of a pattern recognition algorithm to be available within a predetermined time frame, e.g. by prioritising pattern recognition tasks over other tasks, by delegating tasks to processors which are currently idle, by lowering the frame rate (e.g. discarding every other image frame), or by altering the resolution of the image.</paragraph-text><paragraph-text type="body">The control may also take the urgency of pattern recognition tasks into account; for example, an autonomous vehicle could prioritise tracking a detected pedestrian over tracking other more distant objects.</paragraph-text><paragraph-text type="body">The control may also be adaptive to the available processing power or to the available bandwidth, e.g. by selecting simpler and less accurate algorithms when being executed on a mobile device, by dynamically altering between batch processing and real-time processing depending on predetermined criteria, or by skipping dispensable pre-processing steps.</paragraph-text><paragraph-text type="body">Illustrative example of subject matter classified in this place:</paragraph-text><paragraph-text type="body"><media id="media81.png" file-name="cpc-def-G06V-0081.png" type="png" preferred-width="16cm" preferred-height="8.8cm"/></paragraph-text><paragraph-text type="body">Task scheduling to accommodate different steps, e.g. image recording, exposure control, recognition and video output.</paragraph-text></section-body></definition-statement><references><section-title>References</section-title><application-references><section-title>Application-oriented references</section-title><section-body><paragraph-text type="preamble">Examples of places where the subject matter of this place is covered when specially adapted, used for a particular purpose, or incorporated in a larger system:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Recognition of scenes or scene-specific elements</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V20/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Character recognition; Recognising digital ink; Document-oriented image-based pattern recognition</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V30/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Image or video recognition or understanding of human-related, animal-related or biometric patterns in image or video data</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V40/00</class-ref></paragraph-text></table-column></table-row></table></section-body></application-references><informative-references><section-title>Informative references</section-title><section-body><paragraph-text type="preamble">Attention is drawn to the following places, which may be of interest for search:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Image or video recognition or understanding, algorithms using pattern recognition or machine learning</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/70</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Allocating computer resources to programs</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06F9/50</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Data processing for complex mathematical operations</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06F17/10</class-ref></paragraph-text></table-column></table-row></table></section-body></informative-references></references></definition-item>
<definition-item date-revised="2024-08-01"><classification-symbol scheme="cpc">G06V10/98</classification-symbol><definition-title>Detection or correction of errors, e.g. by rescanning the pattern or by human intervention; Evaluation of the quality of the acquired patterns</definition-title><definition-statement><section-title>Definition statement</section-title><section-body><paragraph-text type="preamble">This place covers:</paragraph-text><paragraph-text type="body">Methods and arrangements for detecting or correcting errors in an acquired pattern.</paragraph-text><paragraph-text type="subheading">Notes &#8211; technical background</paragraph-text><paragraph-text type="body">These notes provide more information about the technical subject matter that is classified in this place:</paragraph-text><paragraph-text type="body">Regarding error correction in acquired images, errors may be detected or corrected automatically, or with the help of an operator. The process may also be semi-automatic; for example, it may involve the displaying of an image on a graphical user interface in order to prompt human intervention, if the quality of the image is insufficient for successful recognition.</paragraph-text><paragraph-text type="body">Detecting errors, in particular, may comprise evaluating the quality of given image or video data in order to assess its suitability for analysis by an automated pattern recognition process. Typical quality criteria are sharpness/blurriness, resolution, contrast and brightness.</paragraph-text><paragraph-text type="body">More advanced quality assessment algorithms check the image for objects that are only partly visible (e.g. due to occlusions or because parts of the object have moved outside the field of view). These algorithms may also detect the presence of clutter or shadows, and check whether the position and orientation of the object are as expected, or they determine whether the image complies with quality standards of a particular technical application (e.g. the visibility of the eyes in case of biometric authentication).</paragraph-text><paragraph-text type="body">If the quality of the image or video data is considered insufficient, the process or device may attempt to improve the quality by capturing a further image, potentially by changing parameter settings of the image capturing process (e.g. by switching on active infrared illumination when the captured image is found to be too dark) or by providing the user with instructions on how to re-capture the image.</paragraph-text><paragraph-text type="body">Illustrative examples of subject matter classified in this place:</paragraph-text><paragraph-text type="body">1A.</paragraph-text><paragraph-text type="body"><media id="media82.png" file-name="cpc-def-G06V-0082.png" type="png" preferred-width="13.93cm" preferred-height="5.31cm"/></paragraph-text><paragraph-text type="body">1B.</paragraph-text><paragraph-text type="body"><media id="media83.png" file-name="cpc-def-G06V-0083.png" type="png" preferred-width="14.65cm" preferred-height="5.55cm"/></paragraph-text><paragraph-text type="body">The quality of the acquisition of a fingerprint image is influenced by a hair present on the sensor. A stand-alone image of the hair is either removed (left) from the acquired image or the regions containing the hair are discarded in the subsequent analysis (right).</paragraph-text><paragraph-text type="body">2.</paragraph-text><paragraph-text type="body"><media id="media84.jpeg" file-name="cpc-def-G06V-0084.jpeg" type="jpeg" preferred-width="9.02cm" preferred-height="11.37cm"/></paragraph-text><paragraph-text type="body">Flowchart according to which face recognition is performed only when the acquired image fulfils a predetermined quality standard.</paragraph-text></section-body></definition-statement><references><section-title>References</section-title><application-references><section-title>Application-oriented references</section-title><section-body><paragraph-text type="preamble">Examples of places where the subject matter of this place is covered when specially adapted, used for a particular purpose, or incorporated in a larger system:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Recognition of scenes or scene-specific elements</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V20/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Recognition of human or animal bodies within image or video data</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V40/10</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Recognition of fingerprints or palmprints</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V40/12</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Recognition of vascular patterns</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V40/14</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Recognition of human faces, e.g. facial parts, sketches or expressions within images or video data</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V40/16</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Recognition of eye characteristics within image or video data, e.g. of the iris</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V40/18</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Maintenance of biometric data or enrolment thereof</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V40/50</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Multimodal biometrics</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V40/70</class-ref></paragraph-text></table-column></table-row></table></section-body></application-references><informative-references><section-title>Informative references</section-title><section-body><paragraph-text type="preamble">Attention is drawn to the following places, which may be of interest for search:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Validation or performance evaluation for pattern recognition</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/776</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Investigating the presence of flaws or contamination by the use of optical means</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G01N21/88</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Investigating the presence of flaws in materials by the use of thermal means</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G01N25/72</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Image analysis</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06T7/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Arrangements for detecting and preventing errors in the information received</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">H04L1/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Details of television systems</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">H04N5/00</class-ref></paragraph-text></table-column></table-row></table></section-body></informative-references></references><glossary-of-terms><section-title>Glossary of terms</section-title><section-body><paragraph-text type="preamble">In this place, the following terms or expressions are used with the meaning indicated:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">quality</paragraph-text></table-column><table-column><paragraph-text type="body">quality within the meaning of the present group is a property of the acquired pattern insofar as it has an effect on the accuracy or performance of the pattern recognition process</paragraph-text></table-column></table-row></table></section-body></glossary-of-terms></definition-item>
<definition-item date-revised="2024-08-01"><classification-symbol scheme="cpc">G06V20/00</classification-symbol><definition-title>Scenes; Scene-specific elements  (control of digital cameras <class-ref scheme="cpc">H04N23/60</class-ref>)</definition-title><definition-statement><section-title>Definition statement</section-title><section-body><paragraph-text type="preamble">This place covers:</paragraph-text><paragraph-text type="body">Scene-specific image or video recognition or understanding according to the category of scene that is perceived by the observer or the scene-specific processing performed.</paragraph-text><paragraph-text type="body">Examples of different categories of scenes are underwater scenes, terrestrial scenes, augmented reality scenes, albums, collections, shared content such as social networks photos or video, videos such as a film of a TV broadcasting. The context of an image or video includes scenes under surveillance, traffic scenes, scenes exterior to a vehicle, scenes on the interior of a vehicle. Various types of objects can be analysed, such as three-dimensional objects, microscopic objects, food, trinkets, scene text, etc. Examples of scene-specific processing includes semantic and syntactic analysis and classifying the scene content.</paragraph-text></section-body></definition-statement><references><section-title>References</section-title><limiting-references><section-title>Limiting references</section-title><section-body><paragraph-text type="preamble">This place does not cover:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Devices for controlling television cameras, e.g. remote control</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">H04N23/60</class-ref></paragraph-text></table-column></table-row></table></section-body></limiting-references><informative-references><section-title>Informative references</section-title><section-body><paragraph-text type="preamble">Attention is drawn to the following places, which may be of interest for search:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Character recognition; Recognising digital ink; Document-oriented image-based pattern recognition</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V30/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Recognition of biometric, human-related or animal-related patterns in image or video data</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V40/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Measuring arrangements characterised by the use of optical means</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G01B11/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Systems using the reflection or reradiation of acoustic waves, e.g. sonar systems</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G01S15/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06F3/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Image analysis</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06T7/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Burglar, theft or intruder alarms</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G08B13/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Selective content distribution, e.g. interactive television, Video on Demand [VoD]</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">H04N21/00</class-ref></paragraph-text></table-column></table-row></table></section-body></informative-references></references><glossary-of-terms><section-title>Glossary of terms</section-title><section-body><paragraph-text type="preamble">In this place, the following terms or expressions are used with the meaning indicated:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">scene</paragraph-text></table-column><table-column><paragraph-text type="body">visual representation of the world or of some elements of it, as captured by a sensor or generated by a computer</paragraph-text></table-column></table-row></table></section-body></glossary-of-terms></definition-item>
<definition-item date-revised="2024-08-01"><classification-symbol scheme="cpc">G06V20/05</classification-symbol><definition-title>Underwater scenes</definition-title><definition-statement><section-title>Definition statement</section-title><section-body><paragraph-text type="preamble">This place covers:</paragraph-text><paragraph-text type="body">Detection, identification and recognition of objects specifically adapted to underwater scenes.</paragraph-text><paragraph-text type="body">Categorising underwater objects.</paragraph-text><paragraph-text type="body">Detection, identification and recognition of underwater structures, such as oil or gas pipes.</paragraph-text><paragraph-text type="body">Detection, identification and recognition of objects or animals located on the sea floor.</paragraph-text><paragraph-text type="body">Adapting the recognition according to the underwater conditions, e.g. light scattering or absorption, artifacts, blurring, non-uniform lighting, etc.</paragraph-text><paragraph-text type="body">Recognising underwater objects in the context of simultaneous localisation and mapping [SLAM].</paragraph-text><paragraph-text type="body">Illustrative example of subject matter classified in this place:</paragraph-text><paragraph-text type="body"><media id="media85.png" file-name="cpc-def-G06V-0085.png" type="png" preferred-width="11.45cm" preferred-height="7.05cm"/></paragraph-text><paragraph-text type="body">Example of a system for analysing underwater scenes.</paragraph-text></section-body></definition-statement><references><section-title>References</section-title><informative-references><section-title>Informative references</section-title><section-body><paragraph-text type="preamble">Attention is drawn to the following places, which may be of interest for search:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Recognising three-dimensional [3D] objects in scenes</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V20/64</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Identifying an image sensor based on its output data</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V20/90</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Recognition of biometric, human-related or animal-related patterns in images or video</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V40/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Underwater vessels, e.g. submarines</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">B63G8/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Systems using the reflection or reradiation of acoustic waves, e.g. sonar systems</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G01S15/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Image analysis in general</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06T7/00</class-ref></paragraph-text></table-column></table-row></table></section-body></informative-references></references><glossary-of-terms><section-title>Glossary of terms</section-title><section-body><paragraph-text type="preamble">In this place, the following terms or expressions are used with the meaning indicated:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">AUV</paragraph-text></table-column><table-column><paragraph-text type="body">autonomous underwater vehicles</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">ROV</paragraph-text></table-column><table-column><paragraph-text type="body">remotely operated vehicles</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">UUV</paragraph-text></table-column><table-column><paragraph-text type="body">unmanned underwater vehicles</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">marine snow</paragraph-text></table-column><table-column><paragraph-text type="body">presence of organic material falling from upper layers of the water column</paragraph-text></table-column></table-row></table></section-body></glossary-of-terms></definition-item>
<definition-item date-revised="2024-08-01"><classification-symbol scheme="cpc">G06V20/10</classification-symbol><definition-title>Terrestrial scenes  (scenes under surveillance with static cameras <class-ref scheme="cpc">G06V20/52</class-ref>; scenes perceived from the exterior of a vehicle <class-ref scheme="cpc">G06V20/56</class-ref>; scenes perceived from the interior of a vehicle <class-ref scheme="cpc">G06V20/59</class-ref>)</definition-title><definition-statement><section-title>Definition statement</section-title><section-body><paragraph-text type="preamble">This place covers:</paragraph-text><paragraph-text type="body">Arrangements and methods specifically adapted to recognise terrestrial scenes:</paragraph-text><list><list-item><paragraph-text type="body">Recognising urban or other man-made structures;</paragraph-text></list-item><list-item><paragraph-text type="body">Recognising network patterns such as roads or rivers;</paragraph-text></list-item><list-item><paragraph-text type="body">Recognising vegetation, agricultural fields, etc.;</paragraph-text></list-item><list-item><paragraph-text type="body">Deriving scene properties, e.g. the amount of clutter in terms of population with image objects, the type of background, the existence of various types of objects, detection of the skyline, clouds, weather conditions, etc.;</paragraph-text></list-item><list-item><paragraph-text type="body">Obtaining semantic attributes or information from the scene, such as types of objects and their inter-relations, quantifying the geometric placement of the objects;</paragraph-text></list-item><list-item><paragraph-text type="body">Recognising terrestrial objects in the context of simultaneous localisation and mapping [SLAM].</paragraph-text></list-item></list><paragraph-text type="body">Illustrative example of subject matter classified in this place:</paragraph-text><paragraph-text type="body"><media id="media86.png" file-name="cpc-def-G06V-0086.png" type="png" preferred-width="12.93cm" preferred-height="10.86cm"/></paragraph-text><paragraph-text type="body">Perspective view of the region imaged by consecutive acquisitions of a hyperspectral sensor.</paragraph-text></section-body></definition-statement><references><section-title>References</section-title><limiting-references><section-title>Limiting references</section-title><section-body><paragraph-text type="preamble">This place does not cover:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Surveillance or monitoring of activities, e.g. for recognising suspicious objects</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V20/52</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Recognition or understanding of scenes outside a vehicle by using sensors mounted on the vehicle</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V20/56</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Recognition or understanding of scenes inside of a vehicle</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V20/59</class-ref></paragraph-text></table-column></table-row></table></section-body></limiting-references><informative-references><section-title>Informative references</section-title><section-body><paragraph-text type="preamble">Attention is drawn to the following places, which may be of interest for search:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Printing processes to produce particular kinds of printed work, e.g. patterns; Maps; Sea or meteorological charts</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">B41M3/02</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Navigation</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G01C21/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Systems using the reflection or reradiation of radio waves, e.g. radar systems;</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G01S13/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Systems using the reflection or reradiation of electromagnetic waves other than radio waves, e.g. lidar systems</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G01S17/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Meteorology</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G01W1/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Information retrieval of image data</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06F16/50</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Information retrieval of video data</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06F16/70</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Segmentation for general image processing</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06T7/10</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Motion image analysis</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06T7/20</class-ref></paragraph-text></table-column></table-row></table></section-body></informative-references></references><glossary-of-terms><section-title>Glossary of terms</section-title><section-body><paragraph-text type="preamble">In this place, the following terms or expressions are used with the meaning indicated:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">aerial imagery</paragraph-text></table-column><table-column><paragraph-text type="body">images taken from an aircraft or other flying object (e.g. aircrafts, helicopters, UAVs, balloons, etc.)</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">band</paragraph-text></table-column><table-column><paragraph-text type="body">response sensed by the optical sensor to a certain range of wavelength</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">endmember</paragraph-text></table-column><table-column><paragraph-text type="body">material that has a spectrally unique signature in the wavelength bands used to collect the image</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">GIS</paragraph-text></table-column><table-column><paragraph-text type="body">geographic information system</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Hughes Phenomenon/Curse of dimensionality</paragraph-text></table-column><table-column><paragraph-text type="body">when the dimensionality of the data increases, the volume of the data-space increases. Thus, if the dimensionality of a fixed amount of data is increased, the data becomes sparse in the increased data-space. This causes the classifier&apos;s performance to deteriorate. Increasing the amount of data or decreasing the dimensionality of the data will improve the performance of the classifier.</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">hyperspectral image</paragraph-text></table-column><table-column><paragraph-text type="body">multi-band image where the z dimension corresponds to consecutive spectral wavelengths ranges</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">multispectral image</paragraph-text></table-column><table-column><paragraph-text type="body">multi-band image where the z dimension corresponds to spectral wavelengths ranges (not necessarily consecutive)</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">remote sensing</paragraph-text></table-column><table-column><paragraph-text type="body">process of detecting and monitoring the physical characteristics of an area by measuring its reflected and emitted radiation from a satellite or aircraft</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">SAR</paragraph-text></table-column><table-column><paragraph-text type="body">Synthetic aperture radar</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">spectral image cubespectral image cubes</paragraph-text></table-column><table-column><paragraph-text type="body">data having 3 dimensions, 2 spatial (x, y) and a third spectral dimension</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">UAV</paragraph-text></table-column><table-column><paragraph-text type="body">unmanned aerial vehicles</paragraph-text></table-column></table-row></table></section-body></glossary-of-terms></definition-item>
<definition-item date-revised="2024-08-01"><classification-symbol scheme="cpc">G06V20/13</classification-symbol><definition-title>Satellite images</definition-title><definition-statement><section-title>Definition statement</section-title><section-body><paragraph-text type="preamble">This place covers:</paragraph-text><paragraph-text type="body">Recognising patterns corresponding to different image structures (e.g., objects) in remotely sensed satellite images or video, e.g. optical data (images or video), GPS, radar, LIDAR measurement data or in combination.</paragraph-text><paragraph-text type="body">Object detection, deriving hyperspectral signatures from objects within satellite images. Categorisation of man-made objects / image targets within satellite images.</paragraph-text><paragraph-text type="body">Vegetation detection or monitoring canopy growth within satellite images.</paragraph-text><paragraph-text type="body">Cloud detection and cloud mask segmentation within satellite images.</paragraph-text><paragraph-text type="body">3D measurement of man-made objects, such as building roofs, within satellite images.</paragraph-text><paragraph-text type="body">Change detection, e.g. assessing influence of natural disasters, presence of new objects (anomalies) against a known background within satellite images.</paragraph-text><paragraph-text type="body">Weather condition monitoring by image or video analysis of satellite images.</paragraph-text><paragraph-text type="body">Illustrative example of subject matter classified in this place:</paragraph-text><paragraph-text type="body"><media id="media87.png" file-name="cpc-def-G06V-0087.png" type="png" preferred-width="16cm" preferred-height="14.06cm"/></paragraph-text><paragraph-text type="body">Automatic classification of objects in satellite images.</paragraph-text></section-body></definition-statement><references><section-title>References</section-title><informative-references><section-title>Informative references</section-title><section-body><paragraph-text type="preamble">Attention is drawn to the following places, which may be of interest for search:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Pattern recognition and image understanding in terrestrial scenes with images taken by planes or drones</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V20/17</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Recognising three-dimensional [3D] objects in scenes</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V20/64</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Recognition of technical drawings and geographical maps</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V30/422</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Navigation</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G01C21/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Systems using the reflection or reradiation of radio waves, e.g. radar systems</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G01S13/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Systems using the reflection or reradiation of electromagnetic waves other than radio waves, e.g. lidar systems</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G01S17/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Satellite radio beacon positioning systems, e.g. GPS</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G01S19/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Information retrieval of image data</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06F16/50</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Information retrieval of video data</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06F16/70</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Segmentation for general image processing</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06T7/10</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Motion image analysis</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06T7/20</class-ref></paragraph-text></table-column></table-row></table></section-body></informative-references></references><special-rules><section-title>Special rules of classification</section-title><section-body><paragraph-text type="body">This group covers techniques specifically adapted for remotely sensed satellite images or video. Recognising patterns in aerial images or video acquired from aircrafts, helicopters, unmanned aerial vehicles (UAVs), balloons, etc., is classified in group <class-ref scheme="cpc">G06V20/17</class-ref>. The difference between these two groups consists of how the images are acquired. Image or video techniques classified in group <class-ref scheme="cpc">G06V20/13</class-ref> lack perspective (depth) information, while in the group <class-ref scheme="cpc">G06V20/17</class-ref>, images acquired from aircrafts, helicopters, UAVs, etc. contain the perspective (depth) information.</paragraph-text></section-body></special-rules><glossary-of-terms><section-title>Glossary of terms</section-title><section-body><paragraph-text type="preamble">In this place, the following terms or expressions are used with the meaning indicated:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">hyperspectral images</paragraph-text></table-column><table-column><paragraph-text type="body">images in which one continuous spectrum is measured for each pixel. Generally, the spectral resolution is given in nanometres or wave numbers.</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">remote sensing</paragraph-text></table-column><table-column><paragraph-text type="body">process of detecting and monitoring the physical characteristics of an area by measuring its reflected and emitted radiation from a satellite or aircraft</paragraph-text></table-column></table-row></table></section-body></glossary-of-terms></definition-item>
<definition-item date-revised="2024-08-01"><classification-symbol scheme="cpc">G06V20/17</classification-symbol><definition-title>taken from planes or by drones</definition-title><definition-statement><section-title>Definition statement</section-title><section-body><paragraph-text type="preamble">This place covers:</paragraph-text><paragraph-text type="body">Recognising patterns corresponding to different image structures (e.g. objects) in aerial images or video acquired from aircraft, helicopters, unmanned aerial vehicles (UAVs) or drones, balloons, etc.</paragraph-text><paragraph-text type="body">Categorisation of man-made objects/image targets in aerial images or video.</paragraph-text><paragraph-text type="body">Vegetation detection or monitoring canopy growth in aerial images or video.</paragraph-text><paragraph-text type="body">3D measurement of man-made objects such as building roofs wherein the scene is taken from planes or by drones.</paragraph-text><paragraph-text type="body">Inspection of buildings or other man-made objects, e.g. damage classification, wherein the scene is taken from planes or by drones.</paragraph-text><paragraph-text type="body">Recognising flying entities, such as insects or birds, from images or video captured by drones.</paragraph-text><paragraph-text type="body">Recognising or monitoring the activity of military targets in aerial images or video acquired from aircrafts, helicopters, UAVs or drones, balloons.</paragraph-text><paragraph-text type="body">Illustrative example of subject matter classified in this place:</paragraph-text><paragraph-text type="body">1.</paragraph-text><paragraph-text type="body"><media id="media88.png" file-name="cpc-def-G06V-0088.png" type="png" preferred-width="9.99cm" preferred-height="7.43cm"/></paragraph-text><paragraph-text type="body">Recognising and assessing the damage to a building using a drone.</paragraph-text><paragraph-text type="body">2.</paragraph-text><paragraph-text type="body"><media id="media89.png" file-name="cpc-def-G06V-0089.png" type="png" preferred-width="15.56cm" preferred-height="8.49cm"/></paragraph-text><paragraph-text type="body">Determining the surface of roofs using UAVs.</paragraph-text></section-body></definition-statement><references><section-title>References</section-title><informative-references><section-title>Informative references</section-title><section-body><paragraph-text type="preamble">Attention is drawn to the following places, which may be of interest for search:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Pattern recognition and image understanding in terrestrial scenes with images taken from satellites</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V20/13</class-ref> </paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Recognising three-dimensional [3D] objects in scenes</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V20/64</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Recognition of technical drawings and geographical maps</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V30/422</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Photogrammetry or videogrammetry, e.g. stereogrammetry; Photographic surveying</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G01C11/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Radar or analogous systems, specially adapted for specific applications for mapping or imaging</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G01S13/89</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Lidar systems, specially adapted for mapping or imaging</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G01S17/89</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Information retrieval of image data</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06F16/50</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Information retrieval of video data</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06F16/70</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Segmentation for general image processing</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06T7/10</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Analysis of motion in images</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06T7/20</class-ref></paragraph-text></table-column></table-row></table></section-body></informative-references></references><special-rules><section-title>Special rules of classification</section-title><section-body><paragraph-text type="body">This group covers techniques specifically adapted for aerial images or video acquired from aircraft, helicopters, unmanned aerial vehicles (UAVs), balloons, etc.</paragraph-text><paragraph-text type="body">Recognising patterns in remotely sensed satellite images or video are covered by group <class-ref scheme="cpc">G06V20/13</class-ref>. The difference between these two groups lies in the manner of acquisition of the images. Image or video techniques covered by group <class-ref scheme="cpc">G06V20/17</class-ref> relate to perspective (depth) information, while techniques covered by group <class-ref scheme="cpc">G06V20/13</class-ref>, relate to images acquired from satellites, which lack perspective (depth) information.</paragraph-text></section-body></special-rules><glossary-of-terms><section-title>Glossary of terms</section-title><section-body><paragraph-text type="preamble">In this place, the following terms or expressions are used with the meaning indicated:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">aerial imagery</paragraph-text></table-column><table-column><paragraph-text type="body">images taken from an aircraft or other flying object (e.g. aircrafts, helicopters, UAVs, balloons, etc.)</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">remote sensing</paragraph-text></table-column><table-column><paragraph-text type="body">process of detecting and monitoring the physical characteristics of an area by measuring its reflected and emitted radiation from a satellite or aircraft</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">UAV</paragraph-text></table-column><table-column><paragraph-text type="body">unmanned aerial vehicle</paragraph-text></table-column></table-row></table></section-body></glossary-of-terms></definition-item>
<definition-item date-revised="2024-08-01"><classification-symbol scheme="cpc">G06V20/20</classification-symbol><definition-title>in augmented reality scenes</definition-title><definition-statement><section-title>Definition statement</section-title><section-body><paragraph-text type="preamble">This place covers:</paragraph-text><paragraph-text type="body">Object recognition operating in an augmented reality environment and adapted to provide additional information about a scene to a user. The underlying processing may involve one or more of the following steps:</paragraph-text><paragraph-text type="body">1. acquiring an image of a real scene by an image capture device;</paragraph-text><paragraph-text type="body">2. detecting and recognising objects in the depicted scene;</paragraph-text><paragraph-text type="body">3. acquiring additional information which is related to these objects (e.g. from a database);</paragraph-text><paragraph-text type="body">4. presenting this information on the original image in an overlaid / superimposed manner.</paragraph-text><paragraph-text type="body">The object detection and recognition processes need to be fast due to real-time constrains. For this reason, additional information provided by other sensors (e.g. accelerometers, gyroscopes, GPS, solid state compasses or RFID) can be used to define or limit the analysis based on the information they provide.</paragraph-text><paragraph-text type="body">Examples of adaptations include:</paragraph-text><list><list-item><paragraph-text type="body">the way in which objects of interest are detected and recognised in the image: feature-based detection, geometrical proximity to the object of interest or optical character recognition [OCR] of text in a scene, etc.;</paragraph-text></list-item><list-item><paragraph-text type="body">the way in which additional object related information is obtained, e.g. from a database stored locally in the device, or by internet search, etc.;</paragraph-text></list-item><list-item><paragraph-text type="body">the purpose for which the application is designed: e.g. for visually impaired people, for driver assistance systems, for chirurgical interventions, for presentation of chemical structures, as interactive guide for attractions and museums, or for use on construction sites, etc.</paragraph-text></list-item></list><paragraph-text type="subheading">Notes &#8211; technical background</paragraph-text><paragraph-text type="body">These notes provide more information about the technical subject matter that is classified in this place:</paragraph-text><paragraph-text type="body">Object recognition for augmenting scene information and adapted for head mounted displays and portable devices needs to be fast due to real-time constrains. For this reason, additional information provided by other sensors (e.g. accelerometers, gyroscopes, GPS, solid state compasses, RFID) may be used to define or limit the space subject to analysis.</paragraph-text><paragraph-text type="body">Different approaches to camera pose estimation and registration may be essential to successful object recognition.</paragraph-text><paragraph-text type="body">Illustrative example of subject matter classified in this place:</paragraph-text><paragraph-text type="body"><media id="media90.png" file-name="cpc-def-G06V-0090.png" type="png" preferred-width="12.7cm" preferred-height="8.51cm"/></paragraph-text><paragraph-text type="body">Recognition of a real-world object by a head-mounted computing device.</paragraph-text></section-body></definition-statement><relationship><section-title>Relationships with other classification places</section-title><section-body><paragraph-text type="body">Determining position or orientation of objects or cameras is covered by group <class-ref scheme="cpc">G06T7/70</class-ref>. Input arrangements or combined input and output arrangements for interaction between user and computer is covered by group <class-ref scheme="cpc">G06F3/01</class-ref>.</paragraph-text></section-body></relationship><references><section-title>References</section-title><informative-references><section-title>Informative references</section-title><section-body><paragraph-text type="preamble">Attention is drawn to the following places, which may be of interest for search:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Descriptors for shape, contour or point-related descriptions of extracted image or video features, e.g. scale invariant feature transform [SIFT] or bags of words [BoW]; Salient region features</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/46</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Recognising three-dimensional [3D] objects in scenes</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V20/64</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Labelling scene content</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V20/70</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Character recognition within an image or video; Document-oriented image-based pattern recognition</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V30/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Recognition of biometric, human-related or animal-related patterns in image or video data</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V40/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Input arrangements or combined input and output arrangements for interaction between user and computer</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06F3/01</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Digital output to display device</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06F3/14</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Analysis of motion in images</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06T7/20</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Manipulating 3D models or images for computer graphics</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06T19/00</class-ref></paragraph-text></table-column></table-row></table></section-body></informative-references></references><glossary-of-terms><section-title>Glossary of terms</section-title><section-body><paragraph-text type="preamble">In this place, the following terms or expressions are used with the meaning indicated:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">AR</paragraph-text></table-column><table-column><paragraph-text type="body">augmented reality</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">AR overlay</paragraph-text></table-column><table-column><paragraph-text type="body">images, videos, 3D or other information types superimposed over a target object</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Field of View [FoV]</paragraph-text></table-column><table-column><paragraph-text type="body">area that can be observed through a capture device lens. Depending on the lens focus, the field of view can be adapted and can vary in size.</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">OCR</paragraph-text></table-column><table-column><paragraph-text type="body">optical character recognition</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">VR</paragraph-text></table-column><table-column><paragraph-text type="body">virtual reality</paragraph-text></table-column></table-row></table></section-body></glossary-of-terms></definition-item>
<definition-item date-revised="2024-08-01"><classification-symbol scheme="cpc">G06V20/30</classification-symbol><definition-title>in albums, collections or shared content, e.g. social network photos or video</definition-title><definition-statement><section-title>Definition statement</section-title><section-body><paragraph-text type="preamble">This place covers:</paragraph-text><paragraph-text type="body">Generating groups or clusters from images or video based on their similarity, based on events, backgrounds, identified individuals, etc.</paragraph-text><paragraph-text type="body">Comparing and forming connections between image collections using matching, classification and clustering.</paragraph-text><paragraph-text type="body">Detecting or recognising events in an image collection and ordering these events in an event timeline, based on image content.</paragraph-text><paragraph-text type="body">Construction of a social network by analysis of an image collection.</paragraph-text><paragraph-text type="body">Illustrative example of subject matter classified in this place:</paragraph-text><paragraph-text type="body"><media id="media91.png" file-name="cpc-def-G06V-0091.png" type="png" preferred-width="16cm" preferred-height="9.28cm"/></paragraph-text><paragraph-text type="body">Steps for constructing a dynamic social network from raw video data of observations of people.</paragraph-text></section-body></definition-statement><references><section-title>References</section-title><informative-references><section-title>Informative references</section-title><section-body><paragraph-text type="preamble">Attention is drawn to the following places, which may be of interest for search:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Global feature extraction by analysis of the whole pattern, e.g. global shape, global boundary descriptors or involving frequency domain transformations or autocorrelation</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/42</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Local feature extraction by analysis of parts of the pattern, e.g. by detecting edges, contours, loops, corners, strokes or intersections; Connectivity analysis, e.g. of connected components, edge linking or neighbouring slice analysis</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/44</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Recognition using clustering in general</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/762</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Recognition of patterns in video content, e.g. in a film or a TV broadcasting</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V20/40</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Recognition of scenes under surveillance or monitoring activities, e.g. recognising suspicious objects</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V20/52</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Labelling scene content, e.g. semantic segmentation</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V20/70</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Recognition of human bodies within image or video data</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V40/10</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Recognition of human faces, e.g. facial parts, sketches or expressions within image or video data</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V40/16</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Information retrieval of image data</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06F16/50</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Information retrieval of video data</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06F16/70</class-ref></paragraph-text></table-column></table-row></table></section-body></informative-references></references><special-rules><section-title>Special rules of classification</section-title><section-body><paragraph-text type="body">Detection of events in video surveillance, in particular suspicious activities or objects, is classified in group <class-ref scheme="cpc">G06V20/52</class-ref>. Labelling of scene content, e.g. by semantic segmentation, is classified in group <class-ref scheme="cpc">G06V20/70</class-ref>.</paragraph-text></section-body></special-rules></definition-item>
<definition-item date-revised="2024-08-01"><classification-symbol scheme="cpc">G06V20/40</classification-symbol><definition-title>in video content  (extracting overlay text <class-ref scheme="cpc">G06V20/62</class-ref>; video retrieval <class-ref scheme="cpc">G06F16/70</class-ref>; processing of video elementary streams in video servers <class-ref scheme="cpc">H04N21/234</class-ref>; processing of video elementary streams in video clients <class-ref scheme="cpc">H04N21/44</class-ref>)</definition-title><definition-statement><section-title>Definition statement</section-title><section-body><paragraph-text type="preamble">This place covers:</paragraph-text><paragraph-text type="body">Video summarisation/abstraction, e.g. key-frame extraction, extracting of video features or fingerprints, extraction of representative shots, detecting important frames by analysing the reactions of the viewers or by monitoring parts of the video, such as the TV logo.</paragraph-text><paragraph-text type="body">High-level semantic clustering, classification and understanding of video scenes, e.g. detection, labelling or Markovian modelling. Examples of video content subject to such analysis are sport broadcast events or TV news.</paragraph-text><paragraph-text type="body">Low-level semantic clustering or determination of sections in videos such as scenes and shots; classification of shots, e.g. as close-up shot, medium shot or long shot.</paragraph-text><paragraph-text type="body">Extraction of features, e.g. histogram similarity measures, manifolds, by use of video fingerprints, etc. Examples of low-level features are colour or texture-based features, local interest points (key-points), filter responses, edge features, local descriptors (SIFT, SURF, etc.) or combinations of them (see also group <class-ref scheme="cpc">G06V10/40</class-ref>). Examples of high-level features are features related to camera motion (tracking visual features), the presence of skin, the number of faces present, the size of faces or other human features visible, text or other objects that are identifiable in each frame.</paragraph-text><paragraph-text type="body">Matching video sequences, e.g. by frame or temporal analysis.</paragraph-text><paragraph-text type="body">Segmenting video sequences, e.g. parsing or cutting the sequence.</paragraph-text><paragraph-text type="body">Video categorisation, e.g. classify video content into sport/music/news or recognise commercials in media content for substitution.</paragraph-text><paragraph-text type="body">Sport games analysis, e.g. tactic analysis in sport videos for assistance of coaches and players; final pitching shot indexing for baseball game; indexing the important parts, such as shots, score points, etc; video monitoring the score table.</paragraph-text><paragraph-text type="body">Generation of compact representations of the video sequence as a result of pattern recognition or image understanding, e.g. creating thumbnails or representative icons.</paragraph-text><paragraph-text type="body">Detection and recognition of harmful/sexual/violent content.</paragraph-text><paragraph-text type="body">Discovery of relationships between objects or persons in videos.</paragraph-text><paragraph-text type="body">Detecting a key/anchor person from a video; characterising the main characters.</paragraph-text><paragraph-text type="body">Association of a video with semantic information (e.g. keywords) to describe the content (using e.g. Markov random fields).</paragraph-text><paragraph-text type="body">Generation of semantic labels using a graph which describes the video content, where the nodes are objects or activities and edges are the relationships between them.</paragraph-text><paragraph-text type="body">Illustrative examples of subject matter classified in this place:</paragraph-text><paragraph-text type="body">1.</paragraph-text><paragraph-text type="body"><media id="media92.png" file-name="cpc-def-G06V-0092.png" type="png" preferred-width="11.18cm" preferred-height="5.12cm"/></paragraph-text><paragraph-text type="body">Clustering of the representative frames containing a given face and creation of face thumbnails of a video sequence containing faces.</paragraph-text><paragraph-text type="body">2.</paragraph-text><paragraph-text type="body"><media id="media93.jpeg" file-name="cpc-def-G06V-0093.jpeg" type="jpeg" preferred-width="10.1cm" preferred-height="6.35cm"/></paragraph-text><paragraph-text type="body">Recognising football players in a football match and displaying the representative shots in which a certain player was active.</paragraph-text></section-body></definition-statement><references><section-title>References</section-title><limiting-references><section-title>Limiting references</section-title><section-body><paragraph-text type="preamble">This place does not cover:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Extracting overlay text</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V20/62</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Information retrieval of video data</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06F16/70</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Processing of video elementary streams in video servers</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">H04N21/234</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Processing of video elementary streams in video client devices</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">H04N21/44</class-ref></paragraph-text></table-column></table-row></table></section-body></limiting-references><informative-references><section-title>Informative references</section-title><section-body><paragraph-text type="preamble">Attention is drawn to the following places, which may be of interest for search:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Arrangements for image or video understanding in general</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Global feature extraction by analysis of the whole pattern, e.g. global shape, global boundary descriptors or involving frequency domain transformations or autocorrelation</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/42</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Local feature extraction by analysis of parts of the pattern, e.g. by detecting edges, contours, loops, corners, strokes or intersections; Connectivity analysis, e.g. of connected components, edge linking or neighbouring slice analysis</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/44</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Pattern recognition or machine learning in images or video using clustering</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/762</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Recognition of scenes under surveillance or monitoring activities, e.g. recognising suspicious objects</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V20/52</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Labelling scene content, e.g. deriving syntactic or semantic representations</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V20/70</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Recognition of human or animal bodies</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V40/10</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Recognition of human faces, e.g. facial parts, sketches or expressions</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V40/16</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Recognition of movements or behaviour, e.g. gesture recognition</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V40/20</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Analysis of motion in images</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06T7/20</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Image analysis using motion-based segmentation</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06T7/215</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Editing; Indexing; Addressing; Timing or synchronising; Monitoring; Measuring tape travel</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G11B27/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Television picture signal circuitry for video frequency region</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">H04N5/14</class-ref></paragraph-text></table-column></table-row></table></section-body></informative-references></references><glossary-of-terms><section-title>Glossary of terms</section-title><section-body><paragraph-text type="preamble">In this place, the following terms or expressions are used with the meaning indicated:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">video fingerprinting</paragraph-text></table-column><table-column><paragraph-text type="body">class of dimension reduction techniques for identifying, extracting and summarising characteristic components of a video enabling that video to be uniquely identified</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">video summarisation</paragraph-text></table-column><table-column><paragraph-text type="body">generation of a short summary of the content of a longer video by selecting and presenting the most informative or interesting video frames</paragraph-text></table-column></table-row></table></section-body></glossary-of-terms></definition-item>
<definition-item date-revised="2024-08-01"><classification-symbol scheme="cpc">G06V20/50</classification-symbol><definition-title>Context or environment of the image</definition-title><definition-statement><section-title>Definition statement</section-title><section-body><paragraph-text type="preamble">This place covers:</paragraph-text><paragraph-text type="body">Recognising and understanding scenes according to the context or the environment of the scene, e.g. the type of scene or the situation in which it is acquired.</paragraph-text></section-body></definition-statement><references><section-title>References</section-title><informative-references><section-title>Informative references</section-title><section-body><paragraph-text type="preamble">Attention is drawn to the following places, which may be of interest for search:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Recognition based on the type of objects</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V20/60</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Labelling scene content, e.g. deriving syntactic or semantic representations</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V20/70</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Recognition of biometric, human-related or animal-related patterns in image or video data</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V40/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Recognising movements or behaviour, e.g. gesture recognition</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V40/20</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Radar or analogous systems for traffic control</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G01S13/91</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Radar or analogous systems for anti-collision purposes of land vehicles</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G01S13/931</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Lidar systems for anti-collision purposes of land vehicles</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G01S17/931</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Analysis of motion in images</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06T7/20</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Image analysis for determining position or orientation of objects or cameras</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06T7/70</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Traffic control systems for road vehicles</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G08G1/00</class-ref></paragraph-text></table-column></table-row></table></section-body></informative-references></references><special-rules><section-title>Special rules of classification</section-title><section-body><paragraph-text type="body">Recognising different types of objects is classified in group <class-ref scheme="cpc">G06V20/60</class-ref>. Classification in groups <class-ref scheme="cpc">G06V20/60</class-ref> and <class-ref scheme="cpc">G06V20/50</class-ref> or subgroups is applied when a certain type of object is recognised in a specific scene context. For example, recognition of license plates, covered by group <class-ref scheme="cpc">G06V20/62</class-ref>, is classified also in group <class-ref scheme="cpc">G06V20/52</class-ref> when the recognition is performed in the context of a scene under surveillance, such as a parking lot.</paragraph-text></section-body></special-rules></definition-item>
<definition-item date-revised="2024-08-01"><classification-symbol scheme="cpc">G06V20/52</classification-symbol><definition-title>Surveillance or monitoring of activities, e.g. for recognising suspicious objects  (recognising microscopic objects <class-ref scheme="cpc">G06V20/69</class-ref>)</definition-title><definition-statement><section-title>Definition statement</section-title><section-body><paragraph-text type="preamble">This place covers:</paragraph-text><paragraph-text type="body">Detection and recognition of objects or events in scenes under surveillance, for example by:</paragraph-text><list><list-item><paragraph-text type="body">detecting activity in a restricted area/zone, e.g. detecting intrusion, unsafe situations around working equipment or machines, monitoring of trespassing of a specific area;</paragraph-text></list-item><list-item><paragraph-text type="body">excluding certain spatial/temporal fragments, for example, for privacy protection (e.g. input of a PIN for a cash dispenser surveyed by a camera);</paragraph-text></list-item><list-item><paragraph-text type="body">detecting hazards: fire, explosions, smoke, contamination, fluid spills, etc.; contamination from pollutants, e.g. petroleum; dangers for occupational injuries; detecting flashes originating from machine guns;</paragraph-text></list-item><list-item><paragraph-text type="body">applying pattern recognition or image understanding techniques for counting various image objects (objects of interest, people, etc.), monitoring queues;</paragraph-text></list-item><list-item><paragraph-text type="body">detecting and recognising hidden objects, ammunition, explosives, e.g. as in airport luggage scanner;</paragraph-text></list-item><list-item><paragraph-text type="body">recognising movements/trajectories, determining paths of the objects in the surveyed scene, e.g. detect the flows of persons in public places;</paragraph-text></list-item><list-item><paragraph-text type="body"> identification of certain image objects/persons based on prior information; selection of the relevant surveyed scenes;</paragraph-text></list-item><list-item><paragraph-text type="body">identification and re-identification of image objects/persons, i.e. identification of the same person at different times or in different places along image sequences;</paragraph-text></list-item><list-item><paragraph-text type="body">occupancy or presence detection, e.g. monitoring the filling state of the shelves in a supermarket, keeping track of empty places in a parking lot, elevator occupancy monitoring, seat occupancy in public spaces, e.g. cinemas, concerts, etc.;</paragraph-text></list-item><list-item><paragraph-text type="body">detecting presence for intelligent building control, e.g. for switching off light, for controlling the air conditioning systems, etc.;</paragraph-text></list-item><list-item><paragraph-text type="body">detecting anomalous activities or suspicious behaviour, such as vandalism, robbery, loitering, etc.;</paragraph-text></list-item><list-item><paragraph-text type="body">detecting and recognising suspicious objects or objects left behind;</paragraph-text></list-item><list-item><paragraph-text type="body">monitoring people habits, i.e. in a wearable computing setting (eating patterns, sleeping patterns, washing habits, etc.);</paragraph-text></list-item><list-item><paragraph-text type="body"> monitoring queues, predicting queue waiting time, etc.;</paragraph-text></list-item><list-item><paragraph-text type="body">recognising static or dynamic crowd, e.g. crowd congestion.</paragraph-text></list-item></list><paragraph-text type="body">The subgroup <class-ref scheme="cpc">G06V20/54</class-ref> concerns recognising and understanding of traffic scenes, by detection, identification, classification and recognition of traffic patterns, e.g. cars on the roads, traffic junctions, traffic jams, estimating the travel time.</paragraph-text><paragraph-text type="body">Illustrative examples of subject matter classified in this place:</paragraph-text><paragraph-text type="body">1. <media id="media94.png" file-name="cpc-def-G06V-0094.png" type="png" preferred-width="6.35cm" preferred-height="7.6cm"/></paragraph-text><paragraph-text type="body">Monitoring activities for a scene under surveillance.</paragraph-text><paragraph-text type="body">2.</paragraph-text><paragraph-text type="body"><media id="media95.png" file-name="cpc-def-G06V-0095.png" type="png" preferred-width="10.35cm" preferred-height="7.96cm"/></paragraph-text><paragraph-text type="body">Monitoring the occupancy of a parking lot for a scene under surveillance.</paragraph-text></section-body></definition-statement><references><section-title>References</section-title><limiting-references><section-title>Limiting references</section-title><section-body><paragraph-text type="preamble">This place does not cover:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Recognising microscopic objects</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V20/69</class-ref> </paragraph-text></table-column></table-row></table></section-body></limiting-references><informative-references><section-title>Informative references</section-title><section-body><paragraph-text type="preamble">Attention is drawn to the following places, which may be of interest for search:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Recognition based on the type of objects</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V20/60</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Labelling scene content, e.g. deriving syntactic or semantic representations</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V20/70</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Recognition of biometric, human-related or animal-related patterns in image or video data</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V40/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Recognising movements or behaviour, e.g. gesture recognition</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V40/20</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Analysis of motion in images</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06T7/20</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Burglar, theft or intruder alarm</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G08B13/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Details of television systems</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">H04N5/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Closed-circuit television systems, i.e. systems in which the signal is not broadcast</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">H04N7/18</class-ref></paragraph-text></table-column></table-row></table></section-body></informative-references></references></definition-item>
<definition-item date-revised="2024-08-01"><classification-symbol scheme="cpc">G06V20/56</classification-symbol><definition-title>exterior to a vehicle by using sensors mounted on the vehicle</definition-title><definition-statement><section-title>Definition statement</section-title><section-body><paragraph-text type="preamble">This place covers:</paragraph-text><paragraph-text type="body">Detection, identification and recognition of road lanes, lane markings and borders, free road ahead.</paragraph-text><paragraph-text type="body">Lane and road marking categorisation, e.g. solid lines, dashed lines, markings at pedestrian crossings, direction indicating arrows, etc.</paragraph-text><paragraph-text type="body">Estimation of road geometry characteristics, such as curvature, slope or elevation, using e.g. disparity maps of road surfaces or the relative motion of surrounding objects using a clothoidal lane model.</paragraph-text><paragraph-text type="body">Detection of physical entities located at the side of the road, such as structural barriers (e.g. wall, guardrail), delineators and markers.</paragraph-text><paragraph-text type="body">Recognition of the drivers&apos; driving pattern in relation to the road lanes perceived from the vehicle.</paragraph-text><paragraph-text type="body">Recognising the trajectory of a car relative to the road.</paragraph-text><paragraph-text type="body">Detecting the drivable area ahead of the host vehicle, or of the clear path.</paragraph-text><paragraph-text type="body">Detection or recognition of road surface characteristics, e.g. cracks, holes.</paragraph-text><paragraph-text type="body">Detection, classification and recognition of road signs, indicators, etc.</paragraph-text><paragraph-text type="body">Detection or recognition of potential obstacles, e.g. vehicles ahead, pedestrians.</paragraph-text><paragraph-text type="body">Recognising surrounding objects by the analysis of their relative position or velocity, possibly with the aid of additional sensors.</paragraph-text><paragraph-text type="body">Recognition of surrounding objects with the aid of a map of the environment. Categorising vehicles, e.g. car, lorry, bicycle.</paragraph-text><paragraph-text type="body">Detection or recognition of available parking places; parking assistance by recognising surrounding objects and producing an image of the environment during the parking process with an overview of the host vehicle surroundings, such as a bird-eye view.</paragraph-text><paragraph-text type="body">Detection of foreign matter on the windshield, e.g. water, dirt, snow.</paragraph-text><paragraph-text type="body">Adapting the recognition according to the weather conditions, e.g. rain, fog, snow.</paragraph-text><paragraph-text type="body">Recognition of illumination non-uniformities, e.g. discriminating between objects and shadows.</paragraph-text><paragraph-text type="body">Recognition of scene objects using special illumination, e.g. infrared light for night vision.</paragraph-text><paragraph-text type="body">Recognition and compensation for the effects of non-uniformities in illumination, e.g. shadows.</paragraph-text><paragraph-text type="body">Recognition of light-casting objects, such as traffic lights, lights of the cars ahead, etc.</paragraph-text><paragraph-text type="body">Illustrative examples of subject matter classified in this place:</paragraph-text><paragraph-text type="body">1.</paragraph-text><paragraph-text type="body"><media id="media96.png" file-name="cpc-def-G06V-0096.png" type="png" preferred-width="12cm" preferred-height="8.19cm"/></paragraph-text><paragraph-text type="body">Recognition of lane markers for autonomous driving.</paragraph-text><paragraph-text type="body">2.</paragraph-text><paragraph-text type="body"><media id="media97.png" file-name="cpc-def-G06V-0097.png" type="png" preferred-width="12.34cm" preferred-height="5.42cm"/></paragraph-text><paragraph-text type="body">3.</paragraph-text><paragraph-text type="body"><media id="media98.png" file-name="cpc-def-G06V-0098.png" type="png" preferred-width="11.6cm" preferred-height="7.85cm"/></paragraph-text><paragraph-text type="body">Recognition of the road geometry (e.g. its slope) by image analysis.</paragraph-text></section-body></definition-statement><relationship><section-title>Relationships with other classification places</section-title><section-body><paragraph-text type="body">Recognising and understanding of scenes for autonomous driving makes extensive use of a mix of sensors or modalities which are classified in different places in IPC (see the informative references indicated below).</paragraph-text></section-body></relationship><references><section-title>References</section-title><application-references><section-title>Application-oriented references</section-title><section-body><paragraph-text type="preamble">Examples of places where the subject matter of this place is covered when specially adapted, used for a particular purpose, or incorporated in a larger system:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Drive control systems specially adapted for autonomous road vehicles</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">B60W60/00</class-ref></paragraph-text></table-column></table-row></table></section-body></application-references><informative-references><section-title>Informative references</section-title><section-body><paragraph-text type="preamble">Attention is drawn to the following places, which may be of interest for search:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Recognition or understanding of scenes inside of a vehicle</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V20/59</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Recognition based on the type of objects</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V20/60</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Character recognition</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V30/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Recognition of human or animal bodies, e.g. pedestrians</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V40/10</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Navigation</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G01C21/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Radar or analogous systems for anti-collision purposes of land vehicles</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G01S13/931</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Lidar systems for anti-collision purposes of land vehicles</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G01S17/931</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Control of position, course or altitude of land, water, air or space vehicles, e.g. automatic pilot</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G05D1/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Analysis of motion in images</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06T7/20</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Image analysis for determining position or orientation of objects or cameras</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06T7/70</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Analysis of captured images to determine intrinsic or extrinsic camera parameters, i.e. camera calibration</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06T7/80</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Traffic control systems for road vehicles</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G08G1/00</class-ref></paragraph-text></table-column></table-row></table></section-body></informative-references></references><special-rules><section-title>Special rules of classification</section-title><section-body><paragraph-text type="body">The functions of acquisition, pre-processing, feature extraction, pattern recognition and machine learning classified in group <class-ref scheme="cpc">G06V10/00</class-ref> are also classified in group <class-ref scheme="cpc">G06V20/56</class-ref> according to the function. For instance, special illumination (e.g. infrared) used for night vision, is classified in group <class-ref scheme="cpc">G06V10/143</class-ref> and in group <class-ref scheme="cpc">G06V20/56</class-ref>. Other examples are techniques for determination of a region of interest (ROI) defining the obstacles ahead, classified in group <class-ref scheme="cpc">G06V10/25</class-ref> and in group <class-ref scheme="cpc">G06V20/56</class-ref>.</paragraph-text></section-body></special-rules><glossary-of-terms><section-title>Glossary of terms</section-title><section-body><paragraph-text type="preamble">In this place, the following terms or expressions are used with the meaning indicated:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">ADAS</paragraph-text></table-column><table-column><paragraph-text type="body">&quot;advanced driver-assistance systems&quot;: technologies that assist drivers in driving and parking functions</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">AV</paragraph-text></table-column><table-column><paragraph-text type="body">&quot;autonomous vehicle&quot;: vehicle that is capable of driving itself</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">ECU</paragraph-text></table-column><table-column><paragraph-text type="body">&quot;electronic control unit&quot;: an embedded unit in the vehicle that controls one or more electrical systems, such as the engine control unit or the human-machine interface</paragraph-text></table-column></table-row></table></section-body></glossary-of-terms></definition-item>
<definition-item date-revised="2024-08-01"><classification-symbol scheme="cpc">G06V20/59</classification-symbol><definition-title>inside of a vehicle, e.g. relating to seat occupancy, driver state or inner lighting conditions</definition-title><definition-statement><section-title>Definition statement</section-title><section-body><paragraph-text type="preamble">This place covers:</paragraph-text><paragraph-text type="body">Recognising seat occupancy, e.g. forward or rearward facing child seat.</paragraph-text><paragraph-text type="body">Recognising driver or occupant position, e.g. for automatic seat adjustment, adjustment of the driving wheel or mirrors.</paragraph-text><paragraph-text type="body">Recognising the drivers&apos; state, behaviour, emotions, e.g. attention, drowsiness, hands on the wheel, drivers&apos; gaze (&quot;eyes-off-road&quot;), potential alcohol consumption, etc.</paragraph-text><paragraph-text type="body">Recognising the state of vehicle controls, e.g. dashboard indicators such as speedometers, fuel meters, etc.</paragraph-text><paragraph-text type="body">The recognition may be performed on images taken from an on-board camera located within the vehicle or from images taken from cameras located outside of the vehicle.</paragraph-text><paragraph-text type="body">Illustrative examples of subject matter classified in this place:</paragraph-text><paragraph-text type="body">1.</paragraph-text><paragraph-text type="body"><media id="media99.png" file-name="cpc-def-G06V-0099.png" type="png" preferred-width="6.75cm" preferred-height="8.61cm"/></paragraph-text><paragraph-text type="body">Recognising indicators on the dashboard.</paragraph-text><paragraph-text type="body">2.</paragraph-text><paragraph-text type="body"><media id="media100.png" file-name="cpc-def-G06V-0100.png" type="png" preferred-width="16cm" preferred-height="6.74cm"/></paragraph-text><paragraph-text type="body">Detecting faces within a vehicle, when the camera is located outside of the vehicle.</paragraph-text></section-body></definition-statement><references><section-title>References</section-title><informative-references><section-title>Informative references</section-title><section-body><paragraph-text type="preamble">Attention is drawn to the following places, which may be of interest for search:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Recognition of human or animal bodies, e.g. pedestrians</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V40/10</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Recognition of eye characteristics within images or video, e.g. of the iris</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V40/18</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Recognition of movement or behaviour</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V40/20</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Measuring devices for psychotechnics for vehicle drivers</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">A61B5/18</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Safety devices for propulsion unit control, specially adapted for, or arranged in, vehicles, responsive to condition of driver</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">B60K28/02</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Estimation or calculation of driving parameters for road vehicle drive control systems not related to the control of a particular sub-unit, related to drivers or passengers</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">B60W40/08</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Analysis of motion in images</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06T7/20</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Alarms for indicating a condition of sleep, e.g. anti-dozing alarms</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G08B21/06</class-ref></paragraph-text></table-column></table-row></table></section-body></informative-references></references></definition-item>
<definition-item date-revised="2024-08-01"><classification-symbol scheme="cpc">G06V20/60</classification-symbol><definition-title>Type of objects</definition-title><references><section-title>References</section-title><informative-references><section-title>Informative references</section-title><section-body><paragraph-text type="preamble">Attention is drawn to the following places, which may be of interest for search:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Context or environment of the image</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V20/50</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Character recognition; Recognising digital ink; Document-oriented image-based pattern recognition</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V30/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Recognition of biometric, human-related or animal-related patterns in image or video data</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V40/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Measuring arrangements characterised by the use of optical means</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G01B11/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Three dimensional [3D] modelling for computer graphics</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06T17/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Manipulating 3D models or images for computer graphics</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06T19/00</class-ref></paragraph-text></table-column></table-row></table></section-body></informative-references></references></definition-item>
<definition-item date-revised="2024-08-01"><classification-symbol scheme="cpc">G06V20/62</classification-symbol><definition-title>Text, e.g. of license plates, overlay texts or captions on TV images</definition-title><definition-statement><section-title>Definition statement</section-title><section-body><paragraph-text type="preamble">This place covers:</paragraph-text><paragraph-text type="body">Detection and recognition of text or logo regions in scene imagery, e.g. detection and recognition of street names, business logos or names, license plate numbers, or numbers on the clothing of players in a sporting activity.</paragraph-text><paragraph-text type="body">Localising and recognising text regions on postal items, parcels or containers.</paragraph-text><paragraph-text type="body">Detection and recognition of overlay text in broadcast video, including embedded captions in TV videos or images.</paragraph-text><paragraph-text type="body">Illustrative examples of subject matter classified in this place:</paragraph-text><paragraph-text type="body">1.</paragraph-text><paragraph-text type="body"><media id="media101.png" file-name="cpc-def-G06V-0101.png" type="png" preferred-width="6.01cm" preferred-height="4.3cm"/></paragraph-text><paragraph-text type="body">Image wherein the overlay text (detection and recognition of the TV station) is recognised.</paragraph-text><paragraph-text type="body">2.</paragraph-text><paragraph-text type="body"><media id="media102.png" file-name="cpc-def-G06V-0102.png" type="png" preferred-width="6.69cm" preferred-height="4.3cm"/></paragraph-text><paragraph-text type="body">Image wherein the overlay text (the logo License plate) is recognised.</paragraph-text></section-body></definition-statement><references><section-title>References</section-title><informative-references><section-title>Informative references</section-title><section-body><paragraph-text type="preamble">Attention is drawn to the following places, which may be of interest for search:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Image preprocessing for image or video recognition or understanding involving the determination of region of interest [ROI] or a volume of interest [VOI]</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/25</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Character recognition; Recognising digital ink; Document-orientated image-based pattern recognition</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V30/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Image analysis in general</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06T7/00</class-ref></paragraph-text></table-column></table-row></table></section-body></informative-references></references><glossary-of-terms><section-title>Glossary of terms</section-title><section-body><paragraph-text type="preamble">In this place, the following terms or expressions are used with the meaning indicated:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">overlay text</paragraph-text></table-column><table-column><paragraph-text type="body">text elements superimposed over a video stream</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">regions of interestROI</paragraph-text></table-column><table-column><paragraph-text type="body">samples within images or video identified for a particular purpose</paragraph-text></table-column></table-row></table></section-body></glossary-of-terms><synonyms-keywords><section-title>Synonyms and Keywords</section-title><abbreviations>
<paragraph-text type="preamble">In patent documents, the following abbreviations are often used:</paragraph-text>
<table>
<table-row><table-column><paragraph-text type="body">AOI</paragraph-text></table-column><table-column><paragraph-text type="body">area of interest</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">ROI</paragraph-text></table-column><table-column><paragraph-text type="body">region of interest</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">VOI</paragraph-text></table-column><table-column><paragraph-text type="body">volume of interest</paragraph-text></table-column></table-row></table>
</abbreviations></synonyms-keywords></definition-item>
<definition-item date-revised="2024-08-01"><classification-symbol scheme="cpc">G06V20/64</classification-symbol><definition-title>Three-dimensional objects</definition-title><definition-statement><section-title>Definition statement</section-title><section-body><paragraph-text type="preamble">This place covers:</paragraph-text><paragraph-text type="body">Recognition of objects based on their three-dimensional geometric structure (&quot;3D shape&quot;), potentially also exploiting other visual cues such as surface texture, grey-level image values or colours.</paragraph-text><paragraph-text type="body">Note:</paragraph-text><paragraph-text type="body">The analysed data is three-dimensional in nature, or the reference/template is three-dimensional. The three-dimensional representation can be very varied: depth/range images, also called 2.5D-images (potentially including texture information), point cloud representations, meshes/tessellations/wire frames or finite element representations, voxel representations, representations as manifolds (continuous, smooth or Riemannian manifolds; using local charts; as null sets of a certain set of functions, etc.). The majority of the techniques involved recognise the 3D-surface, or part of the 3D-surface (&quot;front side relative to a camera&quot;) of the three-dimensional object rather than the interior or its volume.</paragraph-text><paragraph-text type="body">Illustrative examples of subject matter classified in this place:</paragraph-text><paragraph-text type="body">1A.</paragraph-text><paragraph-text type="body"><media id="media103.png" file-name="cpc-def-G06V-0103.png" type="png" preferred-width="15.64cm" preferred-height="10.67cm"/></paragraph-text><paragraph-text type="body">1B.</paragraph-text><paragraph-text type="body"><media id="media104.png" file-name="cpc-def-G06V-0104.png" type="png" preferred-width="12.07cm" preferred-height="7.81cm"/></paragraph-text><paragraph-text type="body">1C.</paragraph-text><paragraph-text type="body"><media id="media105.png" file-name="cpc-def-G06V-0105.png" type="png" preferred-width="10.14cm" preferred-height="7.94cm"/></paragraph-text><paragraph-text type="body">3D object recognition for guiding a robot gripper.</paragraph-text></section-body></definition-statement><references><section-title>References</section-title><application-references><section-title>Application-oriented references</section-title><section-body><paragraph-text type="preamble">Examples of places where the subject matter of this place is covered when specially adapted, used for a particular purpose, or incorporated in a larger system:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Pattern recognition and image understanding in terrestrial scenes with images taken by planes or drones</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V20/17</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Surveillance or monitoring of activities, e.g. for recognising suspicious objects</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V20/52</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Recognition of traffic patterns, e.g. cars on the road, trains or boats</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V20/54</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Recognition of scenes exterior to a vehicle by using sensors mounted on the vehicle</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V20/56</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Recognition or understanding of scenes inside of a vehicle</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V20/59</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Recognition of trinkets, e.g. jewellery items, buttons, gun bullets, medication pills</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V20/66</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Recognition of food in scenes</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V20/68</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Recognition of microscopic objects in scenes</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V20/69</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Recognition of biometric, human-related or animal-related patterns</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V40/00</class-ref></paragraph-text></table-column></table-row></table></section-body></application-references><informative-references><section-title>Informative references</section-title><section-body><paragraph-text type="preamble">Attention is drawn to the following places, which may be of interest for search:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Extraction of image or video features using descriptors for shape, contour or point-related descriptors</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/46</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Character recognition; Recognising digital ink; Document-oriented image-based pattern recognition</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V30/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Measuring arrangements characterised by the use of optical means</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G01B11/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Measuring contours or curvatures by projecting a pattern, e.g. moir&#233; fringes on the object</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G01B11/25</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Systems using the reflection or reradiation of electromagnetic waves other than radio waves, e.g. lidar systems</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G01S17/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Image analysis in general</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06T7/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Two dimensional [2D] image generation</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06T11/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Three dimensional [3D] image rendering</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06T15/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Three dimensional [3D] modelling, e.g. data description of 3D objects</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06T17/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Manipulating 3D models or images for computer graphics</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06T19/00</class-ref></paragraph-text></table-column></table-row></table></section-body></informative-references></references><special-rules><section-title>Special rules of classification</section-title><section-body><paragraph-text type="body">Sometimes special illumination (e.g. that produced by grating patterns) is cast into the scene to gather local 3D shape information. In such cases, classification in groups <class-ref scheme="cpc">G06V10/145</class-ref> and <class-ref scheme="cpc">G06V20/64</class-ref> is applied.</paragraph-text></section-body></special-rules><glossary-of-terms><section-title>Glossary of terms</section-title><section-body><paragraph-text type="preamble">In this place, the following terms or expressions are used with the meaning indicated:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">2.5D image</paragraph-text></table-column><table-column><paragraph-text type="body">image that simulates the appearance of being three-dimensional when in fact it is 2D</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">manifold</paragraph-text></table-column><table-column><paragraph-text type="body">topological space with the property that each point has a neighbourhood that is homeomorphic to an open subset of n-dimensional Euclidean space</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">mesh</paragraph-text></table-column><table-column><paragraph-text type="body">collection of vertices, edges and faces that defines the shape of a polyhedral object. Also known as a polygon mesh.</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">tessellation</paragraph-text></table-column><table-column><paragraph-text type="body">dividing of data sets of polygons, i.e. vertex sets, presenting objects in a scene into suitable structures for rendering. In real-time rendering, the data is tessellated into triangles, also known as polygon triangulation</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">topology</paragraph-text></table-column><table-column><paragraph-text type="body">properties of a geometric object that are preserved under continuous deformations</paragraph-text></table-column></table-row></table></section-body></glossary-of-terms></definition-item>
<definition-item date-revised="2024-08-01"><classification-symbol scheme="cpc">G06V20/66</classification-symbol><definition-title>Trinkets, e.g. shirt buttons or jewellery items  (recognising microscopic objects <class-ref scheme="cpc">G06V20/69</class-ref>)</definition-title><definition-statement><section-title>Definition statement</section-title><section-body><paragraph-text type="preamble">This place covers:</paragraph-text><paragraph-text type="body">Detection, recognition (e.g. clustering, classification) of personal accessories or small objects of personal use such as:</paragraph-text><list><list-item><paragraph-text type="body">Shirt buttons;</paragraph-text></list-item><list-item><paragraph-text type="body">Stamps;</paragraph-text></list-item><list-item><paragraph-text type="body">Gun bullets;</paragraph-text></list-item><list-item><paragraph-text type="body">Jewellery items;</paragraph-text></list-item><list-item><paragraph-text type="body">Coins;</paragraph-text></list-item><list-item><paragraph-text type="body">Drugs, pills, ampoules.</paragraph-text></list-item></list><paragraph-text type="body">Recognition of keys for door locks.</paragraph-text><paragraph-text type="body">Recognition of such objects for counting and tracking of these objects.</paragraph-text><paragraph-text type="body">Illustrative example of subject matter classified in this place:</paragraph-text><paragraph-text type="body"><media id="media106.png" file-name="cpc-def-G06V-0106.png" type="png" preferred-width="16cm" preferred-height="11.72cm"/></paragraph-text></section-body></definition-statement><references><section-title>References</section-title><limiting-references><section-title>Limiting references</section-title><section-body><paragraph-text type="preamble">This place does not cover:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Recognising microscopic objects in scenes</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V20/69</class-ref></paragraph-text></table-column></table-row></table></section-body></limiting-references><informative-references><section-title>Informative references</section-title><section-body><paragraph-text type="preamble">Attention is drawn to the following places, which may be of interest for search:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Local feature extraction by analysis of parts of the pattern, e.g. by detecting edges, contours, loops, corners, strokes or intersections; Connectivity analysis, e.g. of connected components, edge linking or neighbouring slice analysis</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/44</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Recognising image objects characterised by unique random patterns</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V20/80</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Methods or arrangements for sensing record carriers</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06K7/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Image analysis in general</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06T7/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Testing specially adapted to determine the identity or genuineness of valuable papers or for segregating those which are unacceptable, e.g. banknotes that are alien to a currency</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G07D7/00</class-ref></paragraph-text></table-column></table-row></table></section-body></informative-references></references></definition-item>
<definition-item date-revised="2024-08-01"><classification-symbol scheme="cpc">G06V20/68</classification-symbol><definition-title>Food, e.g. fruit or vegetables</definition-title><definition-statement><section-title>Definition statement</section-title><section-body><paragraph-text type="preamble">This place covers:</paragraph-text><paragraph-text type="body">Detection, recognition or classification of food items, e.g. on shelves in a supermarket, at the cashier, inside the cart, inside a fridge, inside an oven, etc.</paragraph-text><paragraph-text type="body">Example applications include determining the freshness of the food, determining the portion size, computing the calories intake based on the recognised ingredients.</paragraph-text><paragraph-text type="body">Illustrative example of subject matter classified in this place:</paragraph-text><paragraph-text type="body"><media id="media107.png" file-name="cpc-def-G06V-0107.png" type="png" preferred-width="16cm" preferred-height="13.1cm"/></paragraph-text><paragraph-text type="body">Recognition of the food on the plate using a two-stage process, object localisation followed by object classification.</paragraph-text></section-body></definition-statement><references><section-title>References</section-title><informative-references><section-title>Informative references</section-title><section-body><paragraph-text type="preamble">Attention is drawn to the following places, which may be of interest for search:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Object recognition in augmented reality scenes</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V20/20</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Recognising three-dimensional [3D] objects in scenes</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V20/64</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Recognising image objects characterised by unique random patterns</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V20/80</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Foods, foodstuffs</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">A23L</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Hand carts having more than one axis carrying transport wheels; Steering devices therefor; Equipment therefor</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">B62B3/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Payment architectures, schemes or protocols</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06Q20/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Commerce, e.g. shopping or e-commerce</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06Q30/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Cash registers</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G07G1/00</class-ref></paragraph-text></table-column></table-row></table></section-body></informative-references></references><special-rules><section-title>Special rules of classification</section-title><section-body><paragraph-text type="body">Food recognition is usually performed in an interactive fashion by displaying it on the screen of a mobile phone or in an augmented-reality set-up. In such case, classification in groups <class-ref scheme="cpc">G06V20/20</class-ref> (augmented reality scenes) and <class-ref scheme="cpc">G06V20/68</class-ref> is applied.</paragraph-text></section-body></special-rules></definition-item>
<definition-item date-revised="2024-08-01"><classification-symbol scheme="cpc">G06V20/69</classification-symbol><definition-title>Microscopic objects, e.g. biological cells or cellular parts</definition-title><definition-statement><section-title>Definition statement</section-title><section-body><paragraph-text type="preamble">This place covers:</paragraph-text><paragraph-text type="body">Detection, recognition, clustering, or classification of:</paragraph-text><list><list-item><paragraph-text type="body">biological cells and cellular parts, e.g., cytoplasm, nucleus, cell membrane, chromosomes, cilia, flagella, etc. of all kinds of cells: prokaryotes, eukaryotes, bacteria, etc.;</paragraph-text></list-item><list-item><paragraph-text type="body">other microscopic biological material such as pollen grains;</paragraph-text></list-item><list-item><paragraph-text type="body">images of virus strains;</paragraph-text></list-item><list-item><paragraph-text type="body">crystals.</paragraph-text></list-item></list><paragraph-text type="body">Recognition of such objects for counting and tracking.</paragraph-text><paragraph-text type="body">Detection and classification of certain events (e.g. cellular division, development of an anomaly, detection of replication etc.).</paragraph-text><paragraph-text type="body">Illustrative examples of subject matter classified in this place:</paragraph-text><paragraph-text type="body">1A.</paragraph-text><paragraph-text type="body"><media id="media108.png" file-name="cpc-def-G06V-0108.png" type="png" preferred-width="3.05cm" preferred-height="2.71cm"/></paragraph-text><paragraph-text type="body">1B</paragraph-text><paragraph-text type="body"><media id="media109.png" file-name="cpc-def-G06V-0109.png" type="png" preferred-width="2.86cm" preferred-height="2.58cm"/></paragraph-text><paragraph-text type="body">1C.</paragraph-text><paragraph-text type="body"><media id="media110.png" file-name="cpc-def-G06V-0110.png" type="png" preferred-width="2.86cm" preferred-height="2.54cm"/></paragraph-text><paragraph-text type="body">1D.</paragraph-text><paragraph-text type="body"><media id="media111.png" file-name="cpc-def-G06V-0111.png" type="png" preferred-width="2.75cm" preferred-height="3.22cm"/></paragraph-text><paragraph-text type="body">Detection and recognition of cells in microscopic images.</paragraph-text></section-body></definition-statement><references><section-title>References</section-title><informative-references><section-title>Informative references</section-title><section-body><paragraph-text type="preamble">Attention is drawn to the following places, which may be of interest for search:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Measuring or testing for enzymology or microbiology with condition measuring or sensing means, e.g. colony counters</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">C12M1/34</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Measuring or testing processes involving enzymes, nucleic acids or microorganisms; Compositions thereof; Processes of preparing such compositions</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">C12Q1/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Spectrometry; Spectrophotometry; Monochromators; Measuring colours</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G01J3/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Investigating characteristics or properties of individual particles using electro-optical means</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G01N15/14</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Investigating or analysing materials by the use of optical means by use of fluorescence or phosphorescence</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G01N21/64</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Investigating or analysing materials by specific methods not covered by groups <class-ref scheme="cpc">G01N1/00</class-ref>-<class-ref scheme="cpc">G01N31/00</class-ref>; Analysis of biological material, e.g. blood, urine</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G01N33/48</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Microscopes</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G02B21/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Image analysis in general</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06T7/00</class-ref></paragraph-text></table-column></table-row></table></section-body></informative-references></references><glossary-of-terms><section-title>Glossary of terms</section-title><section-body><paragraph-text type="preamble">In this place, the following terms or expressions are used with the meaning indicated:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">hyperspectral image</paragraph-text></table-column><table-column><paragraph-text type="body">multi-band image where the z dimension corresponds to consecutive spectral wavelengths ranges</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">multispectral image</paragraph-text></table-column><table-column><paragraph-text type="body">multi-band image where the z dimension corresponds to spectral wavelengths ranges (not necessarily consecutive)</paragraph-text></table-column></table-row></table></section-body></glossary-of-terms></definition-item>
<definition-item date-revised="2024-08-01"><classification-symbol scheme="cpc">G06V20/70</classification-symbol><definition-title>Labelling scene content, e.g. deriving syntactic or semantic representations</definition-title><definition-statement><section-title>Definition statement</section-title><section-body><paragraph-text type="preamble">This place covers:</paragraph-text><paragraph-text type="body">Automatic annotation or labelling of scenes.</paragraph-text><paragraph-text type="body">Semantic segmentation of scenes, e.g. by means of labelling each pixel of an image with a corresponding class of what is being represented. This process can be seen as image classification at pixel level.</paragraph-text><paragraph-text type="body">Syntactic segmentation of scenes, e.g. by means of using the structural representation of an image. Examples of structural representations include grammars or graphs. This process can be used instead of statistical pattern recognition when there is a clear structure in the pattern.</paragraph-text><paragraph-text type="body">Illustrative examples of subject matter classified in this place:</paragraph-text><paragraph-text type="body">1.</paragraph-text><paragraph-text type="body"><media id="media112.png" file-name="cpc-def-G06V-0112.png" type="png" preferred-width="16cm" preferred-height="7.1cm"/></paragraph-text><paragraph-text type="body">Semantic segmentation of hair wherein a tiered structure constraint has been used for determining the labels of the pixels.</paragraph-text><paragraph-text type="body">2.</paragraph-text><paragraph-text type="body"><media id="media113.png" file-name="cpc-def-G06V-0113.png" type="png" preferred-width="13.65cm" preferred-height="11.09cm"/></paragraph-text><paragraph-text type="body">Labelling of image objects according to known object classes.</paragraph-text></section-body></definition-statement><references><section-title>References</section-title><informative-references><section-title>Informative references</section-title><section-body><paragraph-text type="preamble">Attention is drawn to the following places, which may be of interest for search:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Image preprocessing for image or video recognition or understanding involving the determination of region of interest [ROI] or a volume of interest [VOI]</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/25</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Segmentation of patterns in the image field; Cutting or merging image elements to establish the pattern region, e.g. region growing, watershed or clustering-based techniques; Detection of occlusion</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/26</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Recognition using syntactic or structural representations of the image or video pattern, e.g. symbolic string recognition; Graph matching</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/86</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Techniques for post-processing in character recognition using context analysis, e.g. lexical, syntactic or semantic context</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V30/262</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Information retrieval; Database structures therefor; File system structures therefor</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06F16/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Image analysis by segmentation or edge detection in general</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06T7/10</class-ref></paragraph-text></table-column></table-row></table></section-body></informative-references></references><glossary-of-terms><section-title>Glossary of terms</section-title><section-body><paragraph-text type="preamble">In this place, the following terms or expressions are used with the meaning indicated:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">dense prediction</paragraph-text></table-column><table-column><paragraph-text type="body">labelling each pixel of an image or video with a corresponding class of what is being represented.</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">semantic image segmentation</paragraph-text></table-column><table-column><paragraph-text type="body">labelling regions (e.g. set of pixels) of an image with a corresponding object class of what is being represented.</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">syntactic pattern recognition</paragraph-text></table-column><table-column><paragraph-text type="body">a form of pattern recognition, in which each object can be represented by a variable-cardinality set of symbolic, nominal features. This allows for representing pattern structures, taking into account more complex interrelationships between attributes than is possible in the case of flat, numerical feature vectors of fixed dimensionality, that are used in statistical classification.</paragraph-text></table-column></table-row></table></section-body></glossary-of-terms></definition-item>
<definition-item date-revised="2024-08-01"><classification-symbol scheme="cpc">G06V20/80</classification-symbol><definition-title>Recognising image objects characterised by unique random patterns</definition-title><definition-statement><section-title>Definition statement</section-title><section-body><paragraph-text type="preamble">This place covers:</paragraph-text><paragraph-text type="body">Authentication of objects or products by physically unclonable function [PUF].</paragraph-text><paragraph-text type="body">Identification of counterfeit goods by PUF.</paragraph-text><paragraph-text type="body">Identification by micro-random structures naturally occurring on the surface of an object.</paragraph-text><paragraph-text type="body">Identification by applying specially designed micro-structures to the surface of an object, e.g. quantum dots or nano-barcodes or ink containing magnetic particles.</paragraph-text><paragraph-text type="body">Encoding the extracted PUF and digitally storing the code for retrieval or printing the code on surface of the object for authentication.</paragraph-text><paragraph-text type="body">Recognition of PUF which change their appearance depending on the incident angle of the illumination.</paragraph-text><paragraph-text type="body">Recognition of PUF by dedicated or general purpose devices; mostly microscopes; they can be fixed, for example in an industrial context, or mobile (e.g. microscope attached to mobile phone or mobile phone with very large zoom), for example for identifying counterfeit goods by user.</paragraph-text><paragraph-text type="body">Examples of objects and products that may be authenticated by this technique include:</paragraph-text><list><list-item><paragraph-text type="body">pharmaceutical and cosmetics products;</paragraph-text></list-item><list-item><paragraph-text type="body">individual pills or packaged substances;</paragraph-text></list-item><list-item><paragraph-text type="body">electronics;</paragraph-text></list-item><list-item><paragraph-text type="body">luxury goods, e.g. watches;</paragraph-text></list-item><list-item><paragraph-text type="body">text documents and certificates;</paragraph-text></list-item><list-item><paragraph-text type="body">weapons;</paragraph-text></list-item><list-item><paragraph-text type="body">agricultural products, e.g. fruits;</paragraph-text></list-item><list-item><paragraph-text type="body">recipients for bio-medical probes.</paragraph-text></list-item></list><paragraph-text type="body">Illustrative example of subject matter classified in this place:</paragraph-text><paragraph-text type="body"><media id="media114.png" file-name="cpc-def-G06V-0114.png" type="png" preferred-width="6.52cm" preferred-height="3.32cm"/></paragraph-text><paragraph-text type="body">Analysis of the random patterns in a material by casting light and encoding the resulting speckle pattern using PUF.</paragraph-text></section-body></definition-statement><relationship><section-title>Relationships with other classification places</section-title><section-body><paragraph-text type="body">While group <class-ref scheme="cpc">G06V20/80</class-ref> aims at recognising objects depicted in the image from their random patterns, group <class-ref scheme="cpc">G06V20/90</class-ref> assumes that the image is analysed without necessarily identifying the image objects. The purpose of the analysis of group <class-ref scheme="cpc">G06V20/90</class-ref> is to assess, based on image imperfections generated by the sensor, whether the image has been captured by the same sensor or not.</paragraph-text></section-body></relationship><references><section-title>References</section-title><informative-references><section-title>Informative references</section-title><section-body><paragraph-text type="preamble">Attention is drawn to the following places, which may be of interest for search:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Investigating or analysing materials by the use of optical means</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G01N21/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Commerce, e.g. shopping or e-commerce</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06Q30/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">General purpose image data processing</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06T1/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Testing specially adapted to determine the identity or genuineness of valuable papers or for segregating those which are unacceptable, e.g. banknotes that are alien to a currency</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G07D7/00</class-ref></paragraph-text></table-column></table-row></table></section-body></informative-references></references><special-rules><section-title>Special rules of classification</section-title><section-body><paragraph-text type="body">Highlighting the non-uniformities in the objects subject to analysis usually involves casting special light (e.g. having a certain spectral content which matches those of the non-uniformities) or using special sensors, which is classified in group <class-ref scheme="cpc">G06V10/10</class-ref> and its subgroups. In such case, classification in groups <class-ref scheme="cpc">G06V10/10</class-ref> and <class-ref scheme="cpc">G06V20/80</class-ref> is applied.</paragraph-text></section-body></special-rules><glossary-of-terms><section-title>Glossary of terms</section-title><section-body><paragraph-text type="preamble">In this place, the following terms or expressions are used with the meaning indicated:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">PUFdigital fingerprintsecurity markphysical dispersion patternphysical scatter pattern</paragraph-text></table-column><table-column><paragraph-text type="body">physical unclonable function</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">physically unclonable features</paragraph-text></table-column><table-column><paragraph-text type="body">unique features on the surface of objects, products or documents which uniquely identify the object, in a manner similar to how fingerprints uniquely identify a person; these unique features may be naturally occurring or purposefully added random microstructures on the physical object surface.</paragraph-text></table-column></table-row></table></section-body></glossary-of-terms></definition-item>
<definition-item date-revised="2024-08-01"><classification-symbol scheme="cpc">G06V20/90</classification-symbol><definition-title>Identifying an image sensor based on its output data</definition-title><definition-statement><section-title>Definition statement</section-title><section-body><paragraph-text type="preamble">This place covers:</paragraph-text><paragraph-text type="body">Identifying an image sensor based on characteristic sensor noise patterns, sensor imperfections, artifacts, or optical defects. Defective pixels which, individually, are normally not perceptible to the human eye may be detected, and the repeatability of their occurrence at the same spatial position may be used for sensor/camera identification.</paragraph-text><paragraph-text type="subheading">Notes &#8211; technical background</paragraph-text><paragraph-text type="body">These notes provide more information about the technical subject matter that is classified in this place:</paragraph-text><paragraph-text type="body">The process of digital camera identification may involve three steps:</paragraph-text><paragraph-text type="body">1. Photo response non-uniformity (PRNU) noise extraction. The PRNU-pattern from the image under investigation is extracted using a denoising filter;</paragraph-text><paragraph-text type="body">2. Extraction of sensor pattern noise (SPN), also known as the camera fingerprint, is obtained by taking a series of flat-field images with the camera under investigation. </paragraph-text><paragraph-text type="body">From each image the PRNU-pattern is extracted and then these patterns are combined to estimate the SPN;</paragraph-text><paragraph-text type="body">3. Comparison. The SPN-pattern of the camera and the PRNU-pattern of the image are compared by calculating for example a correlation metric.</paragraph-text><paragraph-text type="body">Illustrative example of subject matter classified in this place:</paragraph-text><paragraph-text type="body"><media id="media115.png" file-name="cpc-def-G06V-0115.png" type="png" preferred-width="16cm" preferred-height="9.1cm"/></paragraph-text><paragraph-text type="body">Determining whether the two images are taken by the same camera, by implementing the basic three-step process described above.</paragraph-text></section-body></definition-statement><references><section-title>References</section-title><informative-references><section-title>Informative references</section-title><section-body><paragraph-text type="preamble">Attention is drawn to the following places, which may be of interest for search:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Image enhancement or restoration</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06T5/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Speaker identification or verification</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G10L17/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Details of television systems</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">H04N5/00</class-ref></paragraph-text></table-column></table-row></table></section-body></informative-references></references><glossary-of-terms><section-title>Glossary of terms</section-title><section-body><paragraph-text type="preamble">In this place, the following terms or expressions are used with the meaning indicated:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">fixed pattern noise [FPN]</paragraph-text></table-column><table-column><paragraph-text type="body">additive noise caused by dark currents when the sensor array is not exposed to light</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">hardwaremetry</paragraph-text></table-column><table-column><paragraph-text type="body">process of searching for characteristic features for identifying an image sensor</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">photo response non-uniformity [PRNU]</paragraph-text></table-column><table-column><paragraph-text type="body">major source of noise caused when pixels have different light sensitivities caused by the inhomogeneity of silicon wafers</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">SNR</paragraph-text></table-column><table-column><paragraph-text type="body">signal-to-noise ratio</paragraph-text></table-column></table-row></table></section-body></glossary-of-terms></definition-item>
<definition-item date-revised="2024-08-01"><classification-symbol scheme="cpc">G06V30/00</classification-symbol><definition-title>Character recognition; Recognising digital ink; Document-oriented image-based pattern recognition  (scanning, transmission or reproduction of documents or the like <class-ref scheme="cpc">H04N1/00</class-ref>)</definition-title><definition-statement><section-title>Definition statement</section-title><section-body><paragraph-text type="preamble">This place covers:</paragraph-text><paragraph-text type="body">Acquisition, preprocessing, segmentation, feature extraction and recognition of characters that are represented as an image:</paragraph-text><list><list-item><paragraph-text type="body">optical character recognition [OCR] if the text to be recognised consists of machine printed characters;</paragraph-text></list-item><list-item><paragraph-text type="body">offline handwriting symbol and character recognition for different alphabets (e.g. Latin, Kanji, Hiragana, Katakana, etc.).</paragraph-text></list-item></list><paragraph-text type="body">Preprocessing, segmentation, feature extraction and recognition of digital ink (i.e. online handwritten character recognition), where the characters are represented as temporal sequences of handwritten position coordinates, in the form of order-dependent strokes. </paragraph-text><paragraph-text type="body">The analysis may rely on order-independent strokes where point coordinates are represented without temporal information (i.e. offline handwritten character recognition).</paragraph-text><paragraph-text type="body">The above representations include representations in three dimensions, e.g. as written by performing gestures in the air.</paragraph-text><paragraph-text type="body">Document analysis, recognition and understanding, where the document is represented as an image. Possible application scenarios are business forms, standard forms, graphical technical drawings, geographical maps, parcels, letters, credit cards, cheques, etc.</paragraph-text></section-body></definition-statement><references><section-title>References</section-title><limiting-references><section-title>Limiting references</section-title><section-body><paragraph-text type="preamble">This place does not cover:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Scanning, transmission or reproduction of documents or the like</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">H04N1/00</class-ref></paragraph-text></table-column></table-row></table></section-body></limiting-references><application-references><section-title>Application-oriented references</section-title><section-body><paragraph-text type="preamble">Examples of places where the subject matter of this place is covered when specially adapted, used for a particular purpose, or incorporated in a larger system:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Sorting of mail or documents using means for detecting the destination</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">B07C3/10</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Input arrangements for interaction between user and computer</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06F3/01</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Testing specially adapted to determine the identity or genuineness of valuable papers or for segregating those which are unacceptable</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G07D7/00</class-ref></paragraph-text></table-column></table-row></table></section-body></application-references><informative-references><section-title>Informative references</section-title><section-body><paragraph-text type="preamble">Attention is drawn to the following places, which may be of interest for search:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Optical elements, systems or apparatus</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G02B</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06F3/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Handling natural language data</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06F40/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Image contour coding, e.g. using detection of edges</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06T9/20</class-ref></paragraph-text></table-column></table-row></table></section-body></informative-references></references><glossary-of-terms><section-title>Glossary of terms</section-title><section-body><paragraph-text type="preamble">In this place, the following terms or expressions are used with the meaning indicated:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">OCR</paragraph-text></table-column><table-column><paragraph-text type="body">optical character recognition, recognising a machine printed symbol or character based on an image</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">offline handwriting recognition</paragraph-text></table-column><table-column><paragraph-text type="body">recognising a handwritten symbol or character based on an image, i.e. without any temporal information. The difference with respect to OCR is that the symbols or characters are handwritten.</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">online handwriting recognition</paragraph-text></table-column><table-column><paragraph-text type="body">recognising a handwritten symbol or character based on time-series of handwritten coordinates, i.e. with temporal information</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">strokes</paragraph-text></table-column><table-column><paragraph-text type="body">basic components of characters that are either separated spatially and/or temporally, e.g. contiguous segments left by a writing instrument during handwriting</paragraph-text></table-column></table-row></table></section-body></glossary-of-terms><synonyms-keywords><section-title>Synonyms and Keywords</section-title><special-meanings>
<paragraph-text type="preamble">In patent documents, the following words/expressions are often used with the meaning indicated:</paragraph-text>
<table>
<table-row><table-column><paragraph-text type="body">stroke order independent</paragraph-text></table-column><table-column><paragraph-text type="body">analysis where the temporal order of the strokes is not relevant (i.e. offline handwriting recognition)</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">stroke order dependent</paragraph-text></table-column><table-column><paragraph-text type="body">analysis where the temporal order of the strokes is relevant (i.e. online handwriting recognition)</paragraph-text></table-column></table-row></table>
</special-meanings></synonyms-keywords></definition-item>
<definition-item date-revised="2024-08-01"><classification-symbol scheme="cpc">G06V30/10</classification-symbol><definition-title>Character recognition</definition-title><definition-statement><section-title>Definition statement</section-title><section-body><paragraph-text type="preamble">This place covers:</paragraph-text><paragraph-text type="body">Recognition of characters, wherein the characters have been generated by machine or by handwriting.</paragraph-text><paragraph-text type="body">Recognition of characters, wherein the characters have been generated by machine or by handwriting.</paragraph-text><paragraph-text type="body">Image acquisition specially adapted for OCR and/or recognition of handwritten text using handheld instruments (e.g. with touch screens).</paragraph-text><paragraph-text type="body">Instruments generating sequences of position coordinates corresponding to handwriting.</paragraph-text><paragraph-text type="body">OCR of symbols and characters for any language.</paragraph-text><paragraph-text type="body">Stroke segmentation and recognition of whole cursive handwritten words, i.e. whose letters are not separated but are linked together, whether from offline (image representation) or from online (digital ink, e.g. pen input) acquisition:</paragraph-text><paragraph-text type="body">Preprocessing, feature extraction, matching, recognition and classification of all kinds of handwritten characters, symbols, drawings, except signatures, on the basis of trajectories as a function of time of a stylus, finger, etc. Trajectories can be acquired by a touch pad/screen or by a stylus like device (in collaboration with a passive or active surface).</paragraph-text><paragraph-text type="body">Segmentation of strokes, characters or words.</paragraph-text><paragraph-text type="body">Text and character recognition using temporal information, e.g. free-form handwriting.</paragraph-text><paragraph-text type="body">Recognition of drawings using temporal information, e.g. sketches, flow charts, graphical or mathematical symbols or formulae, chemical structure formulae, editorial notes, proof marks.</paragraph-text><paragraph-text type="body">Illustrative examples of subject matter classified in this place:</paragraph-text><paragraph-text type="body">1.</paragraph-text><paragraph-text type="body"><media id="media116.png" file-name="cpc-def-G06V-0116.png" type="png" preferred-width="14.08cm" preferred-height="7.6cm"/></paragraph-text><paragraph-text type="body">Recognition of handwritten English text input via a touch screen.</paragraph-text><paragraph-text type="body">2.</paragraph-text><paragraph-text type="body"><media id="media117.png" file-name="cpc-def-G06V-0117.png" type="png" preferred-width="11.01cm" preferred-height="7.7cm"/></paragraph-text><paragraph-text type="body">Recognition of handwritten Chinese text input via a touch screen.</paragraph-text><paragraph-text type="body">3.</paragraph-text><paragraph-text type="body"><media id="media118.png" file-name="cpc-def-G06V-0118.png" type="png" preferred-width="10.24cm" preferred-height="3.13cm"/></paragraph-text><paragraph-text type="body">Recognition of different symbols.</paragraph-text></section-body></definition-statement><references><section-title>References</section-title><informative-references><section-title>Informative references</section-title><section-body><paragraph-text type="preamble">Attention is drawn to the following places, which may be of interest for search:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Image preprocessing in arrangements for image or video recognition or understanding</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/20</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Noise filtering in arrangements for image or video recognition or understanding</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/30</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Image acquisition of characters, digital ink or documents using hand-held devices for recognition purposes</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V30/142</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Context analysis as post-processing after provisional recognition</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V30/262</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Writer recognition; Reading or verifying signatures</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V40/30</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Arrangements for converting the position or the displacement of a member into a coded form, for input arrangements or input/output arrangements for user-computer interaction</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06F3/03</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Inputting data by handwriting, e.g. gestures or text, to a computer via a graphical user interface [GUI] using a touch-screen or digitiser</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06F3/0488</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">User authentication by graphic or iconic representation, in security arrangements for protecting computers, components thereof, programs or data against unauthorised activity</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06F21/36</class-ref></paragraph-text></table-column></table-row></table></section-body></informative-references></references><glossary-of-terms><section-title>Glossary of terms</section-title><section-body><paragraph-text type="preamble">In this place, the following terms or expressions are used with the meaning indicated:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">digital inkelectronic inke-ink</paragraph-text></table-column><table-column><paragraph-text type="body">technology that digitally represents handwriting, e.g. using a finger or a stylus, in its natural form using temporal information. Digital ink may also be referred to as electronic ink or e-ink.</paragraph-text></table-column></table-row></table></section-body></glossary-of-terms></definition-item>
<definition-item date-revised="2024-08-01"><classification-symbol scheme="cpc">G06V30/12</classification-symbol><definition-title>Detection or correction of errors, e.g. by rescanning the pattern</definition-title><definition-statement><section-title>Definition statement</section-title><section-body><paragraph-text type="preamble">This place covers:</paragraph-text><paragraph-text type="body">Processes and devices for detecting or correcting character recognition errors after image input. The errors can be detected or corrected automatically (e.g. by a computer program), with the help of an operator, or as a semi-automatic process (e.g. by displaying an image on a graphical user interface and requesting human intervention).</paragraph-text><paragraph-text type="body">Detecting errors in particular comprises evaluating the quality of given image or video data with regard to the suitability for being subjected to an automated character recognition process. Typical quality criteria include image sharpness/blurriness, resolution, contrast or brightness.</paragraph-text><paragraph-text type="body">Monitoring print quality by performing character recognition on the prints, wherein quality relates to a property of the characters or of the digital ink insofar as its effect on the accuracy or performance of the character recognition process. Quality within the meaning of this group is a property of the characters or of the digital ink insofar as it has an effect on the accuracy or performance of the pattern recognition process.</paragraph-text><paragraph-text type="body">Illustrative examples of subject matter classified in this place:</paragraph-text><paragraph-text type="body">1.</paragraph-text><paragraph-text type="body"><media id="media119.png" file-name="cpc-def-G06V-0119.png" type="png" preferred-width="15.54cm" preferred-height="8.3cm"/></paragraph-text><paragraph-text type="body">Evaluation of the quality of recognition of the different fields of a bank cheque.</paragraph-text><paragraph-text type="body">2.</paragraph-text><paragraph-text type="body"><media id="media120.png" file-name="cpc-def-G06V-0120.png" type="png" preferred-width="14.05cm" preferred-height="6.88cm"/></paragraph-text><paragraph-text type="body">Evaluation of the quality of recognition of the different fields of a bank cheque.</paragraph-text></section-body></definition-statement><relationship><section-title>Relationships with other classification places</section-title><section-body><paragraph-text type="body">Methods or arrangements for detection or correction of errors covered by this group involve such correction after the acquisition step with the aim of having a reliable input before the subsequent recognition. In contrast, group <class-ref scheme="cpc">G06V30/26</class-ref> covers the methods and arrangements used after recognition with the aim of correcting the final output, by using additional information such as context.</paragraph-text></section-body></relationship><references><section-title>References</section-title><application-references><section-title>Application-oriented references</section-title><section-body><paragraph-text type="preamble">Examples of places where the subject matter of this place is covered when specially adapted, used for a particular purpose, or incorporated in a larger system:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Scenes; Scene-specific elements</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V20/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Recognition of biometric, human-related or animal-related patterns in image or video data</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V40/00</class-ref></paragraph-text></table-column></table-row></table></section-body></application-references><informative-references><section-title>Informative references</section-title><section-body><paragraph-text type="preamble">Attention is drawn to the following places, which may be of interest for search:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Aligning, centring, orientation detection or correction of the image</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/24</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Validation or performance evaluation for pattern recognition or understanding in images or video, in general</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/776</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Detection or correction of errors in image or video recognition or understanding, in general</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/98</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Techniques for post-processing</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V30/26</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Investigating the presence of flaws, defects or contamination in materials by the use of optical means</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G01N21/88</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Investigating the presence of flaws in materials by the use of thermal means</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G01N25/72</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Image analysis</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06T7/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Arrangements for detecting or preventing errors in the digital information received via transmission</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">H04L1/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Scanning, transmission or reproduction of documents or the like</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">H04N1/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Details of television systems</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">H04N5/00</class-ref></paragraph-text></table-column></table-row></table></section-body></informative-references></references><special-rules><section-title>Special rules of classification</section-title><section-body><paragraph-text type="body">Group <class-ref scheme="cpc">G06V30/12</class-ref> may be regarded as relevant to subject matter also classified in other subgroups of group <class-ref scheme="cpc">G06V30/00</class-ref> and so the principles of multiple classification apply.</paragraph-text></section-body></special-rules></definition-item>
<definition-item date-revised="2024-08-01"><classification-symbol scheme="cpc">G06V30/14</classification-symbol><definition-title>Image acquisition</definition-title><definition-statement><section-title>Definition statement</section-title><section-body><paragraph-text type="preamble">This place covers:</paragraph-text><paragraph-text type="body">Image acquisition specially adapted for character recognition.</paragraph-text><paragraph-text type="body">Image acquisition for character recognition using handheld instruments (e.g. with touch screens).</paragraph-text><paragraph-text type="body">Image acquisition for character recognition using handheld instruments generating sequences of position coordinates corresponding to handwriting.</paragraph-text><paragraph-text type="body">Image acquisition for character recognition using a slot moved over the image, discrete sensing elements at predetermined points or automatic curve-following means.</paragraph-text><paragraph-text type="body">Image acquisition for character recognition using alignment or centring of the image pick-up or image-field, e.g. skew correction.</paragraph-text><paragraph-text type="body">Image acquisition for character recognition using segmentation of character regions.</paragraph-text><paragraph-text type="body">Illustrative examples of subject matter classified in this place:</paragraph-text><paragraph-text type="body">1.</paragraph-text><paragraph-text type="body"><media id="media116.png" file-name="cpc-def-G06V-0116.png" type="png" preferred-width="14.08cm" preferred-height="7.6cm"/></paragraph-text><paragraph-text type="body">Acquisition of handwritten text input via a touch screen written with a finger.</paragraph-text><paragraph-text type="body">2.</paragraph-text><paragraph-text type="body"><media id="media121.png" file-name="cpc-def-G06V-0121.png" type="png" preferred-width="4.74cm" preferred-height="3.34cm"/></paragraph-text><paragraph-text type="body">Acquisition of handwritten text input via a touch screen written with a digital pen/stylus.</paragraph-text><paragraph-text type="body">3.</paragraph-text><paragraph-text type="body"><media id="media122.png" file-name="cpc-def-G06V-0122.png" type="png" preferred-width="7.56cm" preferred-height="6.77cm"/></paragraph-text><paragraph-text type="body">Segmentation of characters based on projection profiles.</paragraph-text><paragraph-text type="body">4.</paragraph-text><paragraph-text type="body"><media id="media123.png" file-name="cpc-def-G06V-0123.png" type="png" preferred-width="7.94cm" preferred-height="1.04cm"/></paragraph-text><paragraph-text type="body">Inclination detection and correction before recognition, original slanted text (left), text after correction (right).</paragraph-text></section-body></definition-statement><references><section-title>References</section-title><informative-references><section-title>Informative references</section-title><section-body><paragraph-text type="preamble">Attention is drawn to the following places, which may be of interest for search:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Arrangements for converting the position or the displacement of a member into a coded form, for input arrangements or input/output arrangements for user-computer interaction</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06F3/03</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Inputting data by handwriting, e.g. gestures or text, to a computer via a graphical user interface [GUI] using a touch-screen or digitiser</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06F3/0488</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">User authentication by graphic or iconic representation, in security arrangements for protecting computers, components thereof, programs or data against unauthorised activity</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06F21/36</class-ref></paragraph-text></table-column></table-row></table></section-body></informative-references></references></definition-item>
<definition-item date-revised="2024-08-01"><classification-symbol scheme="cpc">G06V30/16</classification-symbol><definition-title>Image preprocessing</definition-title><definition-statement><section-title>Definition statement</section-title><section-body><paragraph-text type="preamble">This place covers:</paragraph-text><paragraph-text type="body">Image preprocessing specially adapted for character recognition, in particular:</paragraph-text><list><list-item><paragraph-text type="body">Quantising the image signal;</paragraph-text></list-item><list-item><paragraph-text type="body">Noise filtering;</paragraph-text></list-item><list-item><paragraph-text type="body">Normalisation of pattern dimensions;</paragraph-text></list-item><list-item><paragraph-text type="body">Smoothing or thinning of the pattern; skeletonisation.</paragraph-text></list-item></list><paragraph-text type="body">Illustrative examples of subject matter classified in this place:</paragraph-text><paragraph-text type="body">1A-1D.</paragraph-text><paragraph-text type="body"><media id="media124.png" file-name="cpc-def-G06V-0124.png" type="png" preferred-width="12.83cm" preferred-height="17.29cm"/></paragraph-text><paragraph-text type="body">FIG. 1A shows a fragment of an example image with blur in its original state.</paragraph-text><paragraph-text type="body">FIG. 1B shows the fragment shown in FIG. 1A in a restored state after applying a method for restoring blurred images.</paragraph-text><paragraph-text type="body">FIG. 1C shows the fragment shown in FIG. 1B after being binarised.</paragraph-text><paragraph-text type="body">FIG. 1D shows the fragment shown in FIG. 1A after being binarised without applying a method as described herein.</paragraph-text><paragraph-text type="body">2.</paragraph-text><paragraph-text type="body"><media id="media125.png" file-name="cpc-def-G06V-0125.png" type="png" preferred-width="9.46cm" preferred-height="5.14cm"/></paragraph-text><paragraph-text type="body">Left: original character; right: skeletonised version</paragraph-text></section-body></definition-statement><references><section-title>References</section-title><informative-references><section-title>Informative references</section-title><section-body><paragraph-text type="preamble">Attention is drawn to the following places, which may be of interest for search:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Image preprocessing in image or video recognition or understanding</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/20</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Noise filtering in image or video recognition or understanding</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/30</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Image enhancement or restoration, in general</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06T5/00</class-ref></paragraph-text></table-column></table-row></table></section-body></informative-references></references></definition-item>
<definition-item date-revised="2024-08-01"><classification-symbol scheme="cpc">G06V30/18</classification-symbol><definition-title>Extraction of features or characteristics of the image</definition-title><definition-statement><section-title>Definition statement</section-title><section-body><paragraph-text type="preamble">This place covers:</paragraph-text><paragraph-text type="body">Extraction of features or characteristics of the image specifically adapted for character recognition:</paragraph-text><list><list-item><paragraph-text type="body">by coding the contour of the character pattern, e.g. contour-related features;</paragraph-text></list-item><list-item><paragraph-text type="body">by analysing segments intersecting the character pattern, e.g. the segments being obtained with lines, circles drawn on the pattern;</paragraph-text></list-item><list-item><paragraph-text type="body">by deriving mathematical or geometrical properties from the whole character pattern or image, e.g. centre of mass, moments of inertia, etc.</paragraph-text></list-item></list><paragraph-text type="body">Illustrative examples of subject matter classified in this place:</paragraph-text><paragraph-text type="body">1.</paragraph-text><paragraph-text type="body"><media id="media126.png" file-name="cpc-def-G06V-0126.png" type="png" preferred-width="9.44cm" preferred-height="5.44cm"/></paragraph-text><paragraph-text type="body">An explanatory drawing showing an example of each stroke and polygonal approximation.</paragraph-text><paragraph-text type="body">2.</paragraph-text><paragraph-text type="body"><media id="media127.png" file-name="cpc-def-G06V-0127.png" type="png" preferred-width="12.23cm" preferred-height="8.4cm"/></paragraph-text><paragraph-text type="body">Example of the features that are extracted from a &quot;U&quot; handwritten sign based on the cumulative angle feature function.</paragraph-text></section-body></definition-statement><references><section-title>References</section-title><informative-references><section-title>Informative references</section-title><section-body><paragraph-text type="preamble">Attention is drawn to the following places, which may be of interest for search:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Image analysis in general</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06T7/00</class-ref></paragraph-text></table-column></table-row></table></section-body></informative-references></references></definition-item>
<definition-item date-revised="2024-08-01"><classification-symbol scheme="cpc">G06V30/19</classification-symbol><definition-title>Recognition using electronic means</definition-title><definition-statement><section-title>Definition statement</section-title><section-body><paragraph-text type="preamble">This place covers:</paragraph-text><paragraph-text type="body">Recognition of symbols and characters using electronic means specifically adapted to character recognition:</paragraph-text><list><list-item><paragraph-text type="body"> using simultaneous comparisons or correlations of the image signals with a plurality of references, including references that are adjustable by an adaptive method, e.g. learning;</paragraph-text></list-item><list-item><paragraph-text type="body">using sequential comparisons or correlations of the image signals with a plurality of references, wherein at any stage the selection of a reference depends on the result of the preceding comparison.</paragraph-text></list-item></list><paragraph-text type="body">Illustrative examples of subject matter classified in this place:</paragraph-text><paragraph-text type="body">1.</paragraph-text><paragraph-text type="body"><media id="media128.png" file-name="cpc-def-G06V-0128.png" type="png" preferred-width="9.57cm" preferred-height="7.51cm"/></paragraph-text><paragraph-text type="body">Handwritten pattern recognition based on comparison with respect to reference stroke data.</paragraph-text><paragraph-text type="body">2.</paragraph-text><paragraph-text type="body"><media id="media129.png" file-name="cpc-def-G06V-0129.png" type="png" preferred-width="8.68cm" preferred-height="7.15cm"/></paragraph-text><paragraph-text type="body">A convolutional neural network for the recognition of handwritten symbols and characters.</paragraph-text></section-body></definition-statement><references><section-title>References</section-title><informative-references><section-title>Informative references</section-title><section-body><paragraph-text type="preamble">Attention is drawn to the following places, which may be of interest for search:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Text, e.g. of license plates, overlay texts or captions on TV images</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V20/62</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Arrangements for character recognition using optical reference masks, e.g. holographic masks</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V30/199</class-ref></paragraph-text></table-column></table-row></table></section-body></informative-references></references></definition-item>
<definition-item date-revised="2024-08-01"><classification-symbol scheme="cpc">G06V30/22</classification-symbol><definition-title>characterised by the type of writing</definition-title><definition-statement><section-title>Definition statement</section-title><section-body><paragraph-text type="preamble">This place covers:</paragraph-text><paragraph-text type="body">Character recognition characterised by the type of writing, including:</paragraph-text><list><list-item><paragraph-text type="body">recognition of characters separated by spaces, i.e. non-connected characters;</paragraph-text></list-item><list-item><paragraph-text type="body">recognition of printed characters having additional code marks or containing code marks, e.g. the character being composed of individual strokes of different shape, each representing a different code value or having associated magnetic codes;</paragraph-text></list-item><list-item><paragraph-text type="body">recognition of whole cursive handwritten words, i.e. whose letters are not separated but are linked together, whether from offline (scanning) or from online (digital ink, e.g. pen input) acquisition;</paragraph-text></list-item><list-item><paragraph-text type="body">recognition of three-dimensional handwriting, e.g. writing in the air.</paragraph-text></list-item></list><paragraph-text type="body">Illustrative examples of subject matter classified in this place:</paragraph-text><paragraph-text type="body">1.</paragraph-text><paragraph-text type="body"><media id="media130.png" file-name="cpc-def-G06V-0130.png" type="png" preferred-width="6.54cm" preferred-height="6.31cm"/></paragraph-text><paragraph-text type="body">Colour codes embedded in characters to assist their recognition.</paragraph-text><paragraph-text type="body">2.</paragraph-text><paragraph-text type="body"><media id="media131.png" file-name="cpc-def-G06V-0131.png" type="png" preferred-width="7.49cm" preferred-height="4.49cm"/></paragraph-text><paragraph-text type="body">Letters and numbers composed of a combination of sixteen segments each.</paragraph-text><paragraph-text type="body">3.</paragraph-text><paragraph-text type="body"><media id="media132.png" file-name="cpc-def-G06V-0132.png" type="png" preferred-width="6.9cm" preferred-height="2.12cm"/></paragraph-text><paragraph-text type="body">Characters composed of vertical bars, the shape of the bars assisting the optical character recognition.</paragraph-text><paragraph-text type="body">4A.</paragraph-text><paragraph-text type="body"><media id="media133.png" file-name="cpc-def-G06V-0133.png" type="png" preferred-width="7.79cm" preferred-height="7.94cm"/></paragraph-text><paragraph-text type="body">4B.</paragraph-text><paragraph-text type="body"><media id="media134.png" file-name="cpc-def-G06V-0134.png" type="png" preferred-width="6.67cm" preferred-height="6.18cm"/></paragraph-text><paragraph-text type="body">The characters (22F, 22G and 22H) have a different stroke width/length, which results in a characteristic waveform when scanned from right to left by a magnetic reader (22), an application is frequently used for bank cheques.</paragraph-text><paragraph-text type="body">5.</paragraph-text><paragraph-text type="body"><media id="media135.jpeg" file-name="cpc-def-G06V-0135.jpeg" type="jpeg" preferred-width="14.12cm" preferred-height="11.92cm"/></paragraph-text><paragraph-text type="body">Recognition of cursive words by fitting the characters on a deformable grid.</paragraph-text></section-body></definition-statement><references><section-title>References</section-title><informative-references><section-title>Informative references</section-title><section-body><paragraph-text type="preamble">Attention is drawn to the following places, which may be of interest for search:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Image acquisition for character recognition using a slot moved over the image, discrete sensing elements at predetermined points or automatic curve following means</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V30/144</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Methods and arrangements for sensing record carriers</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06K7/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Testing specially adapted to determine the identity or genuineness of valuable papers, e.g. banknotes</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G07D7/00</class-ref></paragraph-text></table-column></table-row></table></section-body></informative-references></references><glossary-of-terms><section-title>Glossary of terms</section-title><section-body><paragraph-text type="preamble">In this place, the following terms or expressions are used with the meaning indicated:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">CMC-7</paragraph-text></table-column><table-column><paragraph-text type="body">special font used for printing characters for magnetic and optical character recognition systems</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">magnetic ink</paragraph-text></table-column><table-column><paragraph-text type="body">ink containing particles of magnetic material used for printing characters to facilitate magnetic character recognition</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">MICR</paragraph-text></table-column><table-column><paragraph-text type="body">magnetic ink character recognition</paragraph-text></table-column></table-row></table></section-body></glossary-of-terms></definition-item>
<definition-item date-revised="2024-08-01"><classification-symbol scheme="cpc">G06V30/24</classification-symbol><definition-title>characterised by the processing or recognition method  (segmentation of character regions <class-ref scheme="cpc">G06V30/148</class-ref>)</definition-title><definition-statement><section-title>Definition statement</section-title><section-body><paragraph-text type="preamble">This place covers:</paragraph-text><paragraph-text type="body">Character recognition characterised by the processing or recognition method, including:</paragraph-text><list><list-item><paragraph-text type="body">Division of character sequences into groups prior to recognition; selection of dictionaries;</paragraph-text></list-item><list-item><paragraph-text type="body">Using graphical properties, e.g. alphabet type, font or type of print when performing recognition;</paragraph-text></list-item><list-item><paragraph-text type="body">Alphabet recognition;</paragraph-text></list-item><list-item><paragraph-text type="body">Font recognition;</paragraph-text></list-item><list-item><paragraph-text type="body">Discrimination between machine-print, hand-print or cursive writing;</paragraph-text></list-item><list-item><paragraph-text type="body">Analysis of linguistic properties, e.g. English or German.</paragraph-text></list-item></list><paragraph-text type="body">Illustrative examples of subject matter classified in this place:</paragraph-text><paragraph-text type="body">1A.</paragraph-text><paragraph-text type="body"><media id="media136.png" file-name="cpc-def-G06V-0136.png" type="png" preferred-width="9.57cm" preferred-height="9.8cm"/></paragraph-text><paragraph-text type="body">1B.</paragraph-text><paragraph-text type="body"><media id="media137.png" file-name="cpc-def-G06V-0137.png" type="png" preferred-width="10.27cm" preferred-height="9.86cm"/></paragraph-text><paragraph-text type="body">Example of correction symbols forming an alphabet (fig. 1A), each symbol having a predefined meaning which allows the text to be automatically processed (fig. 1B).</paragraph-text></section-body></definition-statement><references><section-title>References</section-title><limiting-references><section-title>Limiting references</section-title><section-body><paragraph-text type="preamble">This place does not cover:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Segmentation of character regions, for character recognition</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V30/148</class-ref></paragraph-text></table-column></table-row></table></section-body></limiting-references><informative-references><section-title>Informative references</section-title><section-body><paragraph-text type="preamble">Attention is drawn to the following places, which may be of interest for search:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Methods and arrangements for sensing record carriers</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06K7/00</class-ref></paragraph-text></table-column></table-row></table></section-body></informative-references></references></definition-item>
<definition-item date-revised="2024-08-01"><classification-symbol scheme="cpc">G06V30/26</classification-symbol><definition-title>Techniques for post-processing, e.g. correcting the recognition result</definition-title><definition-statement><section-title>Definition statement</section-title><section-body><paragraph-text type="preamble">This place covers:</paragraph-text><paragraph-text type="body">Techniques for post-processing, e.g. by correcting the recognition result. This usually involves the context analysis of a certain character or word by taking into account neighbouring characters or words (bi-grams, tri-grams, etc.). The specific context can be analysed:</paragraph-text><list><list-item><paragraph-text type="body">lexically, e.g. with a help of a dictionary to correct for mis-recognised characters in a word;</paragraph-text></list-item><list-item><paragraph-text type="body">syntactically, e.g. by considering the syntax rules of a phrase containing the words recognised;</paragraph-text></list-item><list-item><paragraph-text type="body">semantically, e.g. by analysing the intrinsic meaning of the word when considered in the recognised context.</paragraph-text></list-item></list><paragraph-text type="body">Illustrative example of subject matter classified in this place:</paragraph-text><paragraph-text type="body"><media id="media138.png" file-name="cpc-def-G06V-0138.png" type="png" preferred-width="10.39cm" preferred-height="18.73cm"/></paragraph-text><paragraph-text type="body">Example of OCR correction using a directory lookup; in case of non-valid matches, an approximate matching is output as closest match using a confusion matrix.</paragraph-text></section-body></definition-statement><relationship><section-title>Relationships with other classification places</section-title><section-body><paragraph-text type="body">Methods or arrangements for detection or correction of errors provided by group <class-ref scheme="cpc">G06V30/12</class-ref> involves such correction after the acquisition step with the general aim to have a reliable input before the subsequent recognition. In contrast, the present group covers the methods and arrangements used after recognition that aim at correcting the final output by using additional information, such as context.</paragraph-text></section-body></relationship><references><section-title>References</section-title><informative-references><section-title>Informative references</section-title><section-body><paragraph-text type="preamble">Attention is drawn to the following places, which may be of interest for search:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Image or video pattern matching, using syntactic or structural representations</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/86</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Segmentation of character regions, for character recognition</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V30/148</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Information retrieval of unstructured textual data</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06F16/30</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Information retrieval of still image data</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06F16/50</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Handling natural language data</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06F40/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Image analysis using region-based segmentation</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06T7/11</class-ref></paragraph-text></table-column></table-row></table></section-body></informative-references></references><glossary-of-terms><section-title>Glossary of terms</section-title><section-body><paragraph-text type="preamble">In this place, the following terms or expressions are used with the meaning indicated:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">n-gram</paragraph-text></table-column><table-column><paragraph-text type="body">a contiguous sequence of n items from a given sample of text. The items can be syllables, letters, words or base pairs according to the application. Typical examples are bi-grams and tri-grams.</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">trie</paragraph-text></table-column><table-column><paragraph-text type="body">also called &quot;digital tree&quot; or &quot;prefix tree&quot;, is a type of tree data structure used for locating specific keys (items) from within a set of characters or words. In order to access a key (to recover its value, change it, or remove it), the trie is traversed (usually in a depth-first fashion), following the links between nodes.</paragraph-text></table-column></table-row></table></section-body></glossary-of-terms></definition-item>
<definition-item date-revised="2024-08-01"><classification-symbol scheme="cpc">G06V30/28</classification-symbol><definition-title>specially adapted to the type of the alphabet, e.g. Latin alphabet</definition-title><definition-statement><section-title>Definition statement</section-title><section-body><paragraph-text type="preamble">This place covers:</paragraph-text><paragraph-text type="body">OCR techniques specifically adapted to the type of the alphabet, e.g. Latin or Asian alphabets; alphabet recognition.</paragraph-text><paragraph-text type="body">Illustrative examples of subject matter classified in this place:</paragraph-text><paragraph-text type="body">1A.</paragraph-text><paragraph-text type="body"><media id="media139.png" file-name="cpc-def-G06V-0139.png" type="png" preferred-width="13.06cm" preferred-height="6.16cm"/></paragraph-text><paragraph-text type="body">1B.</paragraph-text><paragraph-text type="body"><media id="media140.png" file-name="cpc-def-G06V-0140.png" type="png" preferred-width="7.24cm" preferred-height="8.53cm"/></paragraph-text><paragraph-text type="body">The scanning direction for OCR (horizonal or vertical) is adapted according to the detected alphabet.</paragraph-text></section-body></definition-statement><references><section-title>References</section-title><informative-references><section-title>Informative references</section-title><section-body><paragraph-text type="preamble">Attention is drawn to the following places, which may be of interest for search:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Segmentation of character regions, for character recognition</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V30/148</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Handling natural language data</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06F40/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Scanning, transmission or reproduction of documents or the like</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">H04N1/00</class-ref></paragraph-text></table-column></table-row></table></section-body></informative-references></references></definition-item>
<definition-item date-revised="2024-08-01"><classification-symbol scheme="cpc">G06V30/30</classification-symbol><definition-title>based on the type of data</definition-title><definition-statement><section-title>Definition statement</section-title><section-body><paragraph-text type="preamble">This place covers:</paragraph-text><paragraph-text type="body">Recognition according to the type of data (specific images). Examples are:</paragraph-text><list><list-item><paragraph-text type="body">images containing characters for discriminating human versus automated computer access (&quot;Completely Automated Public Turing test to tell Computers and Humans Apart&quot; - CAPTCHA);</paragraph-text></list-item><list-item><paragraph-text type="body">musical notations.</paragraph-text></list-item></list><paragraph-text type="body">Illustrative examples of subject matter classified in this place:</paragraph-text><paragraph-text type="body">1.</paragraph-text><paragraph-text type="body"><media id="media141.png" file-name="cpc-def-G06V-0141.png" type="png" preferred-width="13.06cm" preferred-height="10.6cm"/></paragraph-text><paragraph-text type="body">Examples of CAPTCHA images.</paragraph-text><paragraph-text type="body">2A.</paragraph-text><paragraph-text type="body"><media id="media142.png" file-name="cpc-def-G06V-0142.png" type="png" preferred-width="7.62cm" preferred-height="4.45cm"/></paragraph-text><paragraph-text type="body">2B.</paragraph-text><paragraph-text type="body"><media id="media143.png" file-name="cpc-def-G06V-0143.png" type="png" preferred-width="8.91cm" preferred-height="4.97cm"/></paragraph-text><paragraph-text type="body">2C.</paragraph-text><paragraph-text type="body"> <media id="media144.png" file-name="cpc-def-G06V-0144.png" type="png" preferred-width="8.51cm" preferred-height="3.92cm"/></paragraph-text><paragraph-text type="body">Recognition of music notations by stroke extraction and segmentation of each note.</paragraph-text></section-body></definition-statement><references><section-title>References</section-title><informative-references><section-title>Informative references</section-title><section-body><paragraph-text type="preamble">Attention is drawn to the following places, which may be of interest for search:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">User authentication by graphic or iconic representation, in security arrangements for protecting computers, components thereof, programs or data against unauthorised activity</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06F21/36</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Teaching music</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G09B15/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Means for the representation of music</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G10G1/00</class-ref></paragraph-text></table-column></table-row></table></section-body></informative-references></references></definition-item>
<definition-item date-revised="2024-08-01"><classification-symbol scheme="cpc">G06V30/32</classification-symbol><definition-title>Digital ink</definition-title><definition-statement><section-title>Definition statement</section-title><section-body><paragraph-text type="preamble">This place covers:</paragraph-text><paragraph-text type="body">Preprocessing, feature extraction, matching, recognition and classification of all kinds of handwritten characters, symbols, drawings, except signatures, on the basis of trajectories as a function of time of a stylus, finger etc. Trajectories can be acquired by a touch pad/screen or by a stylus-like device (in collaboration with a passive or active surface).</paragraph-text><paragraph-text type="body">Segmentation of strokes, characters or words.</paragraph-text><paragraph-text type="body">Text and character recognition using temporal information, e.g. free-form handwriting, Asian scripts.</paragraph-text><paragraph-text type="body">Recognition of drawings using temporal information, e.g. sketches, flow charts, graphical or mathematical symbols or formulae, chemical structure formulae, editorial notes, proof marks.</paragraph-text><paragraph-text type="body">Illustrative examples of subject matter classified in this place:</paragraph-text><paragraph-text type="body">1A.</paragraph-text><paragraph-text type="body"><media id="media145.png" file-name="cpc-def-G06V-0145.png" type="png" preferred-width="9.38cm" preferred-height="14.44cm"/></paragraph-text><paragraph-text type="body">1B.</paragraph-text><paragraph-text type="body"><media id="media146.png" file-name="cpc-def-G06V-0146.png" type="png" preferred-width="8.51cm" preferred-height="14.05cm"/></paragraph-text><paragraph-text type="body">Example of recognising a handwritten flow-chart.</paragraph-text><paragraph-text type="body">2.</paragraph-text><paragraph-text type="body"><media id="media147.png" file-name="cpc-def-G06V-0147.png" type="png" preferred-width="12.38cm" preferred-height="4.91cm"/></paragraph-text><paragraph-text type="body">Recognising handwritten mathematical symbols.</paragraph-text><paragraph-text type="body">3.</paragraph-text><paragraph-text type="body"><media id="media148.gif" file-name="cpc-def-G06V-0148.gif" type="gif" preferred-width="13.59cm" preferred-height="10.18cm"/></paragraph-text><paragraph-text type="body">Uni-strokes for computerised interpretation of handwriting.</paragraph-text></section-body></definition-statement><relationship><section-title>Relationships with other classification places</section-title><section-body><paragraph-text type="body">The recognition of signatures is considered as a biometric trait and it is covered by group <class-ref scheme="cpc">G06V40/30</class-ref>. If functional details concerning the temporal analysis of the digital ink used for signature recognition are present, double classification with the present group is recommended. The present group assumes that the digital ink is inherently provided with temporal information which is relevant during processing. If the temporal information is not relevant, the character recognition groups provided under group <class-ref scheme="cpc">G06V30/10</class-ref> apply.</paragraph-text></section-body></relationship><references><section-title>References</section-title><informative-references><section-title>Informative references</section-title><section-body><paragraph-text type="preamble">Attention is drawn to the following places, which may be of interest for search:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Arrangements for converting the position or the displacement of a member into a coded form, for input arrangements or input/output arrangements for user-computer interaction</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06F3/03</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Inputting data by handwriting, e.g. gestures or text, to a computer via a graphical user interface [GUI] using a touch-screen or digitiser</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06F3/0488</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Digital computing or data processing equipment or methods, specially adapted for specific functions</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06F17/00</class-ref></paragraph-text></table-column></table-row></table></section-body></informative-references></references><glossary-of-terms><section-title>Glossary of terms</section-title><section-body><paragraph-text type="preamble">In this place, the following terms or expressions are used with the meaning indicated:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">digital ink, electronic ink, e-ink</paragraph-text></table-column><table-column><paragraph-text type="body">technology that digitally represents handwriting, e.g. using a finger or a stylus, in its natural form using temporal information. Digital ink may also be referred to as electronic ink or e-ink.</paragraph-text></table-column></table-row></table></section-body></glossary-of-terms></definition-item>
<definition-item date-revised="2024-08-01"><classification-symbol scheme="cpc">G06V30/40</classification-symbol><definition-title>Document-oriented image-based pattern recognition</definition-title><definition-statement><section-title>Definition statement</section-title><section-body><paragraph-text type="preamble">This place covers:</paragraph-text><paragraph-text type="body">Document analysis, understanding and recognition of document images, involving the analysis of the document content, such as analysis of the geometrical or logical structure. Different types of documents can be involved, such as technical drawings, geographical maps, postal images, e.g. labels on parcels, addresses on postal envelopes.</paragraph-text><paragraph-text type="body">Illustrative examples of subject matter classified in this place:</paragraph-text><paragraph-text type="body">1A.</paragraph-text><paragraph-text type="body"><media id="media149.png" file-name="cpc-def-G06V-0149.png" type="png" preferred-width="5.97cm" preferred-height="4.59cm"/></paragraph-text><paragraph-text type="body">1B.</paragraph-text><paragraph-text type="body"><media id="media150.png" file-name="cpc-def-G06V-0150.png" type="png" preferred-width="5.55cm" preferred-height="4.53cm"/></paragraph-text><paragraph-text type="body">Identification of the text region of a document after its skew correction.</paragraph-text><paragraph-text type="body">2.</paragraph-text><paragraph-text type="body"><media id="media151.png" file-name="cpc-def-G06V-0151.png" type="png" preferred-width="12.53cm" preferred-height="7.96cm"/></paragraph-text><paragraph-text type="body">Extraction of image key points, considering them as nodes and constructing a graph representation by connecting with edges the neighbouring nodes; the graph-based representation is later used for document matching.</paragraph-text></section-body></definition-statement><references><section-title>References</section-title><application-references><section-title>Application-oriented references</section-title><section-body><paragraph-text type="preamble">Examples of places where the subject matter of this place is covered when specially adapted, used for a particular purpose, or incorporated in a larger system:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Sorting of mail or documents using means for detecting the destination</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">B07C3/10</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Input arrangements for interaction between user and computer</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06F3/01</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Testing to determine the identity or genuineness of valuable papers, e.g. banknotes</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G07D7/00</class-ref></paragraph-text></table-column></table-row></table></section-body></application-references><informative-references><section-title>Informative references</section-title><section-body><paragraph-text type="preamble">Attention is drawn to the following places, which may be of interest for search:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Recognition of printed characters based on code marks</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V30/224</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06F3/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Information retrieval of unstructured textual data</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06F16/30</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Information retrieval of still image data</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06F16/50</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Handling natural language data</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06F40/00</class-ref></paragraph-text></table-column></table-row></table></section-body></informative-references></references></definition-item>
<definition-item date-revised="2024-08-01"><classification-symbol scheme="cpc">G06V30/41</classification-symbol><definition-title>Analysis of document content  (recognition of printed characters based on code marks <class-ref scheme="cpc">G06V30/224</class-ref>)</definition-title><definition-statement><section-title>Definition statement</section-title><section-body><paragraph-text type="preamble">This place covers:</paragraph-text><paragraph-text type="body">Document analysis, recognition and understanding by processing:</paragraph-text><list><list-item><paragraph-text type="body">Structured documents such as business forms, bank checks whose layout is provided with printed lines or input boxes, bounding boxes, checkboxes, straight lines or tables;</paragraph-text></list-item><list-item><paragraph-text type="body">Classification of document image content by identification of text regions, photographs, tables;</paragraph-text></list-item><list-item><paragraph-text type="body">Extracting and analysing the geometrical structure, e.g. the layout tree representation in which different entities such as paragraphs, images, etc. are represented as nodes of a tree or a graph;</paragraph-text></list-item><list-item><paragraph-text type="body">Extracting and analysing the logical structure, e.g. identification of the chapter headings, sections, columns, titles, paragraphs, captions, page numbers, or identification of the constituting elements such as authors, keywords, postal codes, money amounts;</paragraph-text></list-item><list-item><paragraph-text type="body">Document matching, e.g. by establishing the degree of (dis)similarity between two document images, one reference/template document image and one input document image.</paragraph-text></list-item></list><paragraph-text type="body">Illustrative examples of subject matter classified in this place:</paragraph-text><paragraph-text type="body">1.</paragraph-text><paragraph-text type="body"><media id="media152.png" file-name="cpc-def-G06V-0152.png" type="png" preferred-width="10.12cm" preferred-height="6.12cm"/></paragraph-text><paragraph-text type="body">Identification of the text and image regions; the image tiles are marked with an &quot;I&quot; and text tiles are marked with a &quot;T&quot;.</paragraph-text><paragraph-text type="body">2.</paragraph-text><paragraph-text type="body"><media id="media153.png" file-name="cpc-def-G06V-0153.png" type="png" preferred-width="9.53cm" preferred-height="5.46cm"/></paragraph-text><paragraph-text type="body">Extraction of document structure by analysis of its content, resulting in the identification of elements such as paragraphs, drawings, handwritten annotations.</paragraph-text></section-body></definition-statement><references><section-title>References</section-title><limiting-references><section-title>Limiting references</section-title><section-body><paragraph-text type="preamble">This place does not cover:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Recognition of printed characters based on code marks</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V30/224</class-ref></paragraph-text></table-column></table-row></table></section-body></limiting-references><informative-references><section-title>Informative references</section-title><section-body><paragraph-text type="preamble">Attention is drawn to the following places, which may be of interest for search:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Image preprocessing for image or video recognition or understanding, by selection of a specific region containing or referencing a pattern; Image preprocessing for image or video recognition by locating or processing of specific regions to guide the detection or recognition, e.g. highlights, fiducial marks or predetermined fields</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/22</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Local feature extraction for image or video recognition or understanding by analysis of parts of the pattern, e.g. by detecting edges, contours, loops, corners, strokes or intersections; Extraction of image or video features for image or video recognition or understanding using connectivity analysis, e.g. of connected components, edge linking or neighbouring slice analysis</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/44</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Analysis of text in scene images, e.g. of license plates, overlay texts or captions on TV images</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V20/62</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Aligning or centring of the image pick-up or the image field, for character recognition</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V30/146</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Segmentation of character regions, for character recognition</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V30/148</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Information retrieval of unstructured textual data</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06F16/30</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Information retrieval of still image data</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06F16/50</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Handling natural language data</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06F40/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Image analysis using region-based segmentation</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06T7/11</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Details of scanning heads for optical reproduction of scanned documents or the like</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">H04N1/036</class-ref></paragraph-text></table-column></table-row></table></section-body></informative-references></references></definition-item>
<definition-item date-revised="2024-08-01"><classification-symbol scheme="cpc">G06V30/42</classification-symbol><definition-title>based on the type of document</definition-title><definition-statement><section-title>Definition statement</section-title><section-body><paragraph-text type="preamble">This place covers:</paragraph-text><paragraph-text type="body">Document analysis, understanding and recognition, based on the type of document. </paragraph-text><paragraph-text type="body">Examples include:</paragraph-text><list><list-item><paragraph-text type="body">technical drawings and geographical maps;</paragraph-text></list-item><list-item><paragraph-text type="body">postal images, e.g. labels or addresses on parcels or postal envelopes.</paragraph-text></list-item></list><paragraph-text type="body">Illustrative examples of subject matter classified in this place:</paragraph-text><paragraph-text type="body">1.</paragraph-text><paragraph-text type="body"><media id="media154.png" file-name="cpc-def-G06V-0154.png" type="png" preferred-width="16cm" preferred-height="11.41cm"/></paragraph-text><paragraph-text type="body">Credit card detection (bottom left), perspective mapping (top right), extraction of the relevant fields and recognition of the information.</paragraph-text><paragraph-text type="body">2A.</paragraph-text><paragraph-text type="body"><media id="media155.png" file-name="cpc-def-G06V-0155.png" type="png" preferred-width="7.79cm" preferred-height="5.33cm"/></paragraph-text><paragraph-text type="body">2B.</paragraph-text><paragraph-text type="body"><media id="media156.png" file-name="cpc-def-G06V-0156.png" type="png" preferred-width="4.7cm" preferred-height="5cm"/></paragraph-text><paragraph-text type="body">Acquisition and recognition of elements in a schematic drawing (fig. 2A) and mapping the recognition results into a database (fig. 2B).</paragraph-text></section-body></definition-statement><references><section-title>References</section-title><informative-references><section-title>Informative references</section-title><section-body><paragraph-text type="preamble">Attention is drawn to the following places, which may be of interest for search:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Image preprocessing for image or video recognition or understanding involving the determination of region of interest [ROI] or a volume of interest [VOI]</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/25</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Aligning or centring of the image pick-up or image-field, for character recognition</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V30/146</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Map- or contour-matching specially adapted for navigation in a road network using correlation of data from several navigational instruments</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G01C21/30</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Information retrieval of unstructured textual data</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06F16/30</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Information retrieval of still image data</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06F16/50</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Handling natural language data</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06F40/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Image analysis using region-based segmentation</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06T7/11</class-ref></paragraph-text></table-column></table-row></table></section-body></informative-references></references></definition-item>
<definition-item date-revised="2024-08-01"><classification-symbol scheme="cpc">G06V40/00</classification-symbol><definition-title>Recognition of biometric, human-related or animal-related patterns in image or video data</definition-title><definition-statement><section-title>Definition statement</section-title><section-body><paragraph-text type="preamble">This place covers:</paragraph-text><paragraph-text type="body">Detection, feature extraction, classification, identification, authentication of human-related or animal-related patterns in images or video. It includes monitoring behaviour, habits or activities, such as eating and sleeping patterns, sport activities, gait recognition, hand gestures (both static and dynamic), including those performed on a touch screen or freely in the air.</paragraph-text><paragraph-text type="body">Biometric identification and authentication using body parts, e.g. fingerprints, palmprints, footprints, using faces or eye characteristics such as vessel patterns of the eye sclera or eye fundus, or iris patterns. Other examples of biometric traits include measurements obtained from the hand geometry or the limbs, or personal signatures.</paragraph-text><paragraph-text type="body">Writer recognition, i.e. establishing the identity of the person who wrote a piece of text.</paragraph-text></section-body></definition-statement><references><section-title>References</section-title><application-references><section-title>Application-oriented references</section-title><section-body><paragraph-text type="preamble">Examples of places where the subject matter of this place is covered when specially adapted, used for a particular purpose, or incorporated in a larger system:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Means to switch the anti-theft system of vehicles on or off, using biometry</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">B60R25/25</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">User authentication using biometric data for protecting computers, components thereof, programs or data against unauthorised activity</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06F21/32</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Identity check of a pass holder for individual registration on entry or exit using biometric data</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G07C9/25</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Identity check for individual registration on entry or exit without a pass using biometric data</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G07C9/37</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Acquiring the identification of end-users of distributed media content using their biometric characteristics</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">H04N21/4415</class-ref></paragraph-text></table-column></table-row></table></section-body></application-references><informative-references><section-title>Informative references</section-title><section-body><paragraph-text type="preamble">Attention is drawn to the following places, which may be of interest for search:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Image preprocessing for image or video recognition or understanding</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/20</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Extraction of image or video features, e.g. computing feature vectors, image or video descriptors</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/40</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Recognition of moving objects or obstacles, e.g. vehicles or pedestrians; Recognition of traffic objects, e.g. traffic signs, traffic lights or roads</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V20/58</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Recognition or understanding of scenes inside of a vehicle</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V20/59</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Recognising three-dimensional [3D] objects in scenes</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V20/64</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Recognition of digital ink within image or video data</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V30/32</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Arrangements or fittings on vehicles for protecting or preventing injuries to occupants or pedestrians in case of accidents or other traffic risks, including means for detecting the presence or position of passengers, passenger seats or child seats</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">B60R21/015</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Digitisers as the input arrangement for user-computer interaction, e.g. touch screens or touch pads</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06F3/041</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Analysis of motion in images</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06T7/20</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Image analysis for determining position or orientation of objects</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06T7/70</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Checking-devices for individual entry or exit registers</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G07C9/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Burglar, theft or intruder alarms</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G08B13/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Arrangements for secret or secure communications; Network security protocols</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">H04L9/00</class-ref></paragraph-text></table-column></table-row></table></section-body></informative-references></references><glossary-of-terms><section-title>Glossary of terms</section-title><section-body><paragraph-text type="preamble">In this place, the following terms or expressions are used with the meaning indicated:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">gesture</paragraph-text></table-column><table-column><paragraph-text type="body">posture or hand movement denoting a certain meaning, e.g. deaf sign language.</paragraph-text></table-column></table-row></table></section-body></glossary-of-terms></definition-item>
<definition-item date-revised="2024-08-01"><classification-symbol scheme="cpc">G06V40/10</classification-symbol><definition-title>Human or animal bodies, e.g. vehicle occupants or pedestrians; Body parts, e.g. hands</definition-title><definition-statement><section-title>Definition statement</section-title><section-body><paragraph-text type="preamble">This place covers:</paragraph-text><paragraph-text type="body">Detection, feature extraction, classification, identification and recognition of:</paragraph-text><list><list-item><paragraph-text type="body">human bodies;</paragraph-text></list-item><list-item><paragraph-text type="body">human body parts e.g. arms, hands, legs;</paragraph-text></list-item><list-item><paragraph-text type="body">vehicle occupants or pedestrians as perceived by a camera inside or outside a vehicle;</paragraph-text></list-item><list-item><paragraph-text type="body">animal bodies;</paragraph-text></list-item><list-item><paragraph-text type="body">biometric identification based on hand measurements, e.g. distances between joints, length of the fingers, etc.;</paragraph-text></list-item><list-item><paragraph-text type="body">static gestures, e.g. pose recognition.</paragraph-text></list-item></list><paragraph-text type="body">Illustrative examples of subject matter classified in this place:</paragraph-text><paragraph-text type="body">1A.</paragraph-text><paragraph-text type="body"><media id="media157.png" file-name="cpc-def-G06V-0157.png" type="png" preferred-width="5.12cm" preferred-height="3.64cm"/></paragraph-text><paragraph-text type="body">1B.</paragraph-text><paragraph-text type="body"><media id="media158.png" file-name="cpc-def-G06V-0158.png" type="png" preferred-width="5.19cm" preferred-height="3.3cm"/></paragraph-text><paragraph-text type="body">Successive stages in the process of pose estimation.</paragraph-text><paragraph-text type="body">2.</paragraph-text><paragraph-text type="body"><media id="media159.png" file-name="cpc-def-G06V-0159.png" type="png" preferred-width="7.66cm" preferred-height="7.41cm"/></paragraph-text><paragraph-text type="body">Pose recognition of the arm by quantifying its direction as a vector.</paragraph-text><paragraph-text type="body">3.</paragraph-text><paragraph-text type="body"><media id="media160.png" file-name="cpc-def-G06V-0160.png" type="png" preferred-width="10.29cm" preferred-height="10.63cm"/></paragraph-text><paragraph-text type="body">Different hand configurations used for a secret sign.</paragraph-text></section-body></definition-statement><relationship><section-title>Relationships with other classification places</section-title><section-body><paragraph-text type="body">Recognition of body movements, e.g. gesture recognition in a temporal image sequence, or monitoring sport training in video is classified in group <class-ref scheme="cpc">G06V40/20</class-ref>.</paragraph-text></section-body></relationship><references><section-title>References</section-title><informative-references><section-title>Informative references</section-title><section-body><paragraph-text type="preamble">Attention is drawn to the following places, which may be of interest for search:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Extraction of image or video features by performing operations within image blocks or by using histograms, e.g. histogram oriented gradients [HoG]</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/50</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Recognition of moving objects or obstacles in scenes, e.g. vehicles or pedestrians; Recognition of traffic objects, e.g. traffic signs, traffic lights or roads</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V20/58</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Recognition or understanding of scenes inside of a vehicle</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V20/59</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Recognising three-dimensional [3D] objects in scenes</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V20/64</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Arrangements or fittings on vehicles for protecting or preventing injuries to occupants or pedestrians in case of accidents or other traffic risks, including means for detecting the presence or position of passengers, passenger seats or child seats</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">B60R21/015</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Image analysis for determining position or orientation of objects</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06T7/70</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Burglar, theft or intruder alarms</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G08B13/00</class-ref></paragraph-text></table-column></table-row></table></section-body></informative-references></references><glossary-of-terms><section-title>Glossary of terms</section-title><section-body><paragraph-text type="preamble">In this place, the following terms or expressions are used with the meaning indicated:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">dynamic gesture</paragraph-text></table-column><table-column><paragraph-text type="body">movement of the hand encoding a certain meaning, e.g. deaf sign language</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">static gesture</paragraph-text></table-column><table-column><paragraph-text type="body">posture of a hand denoting a certain meaning</paragraph-text></table-column></table-row></table></section-body></glossary-of-terms></definition-item>
<definition-item date-revised="2024-08-01"><classification-symbol scheme="cpc">G06V40/12</classification-symbol><definition-title>Fingerprints or palmprints</definition-title><definition-statement><section-title>Definition statement</section-title><section-body><paragraph-text type="preamble">This place covers:</paragraph-text><paragraph-text type="body">Acquisition, pre-processing, feature extraction and</paragraph-text><list><list-item><paragraph-text type="body">matching for biometric identification purposes; or</paragraph-text></list-item><list-item><paragraph-text type="body">classification into types; or</paragraph-text></list-item><list-item><paragraph-text type="body">detecting the live character of the finger, i.e. distinguishing from a fake or cadaver finger by using either specialised acquisition arrangements or by image processing.</paragraph-text></list-item><list-item><paragraph-text type="body">Pre-processing and feature extraction for the purpose of fingerprint recognition, e.g.:</paragraph-text></list-item><list-item><paragraph-text type="body">denoising/filtering, enhancement, normalisation;</paragraph-text></list-item><list-item><paragraph-text type="body">minutiae extraction;</paragraph-text></list-item><list-item><paragraph-text type="body">ridge properties extraction, such as ridge spatial frequency and ridge orientation.</paragraph-text></list-item></list><paragraph-text type="body">Illustrative examples of subject matter classified in this place:</paragraph-text><paragraph-text type="body">1.</paragraph-text><paragraph-text type="body"><media id="media161.png" file-name="cpc-def-G06V-0161.png" type="png" preferred-width="7.49cm" preferred-height="2.86cm"/></paragraph-text><paragraph-text type="body">Fingerprint representations by ridges (thin and thick lines) and minutiae (ridge endings (1) and (2) and bifurcations (3)).</paragraph-text><paragraph-text type="body">2.</paragraph-text><paragraph-text type="body"><media id="media162.png" file-name="cpc-def-G06V-0162.png" type="png" preferred-width="10.73cm" preferred-height="8.72cm"/></paragraph-text><paragraph-text type="body">The sets of minutiae extracted from two fingerprint images are matched to establish the person&apos;s identity; the small circles denote matched pairs.</paragraph-text></section-body></definition-statement><references><section-title>References</section-title><informative-references><section-title>Informative references</section-title><section-body><paragraph-text type="preamble">Attention is drawn to the following places, which may be of interest for search:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Image preprocessing for image or video recognition or understanding</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/20</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Noise filtering for image or video recognition or understanding</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/30</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Writer recognition; Reading and verifying signatures</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V40/30</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Spoof detection in image or video recognition</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V40/40</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Multimodal biometrics, e.g. combining information from different biometric modalities</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V40/70</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Identification of persons</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">A61B5/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Fittings or systems for preventing or indicating unauthorised use or theft of vehicles</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">B60R25/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Digitisers as the input arrangement for user-computer interaction, e.g. touch screens or touch pads</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06F3/041</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Security arrangements for protecting computers, components thereof, programs or data against unauthorised activity</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06F21/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Checking-devices for individual entry or exit registers</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G07C9/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Arrangements for secret or secure communications; Network security protocols</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">H04L9/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Means for preventing unauthorised calls from a telephone set</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">H04M1/667</class-ref></paragraph-text></table-column></table-row></table></section-body></informative-references></references><special-rules><section-title>Special rules of classification</section-title><section-body><paragraph-text type="body">Detection of the static pose of the hand or biometrics obtained from hand geometrical arrangement of the fingers, e.g. distance between the finger joints is classified in group <class-ref scheme="cpc">G06V40/10</class-ref>.</paragraph-text><paragraph-text type="body">Techniques involving multiple biometrics are classified in group <class-ref scheme="cpc">G06V40/70</class-ref>.</paragraph-text></section-body></special-rules><glossary-of-terms><section-title>Glossary of terms</section-title><section-body><paragraph-text type="preamble">In this place, the following terms or expressions are used with the meaning indicated:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">fingerprints or palmprints</paragraph-text></table-column><table-column><paragraph-text type="body">2D or 3D images of the (sub-)surface (sub-)epidermal structures of fingers or palm</paragraph-text></table-column></table-row></table></section-body></glossary-of-terms></definition-item>
<definition-item date-revised="2024-08-01"><classification-symbol scheme="cpc">G06V40/13</classification-symbol><definition-title>Sensors therefor</definition-title><definition-statement><section-title>Definition statement</section-title><section-body><paragraph-text type="preamble">This place covers:</paragraph-text><paragraph-text type="body">Fingerprint or palmprint sensors of all kinds:</paragraph-text><list><list-item><paragraph-text type="body">optical sensing, e.g. through reflection in optical elements such as prisms;</paragraph-text></list-item><list-item><paragraph-text type="body">non-contact direct (distance) sensing;</paragraph-text></list-item><list-item><paragraph-text type="body">capacitive/RF (active impedance) sensing;</paragraph-text></list-item><list-item><paragraph-text type="body">ultrasonic sensing;</paragraph-text></list-item><list-item><paragraph-text type="body">thermal sensing;</paragraph-text></list-item><list-item><paragraph-text type="body">pressure sensing;</paragraph-text></list-item><list-item><paragraph-text type="body">piezoelectric sensing;</paragraph-text></list-item><list-item><paragraph-text type="body">sweep sensing etc.</paragraph-text></list-item></list><paragraph-text type="body">Protecting the fingerprint sensors against wear and tear.</paragraph-text><paragraph-text type="body">Illustrative examples of subject matter classified in this place:</paragraph-text><paragraph-text type="body">1A.</paragraph-text><paragraph-text type="body"><media id="media163.png" file-name="cpc-def-G06V-0163.png" type="png" preferred-width="10.31cm" preferred-height="7.75cm"/></paragraph-text><paragraph-text type="body">1B.</paragraph-text><paragraph-text type="body"><media id="media164.png" file-name="cpc-def-G06V-0164.png" type="png" preferred-width="16cm" preferred-height="7.58cm"/></paragraph-text><paragraph-text type="body">Optical fingerprint sensing and capacitive sensing.</paragraph-text><paragraph-text type="body">2.</paragraph-text><paragraph-text type="body">Sweep-type sensing</paragraph-text><paragraph-text type="body"><media id="media165.png" file-name="cpc-def-G06V-0165.png" type="png" preferred-width="16cm" preferred-height="6.06cm"/></paragraph-text><paragraph-text type="body">3.</paragraph-text><paragraph-text type="body"><media id="media166.png" file-name="cpc-def-G06V-0166.png" type="png" preferred-width="9.74cm" preferred-height="3.94cm"/></paragraph-text></section-body></definition-statement><references><section-title>References</section-title><informative-references><section-title>Informative references</section-title><section-body><paragraph-text type="preamble">Attention is drawn to the following places, which may be of interest for search:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Sensors for the recognition of vascular patterns</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V40/145</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Sensors for the recognition of eye characteristics</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V40/19</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Identification of persons</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">A61B5/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Fittings or systems for preventing or indicating unauthorised use or theft of vehicles</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">B60R25/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Sonar systems specially adapted for mapping or imaging</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G01S15/89</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Digitisers as the input arrangement for user-computer interaction, e.g. touch screens or touch pads</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06F3/041</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">User authentication using biometric data for protecting computers, components thereof, programs or data against unauthorised activity</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06F21/32</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Checking-devices for individual entry or exit registers</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G07C9/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Arrangements for secret or secure communications; Network security protocols</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">H04L9/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Means for preventing unauthorised calls from a telephone set</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">H04M1/667</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Scanning, transmission or reproduction of documents or the like</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">H04N1/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Television systems</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">H04N7/00</class-ref></paragraph-text></table-column></table-row></table></section-body></informative-references></references><special-rules><section-title>Special rules of classification</section-title><section-body><paragraph-text type="body">Techniques which combine fingerprint sensors and vein (vascular) sensors are classified in groups <class-ref scheme="cpc">G06V40/13</class-ref> and <class-ref scheme="cpc">G06V40/145</class-ref>.</paragraph-text><paragraph-text type="body">Acquisition of fingerprint images generally requires specialised hardware which is essentially different from normal cameras. For this reason, fingerprint sensors are not classified in the generic group <class-ref scheme="cpc">G06V10/10</class-ref>.</paragraph-text></section-body></special-rules><glossary-of-terms><section-title>Glossary of terms</section-title><section-body><paragraph-text type="preamble">In this place, the following terms or expressions are used with the meaning indicated:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">(RF) active sensing</paragraph-text></table-column><table-column><paragraph-text type="body">active measure of the impedance formed between the finger and an electrode plate in the sensor, typically using RF band waves</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">capacitive sensing</paragraph-text></table-column><table-column><paragraph-text type="body">static measure of the capacitance formed between the skin and an electrode plate in the sensor</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">FTIR sensing</paragraph-text></table-column><table-column><paragraph-text type="body">frustrated total internal reflection sensing &#8211; the finger is imaged at the Brewster angle (air/glass); light rays are reflected only from the valley zones of the fingerprint, the ridges (partly) absorb the light</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">sweep sensor</paragraph-text></table-column><table-column><paragraph-text type="body">sensor acquiring partial fingerprint images and stitching them together to form a full fingerprint image</paragraph-text></table-column></table-row></table></section-body></glossary-of-terms></definition-item>
<definition-item date-revised="2024-08-01"><classification-symbol scheme="cpc">G06V40/14</classification-symbol><definition-title>Vascular patterns</definition-title><definition-statement><section-title>Definition statement</section-title><section-body><paragraph-text type="preamble">This place covers:</paragraph-text><paragraph-text type="body">Vascular pattern acquisition, pre-processing, feature extraction and matching for biometric identification or classification purposes. The steps of pre-processing and feature extraction for the vascular pattern recognition may include:</paragraph-text><list><list-item><paragraph-text type="body">de-noising / filtering, enhancement or normalisation of vein / vessel images;</paragraph-text></list-item><list-item><paragraph-text type="body">detection, segmentation or thinning in vein / vessel images;</paragraph-text></list-item><list-item><paragraph-text type="body">pattern or signature matching in vein / vessel images.</paragraph-text></list-item></list><paragraph-text type="body">Illustrative examples of subject matter classified in this place:</paragraph-text><paragraph-text type="body">1.</paragraph-text><paragraph-text type="body"><media id="media167.png" file-name="cpc-def-G06V-0167.png" type="png" preferred-width="9.91cm" preferred-height="4.8cm"/></paragraph-text><paragraph-text type="body">Vascular patterns of the eye used for biometric identification.</paragraph-text><paragraph-text type="body">2.</paragraph-text><paragraph-text type="body"><media id="media168.png" file-name="cpc-def-G06V-0168.png" type="png" preferred-width="10.82cm" preferred-height="7.66cm"/></paragraph-text><paragraph-text type="body">Vascular patterns of the finger used for biometric identification.</paragraph-text><paragraph-text type="body">3.</paragraph-text><paragraph-text type="body"><media id="media169.png" file-name="cpc-def-G06V-0169.png" type="png" preferred-width="10.96cm" preferred-height="5.86cm"/></paragraph-text><paragraph-text type="body">Vascular patterns of the hand used for biometric identification.</paragraph-text></section-body></definition-statement><references><section-title>References</section-title><informative-references><section-title>Informative references</section-title><section-body><paragraph-text type="preamble">Attention is drawn to the following places, which may be of interest for search:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Image preprocessing for image or video recognition or understanding</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/20</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Recognition of fingerprints or palmprints within images or video</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V40/12</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Recognition of faces within images or video</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V40/16</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Recognition of eye characteristics within images or video, e.g. of the iris</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V40/18</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Multimodal biometrics, e.g. combining information from different biometric modalities</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V40/70</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Identification of persons</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">A61B5/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Fittings or systems for preventing or indicating unauthorised use or theft of vehicles</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">B60R25/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Digitisers as input arrangement for user-computer interaction, e.g. touch screens or touch pads</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06F3/041</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">User authentication using biometric data for protecting computers, components thereof, programs or data against unauthorised activity</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06F21/32</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Checking-devices for individual entry or exit registers</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G07C9/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Arrangements for secret or secure communications; Network security protocols</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">H04L9/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Means for preventing unauthorised calls from a telephone set</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">H04M1/667</class-ref></paragraph-text></table-column></table-row></table></section-body></informative-references></references><special-rules><section-title>Special rules of classification</section-title><section-body><paragraph-text type="body">Techniques involving multiple biometrics are classified in group <class-ref scheme="cpc">G06V40/70</class-ref>.</paragraph-text></section-body></special-rules><glossary-of-terms><section-title>Glossary of terms</section-title><section-body><paragraph-text type="preamble">In this place, the following terms or expressions are used with the meaning indicated:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">vascular patterns</paragraph-text></table-column><table-column><paragraph-text type="body">2D or 3D images of the (sub-)surface of fingers, palm or sclera showing the vessels/veins</paragraph-text></table-column></table-row></table></section-body></glossary-of-terms></definition-item>
<definition-item date-revised="2024-08-01"><classification-symbol scheme="cpc">G06V40/145</classification-symbol><definition-title>Sensors therefor</definition-title><definition-statement><section-title>Definition statement</section-title><section-body><paragraph-text type="preamble">This place covers:</paragraph-text><paragraph-text type="body">Vascular imagers such as a finger vein scanner or palm vein scanner which use near infrared lights combined with a special camera to capture vein patterns.</paragraph-text><paragraph-text type="body">Finger, palm or eye vessels sensors of all kinds.</paragraph-text><paragraph-text type="body">Near infrared cameras used for making the vascular pattern visible.</paragraph-text><paragraph-text type="body">Illustrative example of subject matter classified in this place:</paragraph-text><paragraph-text type="body"><media id="media170.png" file-name="cpc-def-G06V-0170.png" type="png" preferred-width="10.18cm" preferred-height="5.14cm"/></paragraph-text></section-body></definition-statement><relationship><section-title>Relationships with other classification places</section-title><section-body><paragraph-text type="body">Techniques involving acquisition of finger movements on a digitiser are classified in group <class-ref scheme="cpc">G06F3/041</class-ref>.</paragraph-text></section-body></relationship><references><section-title>References</section-title><informative-references><section-title>Informative references</section-title><section-body><paragraph-text type="preamble">Attention is drawn to the following places, which may be of interest for search:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Recognition of fingerprints or palmprints within images or video</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V40/12</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Recognition of faces within images or video</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V40/16</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Recognition of eye characteristics within images or video, e.g. of the iris</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V40/18</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Multimodal biometrics, e.g. combining information from different biometric modalities</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V40/70</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Identification of persons</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">A61B5/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Fittings or systems for preventing or indicating unauthorised use or theft of vehicles</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">B60R25/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Digitisers as input arrangement for user-computer interaction, e.g. touch screens or touch pads</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06F3/041</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">User authentication using biometric data for protecting computers, components thereof, programs or data against unauthorised activity</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06F21/32</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Checking-devices for individual entry or exit registers</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G07C9/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Arrangements for secret or secure communications; Network security protocols</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">H04L9/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Means for preventing unauthorised calls from a telephone set</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">H04M1/667</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Scanners in general</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">H04N1/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Cameras in general</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">H04N7/00</class-ref></paragraph-text></table-column></table-row></table></section-body></informative-references></references><special-rules><section-title>Special rules of classification</section-title><section-body><paragraph-text type="body">Techniques which combine fingerprint sensors and vein (vascular) sensors are classified in groups <class-ref scheme="cpc">G06V40/13</class-ref> and <class-ref scheme="cpc">G06V40/145</class-ref>.</paragraph-text><paragraph-text type="body">Acquisition of vascular patterns generally requires specialised hardware which is essentially different from normal cameras. For this reason, vascular sensors are not classified in the generic group <class-ref scheme="cpc">G06V10/10</class-ref>.</paragraph-text></section-body></special-rules></definition-item>
<definition-item date-revised="2024-08-01"><classification-symbol scheme="cpc">G06V40/16</classification-symbol><definition-title>Human faces, e.g. facial parts, sketches or expressions</definition-title><definition-statement><section-title>Definition statement</section-title><section-body><paragraph-text type="preamble">This place covers:</paragraph-text><paragraph-text type="body">Detection, localisation, representation and recognition of the face or of facial parts.</paragraph-text><paragraph-text type="body">Detection of multiple faces in an image or video, e.g. for video-conferencing.</paragraph-text><paragraph-text type="body">Feature extraction based on the facial image taken as a whole, e.g. holistic features such as colour of the face region, eigenfaces, Fisherfaces, etc., or based on facial parts, e.g. local features such facial components (eyes, nose, etc.), their geometric configuration.</paragraph-text><paragraph-text type="body">Face occlusion detection.</paragraph-text><paragraph-text type="body">Race, gender and age detection based on facial features (e.g. skin wrinkles).</paragraph-text><paragraph-text type="body">Recognition of facial expressions, e.g. static or dynamic expressions.</paragraph-text><paragraph-text type="body">Spoof-by-picture using an image of the face.</paragraph-text><paragraph-text type="body">Detection of faces using different types of acquisition modalities, e.g. infrared (thermal) images, or their combination.</paragraph-text><paragraph-text type="body">Facial skin detection based on skin properties, e.g. skin colour.</paragraph-text><paragraph-text type="body">Illustrative examples of subject matter classified in this place:</paragraph-text><paragraph-text type="body">1.</paragraph-text><paragraph-text type="body"><media id="media171.png" file-name="cpc-def-G06V-0171.png" type="png" preferred-width="9.86cm" preferred-height="9.29cm"/></paragraph-text><paragraph-text type="body">Faces detected in an image.</paragraph-text><paragraph-text type="body">2.</paragraph-text><paragraph-text type="body"><media id="media172.png" file-name="cpc-def-G06V-0172.png" type="png" preferred-width="8.57cm" preferred-height="10.46cm"/></paragraph-text><paragraph-text type="body">Acquisition of a face in 3D by means of a smartphone.</paragraph-text></section-body></definition-statement><references><section-title>References</section-title><informative-references><section-title>Informative references</section-title><section-body><paragraph-text type="preamble">Attention is drawn to the following places, which may be of interest for search:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Recognising three-dimensional [3D] objects in scenes</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V20/64</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Recognition of human or animal bodies in images or video, e.g. vehicle occupants or pedestrians; Body parts, e.g. hands</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V40/10</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Recognition of fingerprints or palmprints within images or video</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V40/12</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Recognition of eye characteristics within images or video, e.g. of the iris</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V40/18</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Multimodal biometrics, e.g. combining information from different biometric modalities</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V40/70</class-ref></paragraph-text></table-column></table-row></table></section-body></informative-references></references><special-rules><section-title>Special rules of classification</section-title><section-body><paragraph-text type="body">Recognition using iris patterns of the eye are classified in group <class-ref scheme="cpc">G06V40/18</class-ref>. If the technical aspects of a document cover aspects relevant both for face recognition and iris recognition, both aspects are classified in groups <class-ref scheme="cpc">G06V40/16</class-ref> and <class-ref scheme="cpc">G06V40/18</class-ref>.</paragraph-text><paragraph-text type="body">Techniques for spoof detection of faces, e.g. spoof-by-picture, are classified in groups <class-ref scheme="cpc">G06V40/16</class-ref> and <class-ref scheme="cpc">G06V40/40</class-ref>.</paragraph-text><paragraph-text type="body">Techniques for face recognition using 3D models are also classified in group <class-ref scheme="cpc">G06V20/64</class-ref>.</paragraph-text></section-body></special-rules><glossary-of-terms><section-title>Glossary of terms</section-title><section-body><paragraph-text type="preamble">In this place, the following terms or expressions are used with the meaning indicated:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">eigenface</paragraph-text></table-column><table-column><paragraph-text type="body">face representation using a principal component analysis in a high-dimensional space created from images of faces. The eigenvectors of the representation are derived from the covariance matrix of the probability distribution computed in this high-dimensional vector space.</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">fisherface</paragraph-text></table-column><table-column><paragraph-text type="body">linear discriminant analysis [LDA] applied in a multi-dimensional representation space created from a set of face images, resulting in a set of basis vectors defining that space</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">frontal face recognition</paragraph-text></table-column><table-column><paragraph-text type="body">face images are generally obtained by placing a camera in front of the subject who is asked to look at the camera while the picture is taken</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">illumination-invariant recognition</paragraph-text></table-column><table-column><paragraph-text type="body">recognition insensitive to changes in lighting conditions</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">multiview face recognition</paragraph-text></table-column><table-column><paragraph-text type="body">employs a gallery of images of every face at various poses to cover multiple views for each face</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">pose-invariant recognition</paragraph-text></table-column><table-column><paragraph-text type="body">recognition insensitive to changes in pose</paragraph-text></table-column></table-row></table></section-body></glossary-of-terms></definition-item>
<definition-item date-revised="2024-08-01"><classification-symbol scheme="cpc">G06V40/18</classification-symbol><definition-title>Eye characteristics, e.g. of the iris</definition-title><definition-statement><section-title>Definition statement</section-title><section-body><paragraph-text type="preamble">This place covers:</paragraph-text><paragraph-text type="body">Acquisition, pre-processing, feature extraction, clustering, classification of eye regions or eye components (e.g. iris, pupil, eyelids, eyelashes, sclera) for:</paragraph-text><list><list-item><paragraph-text type="body">Biometric identification and authentication by eye characteristics, e.g. iris recognition;</paragraph-text></list-item><list-item><paragraph-text type="body">Recognition of eye movements (e.g. fixation, saccade, smooth pursuit) and detection of eye blink;</paragraph-text></list-item><list-item><paragraph-text type="body">Eye tracking, gaze estimation and correction, by acquiring the image of the eye or in combination with the analysis of the scene (e.g. using saliency models) for biometric purposes. The techniques involved may use specialised hardware, such as head-mounted systems, infrared or visible light, or may use computer vision methods, such as modelling eye and scene geometry, appearance-based methods, etc.;</paragraph-text></list-item><list-item><paragraph-text type="body">Red eye detection due to image acquisition using a camera flash;</paragraph-text></list-item><list-item><paragraph-text type="body">Monitoring attention-based eye movements, e.g. for measuring the time spent when looking at products for advertisements purposes;</paragraph-text></list-item><list-item><paragraph-text type="body">Detecting and monitoring the eye open and eye closed states, e.g. for monitoring driver fatigue.</paragraph-text></list-item></list><paragraph-text type="body">Illustrative examples of subject matter classified in this place:</paragraph-text><paragraph-text type="body">1. Iris recognition</paragraph-text><paragraph-text type="body"><media id="media173.png" file-name="cpc-def-G06V-0173.png" type="png" preferred-width="10.92cm" preferred-height="7.6cm"/></paragraph-text><paragraph-text type="body">Patterns of the eye are extracted and used for personal identification.</paragraph-text><paragraph-text type="body">2.</paragraph-text><paragraph-text type="body"><media id="media174.png" file-name="cpc-def-G06V-0174.png" type="png" preferred-width="11.45cm" preferred-height="7.3cm"/></paragraph-text><paragraph-text type="body">The IrisCode (a binary sequence which characterises the texture of the iris) may be used for personal identification.</paragraph-text><paragraph-text type="body">3.</paragraph-text><paragraph-text type="body"><media id="media175.png" file-name="cpc-def-G06V-0175.png" type="png" preferred-width="16cm" preferred-height="6.34cm"/></paragraph-text><paragraph-text type="body">Acquisition of the eye using a dual system based on a low-resolution acquisition to detect the face and high-resolution camera to detect the iris.</paragraph-text><paragraph-text type="body">4.</paragraph-text><paragraph-text type="body"><media id="media176.png" file-name="cpc-def-G06V-0176.png" type="png" preferred-width="7.28cm" preferred-height="5.72cm"/></paragraph-text><paragraph-text type="body">Detection of the iris region using the variations of the image grey levels along a crossing line.</paragraph-text><paragraph-text type="body">5.</paragraph-text><paragraph-text type="body"><media id="media177.png" file-name="cpc-def-G06V-0177.png" type="png" preferred-width="6.01cm" preferred-height="4.64cm"/></paragraph-text><paragraph-text type="body">Geometrical representation of different eye components.</paragraph-text><paragraph-text type="body">6.</paragraph-text><paragraph-text type="body"><media id="media178.png" file-name="cpc-def-G06V-0178.png" type="png" preferred-width="14.9cm" preferred-height="6.37cm"/></paragraph-text><paragraph-text type="body">Eye detection using a neural network.</paragraph-text></section-body></definition-statement><references><section-title>References</section-title><informative-references><section-title>Informative references</section-title><section-body><paragraph-text type="preamble">Attention is drawn to the following places, which may be of interest for search:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Computing image salient features for recognition purposes</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/46</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Recognition or understanding of scenes inside of a vehicle</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V20/59</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Recognition of human or animal bodies within images or video, e.g. vehicle occupants or pedestrians; Body parts, e.g. hands</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V40/10</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Recognition of fingerprints or palmprints in images or video</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V40/12</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Recognition of human faces in images or video, e.g. facial parts, sketches or expressions</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V40/16</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Apparatus for testing the eyes; Instruments for examining the eyes</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">A61B3/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06F3/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Image analysis for determining position or orientation of objects or cameras</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06T7/70</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Scanning, transmission or reproduction of documents or the like; Colour correction or control; Red eye correction</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">H04N1/62</class-ref></paragraph-text></table-column></table-row></table></section-body></informative-references></references><special-rules><section-title>Special rules of classification</section-title><section-body><paragraph-text type="body">Recognition using iris patterns of the eye are classified in group <class-ref scheme="cpc">G06V40/18</class-ref>. If the technical aspects of a document cover aspects relevant both for face recognition and iris recognition, both aspects are classified in groups <class-ref scheme="cpc">G06V40/16</class-ref> and <class-ref scheme="cpc">G06V40/18</class-ref>.</paragraph-text><paragraph-text type="body">Techniques for spoof detection of faces, e.g. spoof-by-picture, are classified in groups <class-ref scheme="cpc">G06V40/16</class-ref> and <class-ref scheme="cpc">G06V40/40</class-ref>.</paragraph-text></section-body></special-rules><glossary-of-terms><section-title>Glossary of terms</section-title><section-body><paragraph-text type="preamble">In this place, the following terms or expressions are used with the meaning indicated:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">LoG</paragraph-text></table-column><table-column><paragraph-text type="body">line of gaze (optical axis)</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">LoS</paragraph-text></table-column><table-column><paragraph-text type="body">line of sight (visual axis)</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">PCR</paragraph-text></table-column><table-column><paragraph-text type="body">pupil corneal reflection</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">PoR</paragraph-text></table-column><table-column><paragraph-text type="body">point of regard</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">WFOV (camera), WFOV</paragraph-text></table-column><table-column><paragraph-text type="body">wide field of view (camera provided with a relatively large view to roughly detect the position of the eye)</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">NFOV (camera), NFOV</paragraph-text></table-column><table-column><paragraph-text type="body">narrow field of view (camera provided with a narrow field of view which acquires a more precise eye image)</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Purkinje images</paragraph-text></table-column><table-column><paragraph-text type="body">reflections of objects present in the environment which can be seen on the structure of the eye, e.g. sclera</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">saliency map</paragraph-text></table-column><table-column><paragraph-text type="body">map displaying areas of higher visual importance, e.g. luminance contrast, semantic contrast, etc.</paragraph-text></table-column></table-row></table></section-body></glossary-of-terms></definition-item>
<definition-item date-revised="2024-08-01"><classification-symbol scheme="cpc">G06V40/19</classification-symbol><definition-title>Sensors therefor</definition-title><definition-statement><section-title>Definition statement</section-title><section-body><paragraph-text type="preamble">This place covers:</paragraph-text><paragraph-text type="body">Special sensors or acquisition arrangements adapted to acquire the image of an eye or its anatomical components (iris, eye fundus, etc.) for biometric purposes.</paragraph-text><paragraph-text type="body">Illustrative examples of subject matter classified in this place:</paragraph-text><paragraph-text type="body">1.</paragraph-text><paragraph-text type="body"><media id="media179.png" file-name="cpc-def-G06V-0179.png" type="png" preferred-width="6.35cm" preferred-height="7.56cm"/></paragraph-text><paragraph-text type="body">Optical system for eye and iris acquisition.</paragraph-text><paragraph-text type="body">2.</paragraph-text><paragraph-text type="body"><media id="media180.png" file-name="cpc-def-G06V-0180.png" type="png" preferred-width="6.1cm" preferred-height="3.89cm"/></paragraph-text><paragraph-text type="body">Optical system for eye and iris acquisition.</paragraph-text></section-body></definition-statement><references><section-title>References</section-title><informative-references><section-title>Informative references</section-title><section-body><paragraph-text type="preamble">Attention is drawn to the following places, which may be of interest for search:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Recognition of fingerprints or palmprints within image or video data</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V40/12</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Recognition of faces within image or video data</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V40/16</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Recognition of eye characteristics within image or video data, e.g. of the iris</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V40/18</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Multimodal biometrics, e.g. combining information from different biometric modalities</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V40/70</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Apparatus for testing the eyes; Instruments for examining the eyes</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">A61B3/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Identification of persons</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">A61B5/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Fittings or systems for preventing or indicating unauthorised use or theft of vehicles</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">B60R25/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Digitisers as the input arrangement for user-computer interaction, e.g. touch screens or touch pads</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06F3/041</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Security arrangements for protecting computers, components thereof, programs or data against unauthorised activity</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06F21/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Checking-devices for individual entry or exit registers</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G07C9/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Arrangements for secret or secure communications; Network security protocols</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">H04L9/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Means for preventing unauthorised calls from a telephone set</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">H04M1/667</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Scanners in general</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">H04N1/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Cameras in general</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">H04N7/00</class-ref></paragraph-text></table-column></table-row></table></section-body></informative-references></references><special-rules><section-title>Special rules of classification</section-title><section-body><paragraph-text type="body">Acquisition of eye patterns generally requires specialised hardware which is essentially different from normal cameras. For this reason, eye sensors are not classified in the generic group <class-ref scheme="cpc">G06V10/10</class-ref>.</paragraph-text></section-body></special-rules></definition-item>
<definition-item date-revised="2024-08-01"><classification-symbol scheme="cpc">G06V40/20</classification-symbol><definition-title>Movements or behaviour, e.g. gesture recognition  (recognition of facial expressions <class-ref scheme="cpc">G06V40/16</class-ref>)</definition-title><definition-statement><section-title>Definition statement</section-title><section-body><paragraph-text type="preamble">This place covers:</paragraph-text><paragraph-text type="body">Detection, tracking, recognition of:</paragraph-text><list><list-item><paragraph-text type="body">Gestures, e.g. whole body, upper body, hand, arm, head, free movements for sport activities, hand movements for interface control;</paragraph-text></list-item><list-item><paragraph-text type="body">Hand or arm movements, e.g. for deaf sign language recognition;</paragraph-text></list-item><list-item><paragraph-text type="body">Gait recognition, e.g. walking, running;</paragraph-text></list-item><list-item><paragraph-text type="body">Lip movement, e.g. for lip-reading.</paragraph-text></list-item></list><paragraph-text type="body">Recognising human behaviour, e.g. daily activities; monitoring eating patterns or calories intake.</paragraph-text><paragraph-text type="body">Recognition of movements during sport activities.</paragraph-text><paragraph-text type="body">Recognising touch or drawing movements on a surface or in a three-dimensional space, e.g. patterns on a touch screen, smart tables, smart whiteboards, etc.</paragraph-text><paragraph-text type="body">Illustrative examples of subject matter classified in this place:</paragraph-text><paragraph-text type="body">1.</paragraph-text><paragraph-text type="body"><media id="media181.png" file-name="cpc-def-G06V-0181.png" type="png" preferred-width="6.94cm" preferred-height="8.19cm"/></paragraph-text><paragraph-text type="body">Recognising the movement of a hand for controlling an object on the screen of a computer.</paragraph-text><paragraph-text type="body">2.</paragraph-text><paragraph-text type="body"><media id="media182.png" file-name="cpc-def-G06V-0182.png" type="png" preferred-width="13cm" preferred-height="7.07cm"/></paragraph-text><paragraph-text type="body">Recognising deaf-sign language by movement analysis.</paragraph-text><paragraph-text type="body">3.</paragraph-text><paragraph-text type="body"><media id="media183.png" file-name="cpc-def-G06V-0183.png" type="png" preferred-width="9.44cm" preferred-height="7.05cm"/></paragraph-text><paragraph-text type="body">Recognising human activities, e.g. walking.</paragraph-text><paragraph-text type="body">4.</paragraph-text><paragraph-text type="body"><media id="media184.png" file-name="cpc-def-G06V-0184.png" type="png" preferred-width="9.12cm" preferred-height="12.66cm"/></paragraph-text><paragraph-text type="body">Recognising lips states and their motion.</paragraph-text></section-body></definition-statement><references><section-title>References</section-title><limiting-references><section-title>Limiting references</section-title><section-body><paragraph-text type="preamble">This place does not cover:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Facial expression recognition</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V40/16</class-ref></paragraph-text></table-column></table-row></table></section-body></limiting-references><informative-references><section-title>Informative references</section-title><section-body><paragraph-text type="preamble">Attention is drawn to the following places, which may be of interest for search:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Recognition of scenes; Scene-specific elements</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V20/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Recognition of human or animal bodies within image or video data</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V40/10</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Recognition of fingerprints or palmprints within image or video data</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V40/12</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Input arrangements or combined input and output arrangements for interaction between user and computer</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06F3/01</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Analysis of motion in images</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06T7/20</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Speech recognition using position of the lips, movement of the lips or face analysis</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G10L15/25</class-ref></paragraph-text></table-column></table-row></table></section-body></informative-references></references><special-rules><section-title>Special rules of classification</section-title><section-body><paragraph-text type="body">Recognising activities for scenes under surveillance (e.g. suspicious activities, occupancy, etc.) is classified in group <class-ref scheme="cpc">G06V20/52</class-ref>.</paragraph-text><paragraph-text type="body">Static gesture recognition, e.g. recognition of deaf signs is classified in group <class-ref scheme="cpc">G06V40/10</class-ref>.</paragraph-text></section-body></special-rules><glossary-of-terms><section-title>Glossary of terms</section-title><section-body><paragraph-text type="preamble">In this place, the following terms or expressions are used with the meaning indicated:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">dynamic gesture</paragraph-text></table-column><table-column><paragraph-text type="body">movement of the hand encoding a certain meaning.</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">gait recognition</paragraph-text></table-column><table-column><paragraph-text type="body">recognising a person&apos;s manner of walking.</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">static gesture</paragraph-text></table-column><table-column><paragraph-text type="body">posture of a hand denoting a certain meaning.</paragraph-text></table-column></table-row></table></section-body></glossary-of-terms></definition-item>
<definition-item date-revised="2024-08-01"><classification-symbol scheme="cpc">G06V40/30</classification-symbol><definition-title>Writer recognition; Reading and verifying signatures</definition-title><definition-statement><section-title>Definition statement</section-title><section-body><paragraph-text type="preamble">This place covers:</paragraph-text><paragraph-text type="body">Acquisition, pre-processing, feature extraction and classification of handwritten signatures and handwritten text input to identify the writer.</paragraph-text><paragraph-text type="body">The processing may be based on a bit map image showing the signature (called static or offline signature recognition) or a signal representing the position, velocity, acceleration or pressure of the writing tip (called dynamic or online signature recognition).</paragraph-text><paragraph-text type="body">Illustrative examples of subject matter classified in this place:</paragraph-text><paragraph-text type="body">1.</paragraph-text><paragraph-text type="body"><media id="media185.png" file-name="cpc-def-G06V-0185.png" type="png" preferred-width="3.89cm" preferred-height="6.35cm"/></paragraph-text><paragraph-text type="body">Handwriting input using a grid defined on the screen of a mobile phone.</paragraph-text><paragraph-text type="body">2.</paragraph-text><paragraph-text type="body"><media id="media186.png" file-name="cpc-def-G06V-0186.png" type="png" preferred-width="3.28cm" preferred-height="5.61cm"/></paragraph-text><paragraph-text type="body">Transforming a signature to a consistent angle of inclination for recognition purposes.</paragraph-text><paragraph-text type="body">3.</paragraph-text><paragraph-text type="body"><media id="media187.png" file-name="cpc-def-G06V-0187.png" type="png" preferred-width="16cm" preferred-height="9.68cm"/></paragraph-text><paragraph-text type="body">Temporal analysis of a pen stroke for signature encoding.</paragraph-text></section-body></definition-statement><references><section-title>References</section-title><informative-references><section-title>Informative references</section-title><section-body><paragraph-text type="preamble">Attention is drawn to the following places, which may be of interest for search:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Image acquisition for image or video recognition or understanding</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/10</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Image preprocessing for image or video recognition or understanding</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/20</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Arrangements for image or video recognition using probabilistic graphical models, e.g. Markov models or Bayesian networks</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/84</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Image-based acquisition using hand-held instruments for character recognition; Constructional details of the instruments</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V30/142</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Character recognition of cursive writing</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V30/226</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Character recognition; Recognition of three-dimensional handwriting, e.g. writing in the air</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V30/228</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Recognising digital ink, i.e. recognising handwritten individual characters or symbols represented by temporal sequences of position coordinates</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V30/32</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Image-based pattern recognition of technical drawings or geographical maps</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V30/422</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Input arrangements for converting the position or the displacement of a member into a coded form</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06F3/03</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Interaction techniques based on a graphical user interface; Using specific features provided by the device; Entering handwritten data, e.g. gestures, text</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06F3/04883</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Security arrangements for protecting computers, components thereof, programs or data against unauthorised activity</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06F21/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Security arrangements in wireless communication networks, e.g. access security or fraud detection; Authentication, e.g. verifying user identity or authorisation; Protecting privacy or anonymity</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">H04W12/00</class-ref></paragraph-text></table-column></table-row></table></section-body></informative-references></references><special-rules><section-title>Special rules of classification</section-title><section-body><paragraph-text type="body">Details about the temporal aspects in the acquisition, preprocessing, feature extraction or recognition of the digital ink are classified in group <class-ref scheme="cpc">G06V30/32</class-ref>.</paragraph-text></section-body></special-rules><glossary-of-terms><section-title>Glossary of terms</section-title><section-body><paragraph-text type="preamble">In this place, the following terms or expressions are used with the meaning indicated:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">off-line signature</paragraph-text></table-column><table-column><paragraph-text type="body">analysis of a (static) image characterising the signature</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">on-line signature</paragraph-text></table-column><table-column><paragraph-text type="body">analysis of a temporal sequence of position, velocity, acceleration or pressure values characterising the signature</paragraph-text></table-column></table-row></table></section-body></glossary-of-terms></definition-item>
<definition-item date-revised="2024-08-01"><classification-symbol scheme="cpc">G06V40/40</classification-symbol><definition-title>Spoof detection, e.g. liveness detection</definition-title><definition-statement><section-title>Definition statement</section-title><section-body><paragraph-text type="preamble">This place covers:</paragraph-text><paragraph-text type="body">Spoof detection, i.e. detecting an attempt to fool a biometric system by presenting data which is not genuine. An example is the detection of inanimate replicas of living tissue, and the distinguishing of such replicas, e.g. a rubber model of a finger, from parts of living beings.</paragraph-text><paragraph-text type="body">Spoof detection can be performed using acquisition arrangements in which the sensor is provided with specialised hardware to assess or highlight the genuineness of the acquired data (e.g. using special illumination in infrared) or by performing image processing operations (e.g. colour analysis to discriminate the genuine skin against a copy). Multiple biometric modalities can be involved:</paragraph-text><list><list-item><paragraph-text type="body">Signals such as blood pressure, pulse and perspiration at the fingertips, hippus movement of the pupil, brain waves [EEG] and electrical heart signals [ECG] in combination with other biometric images;</paragraph-text></list-item><list-item><paragraph-text type="body">Reflexive signals such as pupillary light reflex (pupil dilation), corneal reflex (blink reflex) and patellar reflex (knee-jerk);</paragraph-text></list-item><list-item><paragraph-text type="body">Voluntary signals given unconsciously or as a response to a &quot;challenge&quot; such as blinking, mouth movements and facial expressions.</paragraph-text></list-item></list><paragraph-text type="body">Other properties of a body can be assessed:</paragraph-text><list><list-item><paragraph-text type="body">Determination of the flatness of a face to detect use of a picture to challenge a biometric system (&quot;spoof-by-picture&quot;);</paragraph-text></list-item><list-item><paragraph-text type="body">Light distribution in a real finger which differs from a fake finger.</paragraph-text></list-item></list><paragraph-text type="body">Illustrative examples of subject matter classified in this place:</paragraph-text><paragraph-text type="body">1A.</paragraph-text><paragraph-text type="body"><media id="media189.png" file-name="cpc-def-G06V-0189.png" type="png" preferred-width="7.32cm" preferred-height="9.17cm"/></paragraph-text><paragraph-text type="body">1B.</paragraph-text><paragraph-text type="body"><media id="media188.png" file-name="cpc-def-G06V-0188.png" type="png" preferred-width="4.21cm" preferred-height="9cm"/></paragraph-text><paragraph-text type="body">Recognition method using hand biometrics with anti-counterfeiting. The user is asked to perform randomly selected gestures with the hand, e.g. rotate the hand to the left, clench into a fist. The gestures are recognised, allowing the method to determine that a real user is standing in from of the camera.</paragraph-text><paragraph-text type="body">2.</paragraph-text><paragraph-text type="body"><media id="media190.png" file-name="cpc-def-G06V-0190.png" type="png" preferred-width="14.39cm" preferred-height="8.21cm"/></paragraph-text><paragraph-text type="body">When the eye opens, the eye aspect ratio is roughly constant, and only fluctuates around the range 0.25. Once the eye blinks and closes, because the vertical distance is almost zero, the eye aspect ratio is correspondingly reduced to zero. When the eye opens again, the eye aspect ratio rises to the range 0.25 again. These measurements may indicate if the person is a real, genuine person or a fake.</paragraph-text></section-body></definition-statement><relationship><section-title>Relationships with other classification places</section-title><section-body><paragraph-text type="body">Authentication of users for computer access is classified in group <class-ref scheme="cpc">G06F21/32</class-ref>.</paragraph-text><paragraph-text type="body">Authentication of financial documents is classified in group <class-ref scheme="cpc">G07D7/00</class-ref>.</paragraph-text></section-body></relationship><references><section-title>References</section-title><informative-references><section-title>Informative references</section-title><section-body><paragraph-text type="preamble">Attention is drawn to the following places, which may be of interest for search:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Detection or correction or errors, e.g. by rescanning the pattern; Evaluation of the quantity of an acquired biometric pattern</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/98</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Recognition of fingerprints or palmprints in images or video</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V40/12</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Recognition of vascular patterns in images or video</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V40/14</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Recognition of human faces in images or video, e.g. facial parts, sketches or expressions</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V40/16</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Recognition of eye characteristics in images or video, e.g. of the iris</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V40/18</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Recognition of movements or behaviour in images or video, e.g. gesture recognition</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V40/20</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Recognition of signatures</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V40/30</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Multimodal biometrics, e.g. combining information from different biometric modalities</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V40/70</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Fittings or systems for preventing or indicating unauthorised use or theft of vehicles</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">B60R25/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Digitisers as the input arrangement for user-computer interaction, e.g. touch screens or touch pads</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06F3/041</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">User authentication using biometric data for protecting computers, components thereof, programs or data against unauthorised activity</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06F21/32</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">User authentication by graphic or iconic representation for protecting computers, components thereof, programs or data against unauthorised activity</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06F21/36</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Checking-devices for individual entry or exit registers</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G07C9/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Testing specially adapted to determine the identity or genuineness of valuable papers</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G07D7/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Arrangements for secret or secure communications; Network security protocols</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">H04L9/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Means for preventing unauthorised calls from a telephone set</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">H04M1/667</class-ref></paragraph-text></table-column></table-row></table></section-body></informative-references></references><special-rules><section-title>Special rules of classification</section-title><section-body><paragraph-text type="body">This group is used alone when no technical contribution can be identified in the processing associated with biometric authentication. If, however, a technical contribution can be identified in biometric authentication, the respective groups are allocated in combination with this group. In other words, anti-spoofing is usually a part of an authentication process which acts as a verifier of liveliness, thus anti-spoofing inventions often rely on processing biometric data of a certain modality provided in the following groups:</paragraph-text><list><list-item><paragraph-text type="body">Fingerprints or palmprints &#8211; group <class-ref scheme="cpc">G06V40/12</class-ref>; </paragraph-text></list-item><list-item><paragraph-text type="body">Human faces, e.g. facial parts, sketches or expressions &#8211; group <class-ref scheme="cpc">G06V40/16</class-ref>; </paragraph-text></list-item><list-item><paragraph-text type="body">Eyes &#8211; group <class-ref scheme="cpc">G06V40/18</class-ref>; </paragraph-text></list-item><list-item><paragraph-text type="body">Movements or behaviour, e.g. gesture recognition &#8211; group <class-ref scheme="cpc">G06V40/20</class-ref>; </paragraph-text></list-item><list-item><paragraph-text type="body">Signatures &#8211; group <class-ref scheme="cpc">G06V40/30</class-ref>.</paragraph-text></list-item></list><paragraph-text type="body">For example, in order to assure safe biometric authentication, the face matching process classified in group <class-ref scheme="cpc">G06V40/16</class-ref> combined with a liveness detection, e.g. by determining if the user in front of the camera is moving his mouth when requested so that a spoof-by-picture attack can be prevented, is classified also in group <class-ref scheme="cpc">G06V40/40</class-ref>.</paragraph-text></section-body></special-rules></definition-item>
<definition-item date-revised="2024-08-01"><classification-symbol scheme="cpc">G06V40/50</classification-symbol><definition-title>Maintenance of biometric data or enrolment thereof</definition-title><definition-statement><section-title>Definition statement</section-title><section-body><paragraph-text type="preamble">This place covers:</paragraph-text><paragraph-text type="body">Maintenance of biometric data which includes, e.g., enrolment of a user using biometric information or updating the biometric information stored in a database for each user.</paragraph-text><paragraph-text type="body">The enrolment process may include the decision which of the plurality of templates should be stored and used for future authentication of the user.</paragraph-text><paragraph-text type="body">Typical examples include replacement of enrolment data with the more recent one using temporal criteria (e.g. compensating for the aging of the person) or using quality criteria (e.g. a reference with higher quality has become available during system use).</paragraph-text><paragraph-text type="body">Illustrative examples of subject matter classified in this place:</paragraph-text><paragraph-text type="body">1.</paragraph-text><paragraph-text type="body"><media id="media191.png" file-name="cpc-def-G06V-0191.png" type="png" preferred-width="11.73cm" preferred-height="6.75cm"/></paragraph-text><paragraph-text type="body">Fingerprint authentication with template updating.</paragraph-text><paragraph-text type="body">2.</paragraph-text><paragraph-text type="body"><media id="media192.png" file-name="cpc-def-G06V-0192.png" type="png" preferred-width="16cm" preferred-height="9.55cm"/></paragraph-text><paragraph-text type="body">Fingerprint template update based on its quality.</paragraph-text></section-body></definition-statement><references><section-title>References</section-title><informative-references><section-title>Informative references</section-title><section-body><paragraph-text type="preamble">Attention is drawn to the following places, which may be of interest for search:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Detection or correction or errors, e.g. by rescanning the pattern; Evaluation of the quality of an acquired biometric pattern</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V10/98</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Recognition of fingerprints or palmprints in images or video</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V40/12</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Recognition of vascular patterns in images or video</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V40/14</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Recognition of human faces in images or video, e.g. facial parts, sketches or expressions</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V40/16</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Recognition of eye characteristics within images or video, e.g. of the iris</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V40/18</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Recognition of movements or behaviour in images or video, e.g. gesture recognition</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V40/20</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Recognition of signatures</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V40/30</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Multimodal biometrics, e.g. combining information from different biometric modalities</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V40/70</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Fittings or systems for preventing or indicating unauthorised use or theft of vehicles</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">B60R25/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Digitisers as the input arrangement for user-computer interaction, e.g. touch screens or touch pads</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06F3/041</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">User authentication using biometric data for protecting computers, components thereof, programs or data against unauthorised activity</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06F21/32</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">User authentication by graphic or iconic representation for protecting computers, components thereof, programs or data against unauthorised activity</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06F21/36</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Checking-devices for individual entry or exit registers</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G07C9/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Testing specially adapted to determine the identity or genuineness of valuable papers</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G07D7/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Arrangements for secret or secure communications; Network security protocols</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">H04L9/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Means for preventing unauthorised calls from a telephone set</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">H04M1/667</class-ref></paragraph-text></table-column></table-row></table></section-body></informative-references></references><special-rules><section-title>Special rules of classification</section-title><section-body><paragraph-text type="body">This group is used alone when no technical contribution can be identified in the processing associated with authentication. If, however, a technical contribution can be identified in user authentication, the respective groups are allocated in combination with this group. In other words, the maintenance of the biometric information improves the authentication process by updating the templates stored or initially storing templates during enrolment which ensure a certain quality, thus such inventions often rely on the following groups:</paragraph-text><list><list-item><paragraph-text type="body">Fingerprints or palmprints &#8211; group <class-ref scheme="cpc">G06V40/12</class-ref>; </paragraph-text></list-item><list-item><paragraph-text type="body">Human faces, e.g. facial parts, sketches or expressions &#8211; group <class-ref scheme="cpc">G06V40/16</class-ref>; </paragraph-text></list-item><list-item><paragraph-text type="body">Eyes &#8211; group <class-ref scheme="cpc">G06V40/18</class-ref>; </paragraph-text></list-item><list-item><paragraph-text type="body">Movements or behaviour, e.g. gesture recognition &#8211; group <class-ref scheme="cpc">G06V40/20</class-ref>; </paragraph-text></list-item><list-item><paragraph-text type="body">Signatures &#8211; group <class-ref scheme="cpc">G06V40/30</class-ref>.</paragraph-text></list-item></list><paragraph-text type="body">If the maintenance or enrolment involves quality-based criteria, classification in groups <class-ref scheme="cpc">G06V10/98</class-ref> and <class-ref scheme="cpc">G06V40/50</class-ref> is applied.</paragraph-text></section-body></special-rules></definition-item>
<definition-item date-revised="2024-08-01"><classification-symbol scheme="cpc">G06V40/60</classification-symbol><definition-title>Static or dynamic means for assisting the user to position a body part for biometric acquisition</definition-title><definition-statement><section-title>Definition statement</section-title><section-body><paragraph-text type="preamble">This place covers:</paragraph-text><paragraph-text type="body">Means for assisting the user to position body parts such as face, eye(s), hand(s) for the purpose of biometric identification using either static means, e.g. a finger guide for fingerprint acquisition, or dynamic means, e. g. a visual indication on an interactive screen.</paragraph-text><paragraph-text type="body">Illustrative examples of subject matter classified in this place:</paragraph-text><paragraph-text type="body">1A.</paragraph-text><paragraph-text type="body"><media id="media193.png" file-name="cpc-def-G06V-0193.png" type="png" preferred-width="5.1cm" preferred-height="5.33cm"/></paragraph-text><paragraph-text type="body">1B.</paragraph-text><paragraph-text type="body"><media id="media194.png" file-name="cpc-def-G06V-0194.png" type="png" preferred-width="5.21cm" preferred-height="6.65cm"/></paragraph-text><paragraph-text type="body">Special cradle used for finger positioning to allow reproducibility during subsequent acquisitions.</paragraph-text><paragraph-text type="body">2.</paragraph-text><paragraph-text type="body"><media id="media195.png" file-name="cpc-def-G06V-0195.png" type="png" preferred-width="8.26cm" preferred-height="5.88cm"/></paragraph-text><paragraph-text type="body">Determination of the liveness of a person by giving him visual feedback on a screen provided on the side of the car and giving directions to move the face in a certain way.</paragraph-text></section-body></definition-statement><references><section-title>References</section-title><informative-references><section-title>Informative references</section-title><section-body><paragraph-text type="preamble">Attention is drawn to the following places, which may be of interest for search:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Recognition of human or animal bodies in images or video, e.g. vehicle occupants or pedestrians; Body parts, e.g. hands</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V40/10</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Recognition of fingerprints or palmprints in images or video</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V40/12</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Recognition of human faces in images or video, e.g. facial parts, sketches or expressions</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V40/16</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Recognition of eye characteristics in images or video, e.g. of the iris</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V40/18</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Multimodal biometrics, e.g. combining information from different biometric modalities</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V40/70</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Input arrangements or combined input and output arrangements for interaction between user and computer</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06F3/01</class-ref></paragraph-text></table-column></table-row></table></section-body></informative-references></references><special-rules><section-title>Special rules of classification</section-title><section-body><paragraph-text type="body">When fingerprint acquisition is performed by requiring the user to place the finger in a recess specially provided to guide the acquisition, classification in groups <class-ref scheme="cpc">G06V40/60</class-ref> and <class-ref scheme="cpc">G06V40/13</class-ref> is applied. Similarly, when the face acquisition is guided by user feedback, classification in groups <class-ref scheme="cpc">G06V40/16</class-ref> and <class-ref scheme="cpc">G06V40/60</class-ref> is done.</paragraph-text></section-body></special-rules><glossary-of-terms><section-title>Glossary of terms</section-title><section-body><paragraph-text type="preamble">In this place, the following terms or expressions are used with the meaning indicated:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">fingerprint guide</paragraph-text></table-column><table-column><paragraph-text type="body">mechanical component specially provided to guide the placement of the finger during fingerprint acquisition</paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">visual feedback</paragraph-text></table-column><table-column><paragraph-text type="body">visual information indicating the position of a body part during image acquisition</paragraph-text></table-column></table-row></table></section-body></glossary-of-terms></definition-item>
<definition-item date-revised="2024-08-01"><classification-symbol scheme="cpc">G06V40/70</classification-symbol><definition-title>Multimodal biometrics, e.g. combining information from different biometric modalities</definition-title><definition-statement><section-title>Definition statement</section-title><section-body><paragraph-text type="preamble">This place covers:</paragraph-text><paragraph-text type="body">Biometric identification and authentication using multiple modalities at the same time, e.g. fingerprint and face, iris and face, etc. The image-based biometric modalities can be combined with non-image based modalities, such as voice or physiological measurements (heart rate).</paragraph-text><paragraph-text type="body">Illustrative examples of subject matter classified in this place:</paragraph-text><paragraph-text type="body">1.</paragraph-text><paragraph-text type="body"><media id="media196.png" file-name="cpc-def-G06V-0196.png" type="png" preferred-width="11.05cm" preferred-height="4.45cm"/></paragraph-text><paragraph-text type="body">Multiple biometric modalities are encoded in a database and used for personal identification.</paragraph-text><paragraph-text type="body">2.</paragraph-text><paragraph-text type="body"><media id="media197.png" file-name="cpc-def-G06V-0197.png" type="png" preferred-width="5.06cm" preferred-height="5.95cm"/></paragraph-text><paragraph-text type="body">Multiple biometric modalities analysis on a smartphone.</paragraph-text></section-body></definition-statement><references><section-title>References</section-title><informative-references><section-title>Informative references</section-title><section-body><paragraph-text type="preamble">Attention is drawn to the following places, which may be of interest for search:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">Recognition of fingerprints or palmprints in images or video</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V40/12</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Recognition of vascular patterns in images or video</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V40/14</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Recognition of human faces in images or video</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V40/16</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Recognition of eye characteristics in images or video, e.g. of the iris</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V40/18</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Recognition of movement or behaviour in images or video</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V40/20</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Writer recognition; Reading and verifying signatures</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06V40/30</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Identification of persons</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">A61B5/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Input arrangements or combined input and output arrangements for interaction between user and computer</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G06F3/01</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Speech recognition</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G10L15/00</class-ref></paragraph-text></table-column></table-row>
<table-row><table-column><paragraph-text type="body">Speaker identification or verification</paragraph-text></table-column><table-column><paragraph-text type="body"><class-ref scheme="cpc">G10L17/00</class-ref></paragraph-text></table-column></table-row></table></section-body></informative-references></references><special-rules><section-title>Special rules of classification</section-title><section-body><paragraph-text type="body">If the fusion between the different biometric modalities is performed, classification in groups <class-ref scheme="cpc">G06V10/80</class-ref> and <class-ref scheme="cpc">G06V40/70</class-ref> is applied.</paragraph-text></section-body></special-rules><glossary-of-terms><section-title>Glossary of terms</section-title><section-body><paragraph-text type="preamble">In this place, the following terms or expressions are used with the meaning indicated:</paragraph-text><table>
<table-row><table-column><paragraph-text type="body">multimodal biometrics</paragraph-text></table-column><table-column><paragraph-text type="body">using multiple biometric traits for identification or authentication</paragraph-text></table-column></table-row></table></section-body></glossary-of-terms></definition-item></definitions>